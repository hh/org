<html><head><meta charset="utf-8" /><meta content="width=device-width, initial-scale=1.0" name="viewport" /><meta content="" name="author" /><meta content="" name="description" /><title></title><link href="http://localhost:4000/static/css/firn_base.css" rel="stylesheet" /><link href="http://localhost:4000/static/css/ii.css" rel="stylesheet" /></head><body><main><article class="content"><div><div><section></section><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="introduction"><span class="firn-headline-text"><span>Introduction</span></span></h1><section><p><span>In this guide we will launch a highly-available three Node Kubernetes cluster on Equinix Metal using Talos as the Node OS, as well as bootstrap, and controlPlane provider for Cluster-API.</span></p><ul><li><p><span>What is </span><a class="firn-external" href="https://cluster-api.sigs.k8s.io/" target="_blank">Cluster-API</a><span>? ::</span></p></li></ul><blockquote><p><span>Cluster API is a Kubernetes sub-project focused on providing declarative APIs and tooling to simplify provisioning, upgrading, and operating multiple Kubernetes clusters.</span></p></blockquote><ul><li><p><span>What is </span><a class="firn-external" href="https://www.talos.dev/" target="_blank">Talos</a><span>? ::</span></p></li></ul><blockquote><p><span>Talos is a modern OS designed to be secure, immutable, and minimal.</span></p></blockquote><ul><li><p><span>What is </span><a class="firn-external" href="https://metal.equinix.com/" target="_blank">Equinix Metal</a><span>? ::</span></p></li></ul><blockquote><p><span>A globally-available bare metal “as-a-service” that can be deployed and interconnected in minutes.</span></p></blockquote><p><span>The folks over at Equinix Metal have a wonderful heart for supporting Open Source communities.</span></p><ul><li><p><span>Why is this important? :: In general: Orchestrating a container based OS such as Talos (</span><a class="firn-external" href="http://flatcar-linux.org/" target="_blank">Flatcar</a><span>, </span><a class="firn-external" href="https://getfedora.org/coreos/" target="_blank">Fedora CoreOS</a><span>, or </span><a class="firn-external" href="https://rancher.com/products/rancher/" target="_blank">RancherOS</a><span>) shifts focus from the Nodes to the workloads. In terms of Talos: Currently the documentation for running an OS such as Talos in Equinix Metal for Kubernetes with Cluster-API is not so well documented and therefore inaccessible. It's important to fill in the gaps of knowledge.</span></p></li></ul></section></div><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="dependencies"><span class="firn-headline-text"><span>Dependencies</span></span></h1><section><p><span>What you'll need for this guide:</span></p><ul><li><p><a class="firn-external" href="https://github.com/talos-systems/talos/releases/tag/v0.8.1" target="_blank">talosctl</a></p></li><li><p><a class="firn-external" href="https://kubernetes.io/docs/tasks/tools/install-kubectl/" target="_blank">kubectl</a></p></li><li><p><a class="firn-external" href="https://github.com/packethost/packet-cli" target="_blank">packet-cli</a></p></li><li><p><span>the ID and API token of existing Equinix Metal project</span></p></li><li><p><span>an existing Kubernetes cluster with a public IP (such as </span><a class="firn-external" href="http://kind.sigs.k8s.io/" target="_blank">kind</a><span>, </span><a class="firn-external" href="https://minikube.sigs.k8s.io/" target="_blank">minikube</a><span>, or a cluster already on Equinix Metal)</span></p></li></ul></section></div><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="prelimiary-steps"><span class="firn-headline-text"><span>Prelimiary steps</span></span></h1><section><p><span>In order to talk to Equinix Metal, we'll export environment variables to configure resources and talk via </span><code>packet-cli</code><span>.</span></p><p><span>Set the correct project to create and manage resources in:</span></p><pre class="language-tmate"><code class="language-tmate">  read -p 'PACKET_PROJECT_ID: ' PACKET_PROJECT_ID
</code></pre><p><span>The API key for your account or project:</span></p><pre class="language-tmate"><code class="language-tmate">  read -p 'PACKET_API_KEY: ' PACKET_API_KEY
</code></pre><p><span>Export the variables to be accessible from </span><code>packet-cli</code><span> and </span><code>clusterctl</code><span> later on:</span></p><pre class="language-tmate"><code class="language-tmate">  export PACKET_PROJECT_ID PACKET_API_KEY PACKET_TOKEN=$PACKET_API_KEY
</code></pre><p><span>In the existing cluster, a public LoadBalancer IP will be needed. I have already installed nginx-ingress in this cluster, which has got a Service with the cluster's elastic IP.
We'll need this IP address later for use in booting the servers.
If you have set up your existing cluster differently, it'll just need to be an IP that we can use.</span></p><pre class="language-tmate"><code class="language-tmate">  export LOAD_BALANCER_IP="$(kubectl -n nginx-ingress get svc nginx-ingress-ingress-nginx-controller -o=jsonpath='{.status.loadBalancer.ingress[0].ip}')"
</code></pre></section></div><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="setting-up-cluster-api"><span class="firn-headline-text"><span>Setting up Cluster-API</span></span></h1><section><p><span>Install Talos providers for Cluster-API bootstrap and controlplane in your existing cluster:</span></p><pre class="language-tmate"><code class="language-tmate">  clusterctl init -b talos -c talos -i packet
</code></pre><p><span>This will install Talos's bootstrap and controlPlane controllers as well as the Packet / Equinix Metal infrastructure provider.</span></p><p><strong><strong><span>Important</span></strong></strong><span> note:</span></p><ul><li><p><span>the </span><code>bootstrap-talos</code><span> controller in the </span><code>cabpt-system</code><span> namespace must be running a version greater than </span><code>v0.2.0-alpha.8</code><span>. The version can be displayed in with </span><code>clusterctl upgrade plan</code><span> when it's installed.</span></p></li></ul></section></div><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="setting-up-matchbox"><span class="firn-headline-text"><span>Setting up Matchbox</span></span></h1><section><p><span>Currently, since Equinix Metal have </span><strong><strong><span>not</span></strong></strong><span> yet added support for Talos, it is necessary to install </span><a class="firn-external" href="https://matchbox.psdn.io/" target="_blank">Matchbox</a><span> to boot the servers (There is an </span><a class="firn-external" href="https://github.com/packethost/packet-images/issues/26" target="_blank">issue</a><span> in progress and </span><a class="firn-external" href="https://feedback.equinixmetal.com/operating-systems/p/talos-as-officially-supported-operating-system" target="_blank">feedback</a><span> for adding support).</span></p><ul><li><p><span>What is Matchbox? ::</span></p></li></ul><blockquote><p><span>Matchbox is a service that matches bare-metal machines to profiles that PXE boot and provision clusters.</span></p></blockquote><p><span>Here is the manifest for a basic matchbox installation:</span></p><pre class="language-yaml"><code class="language-yaml">  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: matchbox
  spec:
    replicas: 1
    strategy:
      rollingUpdate:
        maxUnavailable: 1
    selector:
      matchLabels:
        name: matchbox
    template:
      metadata:
        labels:
          name: matchbox
      spec:
        containers:
          - name: matchbox
            image: quay.io/poseidon/matchbox:v0.9.0
            env:
              - name: MATCHBOX_ADDRESS
                value: "0.0.0.0:8080"
              - name: MATCHBOX_LOG_LEVEL
                value: "debug"
            ports:
              - name: http
                containerPort: 8080
            livenessProbe:
              initialDelaySeconds: 5
              httpGet:
                path: /
                port: 8080
            resources:
              requests:
                cpu: 30m
                memory: 20Mi
              limits:
                cpu: 50m
                memory: 50Mi
            volumeMounts:
              - name: data
                mountPath: /var/lib/matchbox
              - name: assets
                mountPath: /var/lib/matchbox/assets
        volumes:
          - name: data
            hostPath:
              path: /var/local/matchbox/data
          - name: assets
            hostPath:
              path: /var/local/matchbox/assets
  ---
  apiVersion: v1
  kind: Service
  metadata:
    name: matchbox
    annotations:
      metallb.universe.tf/allow-shared-ip: nginx-ingress
  spec:
    type: LoadBalancer
    selector:
      name: matchbox
    ports:
      - name: http
        protocol: TCP
        port: 8080
        targetPort: 8080
</code></pre><p><span>Save it as </span><code>matchbox.yaml</code></p><p><span>The manifests above were inspired by the manifests in the </span><a class="firn-external" href="https://github.com/poseidon/matchbox/tree/master/contrib/k8s" target="_blank">matchbox repo</a><span>.
For production it might be wise to use:</span></p><ul><li><p><span>an Ingress with full TLS</span></p></li><li><p><span>a ReadWriteMany storage provider instead hostPath for scaling</span></p></li></ul><p><span>With the manifests ready to go, we'll install Matchbox into the </span><code>matchbox</code><span> namespace on the existing cluster with the following commands:</span></p><pre class="language-tmate"><code class="language-tmate">  kubectl create ns matchbox
  kubectl -n matchbox apply -f ./matchbox.yaml
</code></pre><p><span>You may need to patch the </span><code>Service.spec.externalIPs</code><span> to have an IP to access it from if one is not populated:</span></p><pre class="language-tmate"><code class="language-tmate">  kubectl -n matchbox patch \
    service matchbox \
    -p "{\"spec\":{\"externalIPs\":[\"$LOAD_BALANCER_IP\"]}}"
</code></pre><p><span>Once the pod is live, We'll need to create a directory structure for storing Talos boot assets:</span></p><pre class="language-tmate"><code class="language-tmate">  kubectl -n matchbox exec -it \
    deployment/matchbox -- \
    mkdir -p /var/lib/matchbox/{profiles,groups} /var/lib/matchbox/assets/talos
</code></pre><p><span>Inside the Matchbox container, we'll download the Talos boot assets for Talos version 0.8.1 into the assets folder:</span></p><pre class="language-tmate"><code class="language-tmate">  kubectl -n matchbox exec -it \
    deployment/matchbox -- \
    wget -P /var/lib/matchbox/assets/talos \
    https://github.com/talos-systems/talos/releases/download/v0.8.1/initramfs-amd64.xz \
    https://github.com/talos-systems/talos/releases/download/v0.8.1/vmlinuz-amd64
</code></pre><p><span>Now that the assets have been downloaded, run a checksum against them to verify:</span></p><pre class="language-tmate"><code class="language-tmate">  kubectl -n matchbox exec -it \
    deployment/matchbox -- \
    sh -c "cd /var/lib/matchbox/assets/talos && \
      wget -O- https://github.com/talos-systems/talos/releases/download/v0.8.1/sha512sum.txt 2> /dev/null \
      | sed 's,_out/,,g' \
      | grep 'initramfs-amd64.xz\|vmlinuz-amd64' \
      | sha512sum -c -"
</code></pre><p><span>Since there's only one Pod in the Matchbox deployment, we'll export it's name to copy files into it:</span></p><pre class="language-tmate"><code class="language-tmate">  export MATCHBOX_POD_NAME=$(kubectl -n matchbox get pods -l name=matchbox -o=jsonpath='{.items[0].metadata.name}')
</code></pre><p><a class="firn-external" href="https://matchbox.psdn.io/matchbox/#profiles" target="_blank">Profiles in Matchbox</a><span> are JSON configurations for how the servers should boot, where from, and their kernel args. Save this file as </span><code>profile-default-amd64.json</code></p><pre class="language-json"><code class="language-json">  {
    "id": "default-amd64",
    "name": "default-amd64",
    "boot": {
      "kernel": "/assets/talos/vmlinuz-amd64",
      "initrd": [
        "/assets/talos/initramfs-amd64.xz"
      ],
      "args": [
        "initrd=initramfs-amd64.xz",
        "init_on_alloc=1",
        "init_on_free=1",
        "slub_debug=P",
        "pti=on",
        "random.trust_cpu=on",
        "console=tty0",
        "console=ttyS1,115200n8",
        "slab_nomerge",
        "printk.devkmsg=on",
        "talos.platform=packet",
        "talos.config=none"
      ]
    }
  }
</code></pre><p><a class="firn-external" href="https://matchbox.psdn.io/matchbox/#groups" target="_blank">Groups in Matchbox</a><span> are a way of letting servers pick up profiles based on selectors. Save this file as </span><code>group-default-amd64.json</code></p><pre class="language-json"><code class="language-json">  {
    "id": "default-amd64",
    "name": "default-amd64",
    "profile": "default-amd64",
    "selector": {
      "arch": "amd64"
    }
  }
</code></pre><p><span>We'll copy the profile and group into their respective folders:</span></p><pre class="language-tmate"><code class="language-tmate">  kubectl -n matchbox \
    cp ./profile-default-amd64.json \
    $MATCHBOX_POD_NAME:/var/lib/matchbox/profiles/default-amd64.json
  kubectl -n matchbox \
    cp ./group-default-amd64.json \
    $MATCHBOX_POD_NAME:/var/lib/matchbox/groups/default-amd64.json
</code></pre><p><span>List the files to validate that they were written correctly:</span></p><pre class="language-tmate"><code class="language-tmate">  kubectl -n matchbox exec -it \
    deployment/matchbox -- \
    sh -c 'ls -alh /var/lib/matchbox/*/'
</code></pre></section><div class="firn-headline-section firn-headline-section-2"><h2 class="firn-headline firn-headline-2" id="testing-matchbox"><span class="firn-headline-text"><span>Testing Matchbox</span></span></h2><section><p><span>Using </span><code>curl</code><span>, we can verify Matchbox's running state:</span></p><pre class="language-tmate"><code class="language-tmate">  curl http://$LOAD_BALANCER_IP:8080
</code></pre><p><span>To test matchbox, we'll create an invalid userdata configuration for Talos, saving as </span><code>userdata.txt</code><span>:</span></p><pre class="language-text"><code class="language-text">#!talos
</code></pre><p><span>Feel free to use a valid one.</span></p><p><span>Now let's talk to Equinix Metal to create a server pointing to the Matchbox server:</span></p><pre class="language-tmate"><code class="language-tmate">   packet-cli device create \
    --hostname talos-pxe-boot-test-1 \
    --plan c1.small.x86 \
    --facility sjc1 \
    --operating-system custom_ipxe \
    --project-id "$PACKET_PROJECT_ID" \
    --ipxe-script-url "http://$LOAD_BALANCER_IP:8080/ipxe?arch=amd64" \
    --userdata-file=./userdata.txt
</code></pre><p><span>In the meanwhile, we can watch the logs to see how things are:</span></p><pre class="language-tmate"><code class="language-tmate">  kubectl -n matchbox logs deployment/matchbox -f --tail=100
</code></pre><p><span>Looking at the logs, there should be some get requests of resources that will be used to boot the OS.</span></p><p><span>Notes:</span></p><ul><li><p><span>fun fact: you can run Matchbox on Android using </span><a class="firn-external" href="https://f-droid.org/en/packages/com.termux/" target="_blank">Termux</a><span>.</span></p></li></ul></section></div></div><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="the-cluster"><span class="firn-headline-text"><span>The cluster</span></span></h1><div class="firn-headline-section firn-headline-section-2"><h2 class="firn-headline firn-headline-2" id="preparing-the-cluster"><span class="firn-headline-text"><span>Preparing the cluster</span></span></h2><section><p><span>Here we will declare the template that we will shortly generate our usable cluster from:</span></p><pre class="language-yaml"><code class="language-yaml">  kind: TalosControlPlane
  apiVersion: controlplane.cluster.x-k8s.io/v1alpha3
  metadata:
    name: "${CLUSTER_NAME}-control-plane"
  spec:
    version: ${KUBERNETES_VERSION}
    replicas: ${CONTROL_PLANE_MACHINE_COUNT}
    infrastructureTemplate:
      apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
      kind: PacketMachineTemplate
      name: "${CLUSTER_NAME}-control-plane"
    controlPlaneConfig:
      init:
        generateType: init
        configPatches:
          - op: replace
            path: /machine/install
            value:
              disk: /dev/sda
              image: ghcr.io/talos-systems/installer:v0.8.1
              bootloader: true
              wipe: false
              force: false
          - op: add
            path: /machine/kubelet/extraArgs
            value:
              cloud-provider: external
          - op: add
            path: /cluster/apiServer/extraArgs
            value:
              cloud-provider: external
          - op: add
            path: /cluster/controllerManager/extraArgs
            value:
              cloud-provider: external
          - op: add
            path: /cluster/extraManifests
            value:
            - https://github.com/packethost/packet-ccm/releases/download/v1.1.0/deployment.yaml
          - op: add
            path: /cluster/allowSchedulingOnMasters
            value: true
      controlplane:
        generateType: controlplane
        configPatches:
          - op: replace
            path: /machine/install
            value:
              disk: /dev/sda
              image: ghcr.io/talos-systems/installer:v0.8.1
              bootloader: true
              wipe: false
              force: false
          - op: add
            path: /machine/kubelet/extraArgs
            value:
              cloud-provider: external
          - op: add
            path: /cluster/apiServer/extraArgs
            value:
              cloud-provider: external
          - op: add
            path: /cluster/controllerManager/extraArgs
            value:
              cloud-provider: external
          - op: add
            path: /cluster/allowSchedulingOnMasters
            value: true
  ---
  apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
  kind: PacketMachineTemplate
  metadata:
    name: "${CLUSTER_NAME}-control-plane"
  spec:
    template:
      spec:
        OS: custom_ipxe
        ipxeURL: "http://${IPXE_SERVER_IP}:8080/ipxe?arch=amd64"
        billingCycle: hourly
        machineType: "${CONTROLPLANE_NODE_TYPE}"
        sshKeys:
          - "${SSH_KEY}"
        tags: []
  ---
  apiVersion: cluster.x-k8s.io/v1alpha3
  kind: Cluster
  metadata:
    name: "${CLUSTER_NAME}"
  spec:
    clusterNetwork:
      pods:
        cidrBlocks:
          - ${POD_CIDR:=192.168.0.0/16}
      services:
        cidrBlocks:
          - ${SERVICE_CIDR:=172.26.0.0/16}
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
      kind: PacketCluster
      name: "${CLUSTER_NAME}"
    controlPlaneRef:
      apiVersion: controlplane.cluster.x-k8s.io/v1alpha3
      kind: TalosControlPlane
      name: "${CLUSTER_NAME}-control-plane"
  ---
  apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
  kind: PacketCluster
  metadata:
    name: "${CLUSTER_NAME}"
  spec:
    projectID: "${PACKET_PROJECT_ID}"
    facility: "${FACILITY}"
  ---
  apiVersion: cluster.x-k8s.io/v1alpha3
  kind: MachineDeployment
  metadata:
    name: ${CLUSTER_NAME}-worker-a
    labels:
      cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
      pool: worker-a
  spec:
    replicas: ${WORKER_MACHINE_COUNT}
    clusterName: ${CLUSTER_NAME}
    selector:
      matchLabels:
        cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
        pool: worker-a
    template:
      metadata:
        labels:
          cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
          pool: worker-a
      spec:
        version: ${KUBERNETES_VERSION}
        clusterName: ${CLUSTER_NAME}
        bootstrap:
          configRef:
            name: ${CLUSTER_NAME}-worker-a
            apiVersion: bootstrap.cluster.x-k8s.io/v1alpha3
            kind: TalosConfigTemplate
        infrastructureRef:
          name: ${CLUSTER_NAME}-worker-a
          apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
          kind: PacketMachineTemplate
  ---
  apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
  kind: PacketMachineTemplate
  metadata:
    name: ${CLUSTER_NAME}-worker-a
  spec:
    template:
      spec:
        OS: custom_ipxe
        ipxeURL: "http://${IPXE_SERVER_IP}:8080/ipxe?arch=amd64"
        billingCycle: hourly
        machineType: "${WORKER_NODE_TYPE}"
        sshKeys:
          - "${SSH_KEY}"
        tags: []
  ---
  apiVersion: bootstrap.cluster.x-k8s.io/v1alpha3
  kind: TalosConfigTemplate
  metadata:
    name: ${CLUSTER_NAME}-worker-a
    labels:
      cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
  spec:
    template:
      spec:
        generateType: init
</code></pre><p><span>Inside of </span><code>TalosControlPlane.spec.controlPlaneConfig.init</code><span>, I'm very much liking the use of </span><code>generateType: init</code><span> paired with </span><code>configPatches</code><span>. This enables:</span></p><ul><li><p><span>configuration to be generated;</span></p></li><li><p><span>management of certificates out of the cluster operator's hands;</span></p></li><li><p><span>another level of standardisation; and</span></p></li><li><p><span>overrides to be added where needed</span></p></li></ul><p><span>Notes:</span></p><ul><li><p><span>the ClusterAPI template above uses Packet-Cloud-Controller manager version 1.1.0</span></p></li></ul></section><div class="firn-headline-section firn-headline-section-3"><h3 class="firn-headline firn-headline-3" id="templating-your-configuration"><span class="firn-headline-text"><span>Templating your configuration</span></span></h3><section><p><span>Set environment variables for configuration:</span></p><pre class="language-bash"><code class="language-bash">  <<cluster-config-env-name>>
  export FACILITY=sjc1
  export KUBERNETES_VERSION=v1.20.2
  export POD_CIDR=10.244.0.0/16
  export SERVICE_CIDR=10.96.0.0/12
  export CONTROLPLANE_NODE_TYPE=c1.small.x86
  export CONTROL_PLANE_MACHINE_COUNT=3
  export WORKER_NODE_TYPE=c1.small.x86
  export WORKER_MACHINE_COUNT=0
  export SSH_KEY=""
  export IPXE_URL=$LOAD_BALANCER_IP
</code></pre><p><span>In the variables above, we will create a cluster which has three small controlPlane nodes to run workloads.</span></p></section></div><div class="firn-headline-section firn-headline-section-3"><h3 class="firn-headline firn-headline-3" id="render-the-manifests"><span class="firn-headline-text"><span>Render the manifests</span></span></h3><section><p><span>Render your cluster configuration from the template:</span></p><pre class="language-tmate"><code class="language-tmate">  clusterctl config cluster "$CLUSTER_NAME" \
    --from ./talos-packet-cluster-template.yaml \
    -n "$CLUSTER_NAME" > "$CLUSTER_NAME"-cluster-capi.yaml
</code></pre></section></div></div><div class="firn-headline-section firn-headline-section-2"><h2 class="firn-headline firn-headline-2" id="creating-the-cluster"><span class="firn-headline-text"><span>Creating the cluster</span></span></h2><section><p><span>With the template for the cluster rendered to how wish to deploy it, it's now time to apply it:</span></p><pre class="language-tmate"><code class="language-tmate">  kubectl create ns "$CLUSTER_NAME"
  kubectl -n "$CLUSTER_NAME" apply -f ./"$CLUSTER_NAME"-cluster-capi.yaml
</code></pre><p><span>The cluster will now be brought up, we can see the progress by taking a look at the resources:</span></p><pre class="language-tmate"><code class="language-tmate">  kubectl -n "$CLUSTER_NAME" get machines,clusters,packetmachines,packetclusters
</code></pre><p><span>Note: As expected, the cluster may take some time to appear and be accessible.</span></p><p><span>Not long after applying, a KubeConfig is available. Fetch the KubeConfig from the existing cluster with:</span></p><pre class="language-tmate"><code class="language-tmate">  kubectl -n "$CLUSTER_NAME" get secrets \
    "$CLUSTER_NAME"-kubeconfig -o=jsonpath='{.data.value}' \
    | base64 -d > $HOME/.kube/"$CLUSTER_NAME"
</code></pre><p><span>Using the KubeConfig from the new cluster, check out the status of it:</span></p><pre class="language-tmate"><code class="language-tmate">  kubectl --kubeconfig $HOME/.kube/"$CLUSTER_NAME" cluster-info
</code></pre><p><span>Once the APIServer is reachable, create configuration for how the Packet-Cloud-Controller-Manager should talk to Equinix-Metal:</span></p><pre class="language-tmate"><code class="language-tmate">  kubectl --kubeconfig $HOME/.kube/"$CLUSTER_NAME" -n kube-system \
    create secret generic packet-cloud-config \
    --from-literal=cloud-sa.json="{\"apiKey\": \"${PACKET_API_KEY}\",\"projectID\": \"${PACKET_PROJECT_ID}\"}"
</code></pre><p><span>Since we're able to talk to the APIServer, we can check how all Pods are doing:</span></p><pre class="language-bash"><code class="language-bash">  <<cluster-config-env-name>>
  kubectl --kubeconfig $HOME/.kube/"$CLUSTER_NAME"\
    -n kube-system get pods
</code></pre><p><span>Listing Pods shows that everything is live and in a good state:</span></p><pre class="language-bash"><code class="language-bash">NAMESPACE     NAME                                                     READY   STATUS    RESTARTS   AGE
kube-system   coredns-5b55f9f688-fb2cb                                 1/1     Running   0          25m
kube-system   coredns-5b55f9f688-qsvg5                                 1/1     Running   0          25m
kube-system   kube-apiserver-665px                                     1/1     Running   0          19m
kube-system   kube-apiserver-mz68q                                     1/1     Running   0          19m
kube-system   kube-apiserver-qfklt                                     1/1     Running   2          19m
kube-system   kube-controller-manager-6grxd                            1/1     Running   0          19m
kube-system   kube-controller-manager-cf76h                            1/1     Running   0          19m
kube-system   kube-controller-manager-dsmgf                            1/1     Running   0          19m
kube-system   kube-flannel-brdxw                                       1/1     Running   0          24m
kube-system   kube-flannel-dm85d                                       1/1     Running   0          24m
kube-system   kube-flannel-sg6k9                                       1/1     Running   0          24m
kube-system   kube-proxy-flx59                                         1/1     Running   0          24m
kube-system   kube-proxy-gbn4l                                         1/1     Running   0          24m
kube-system   kube-proxy-ns84v                                         1/1     Running   0          24m
kube-system   kube-scheduler-4qhjw                                     1/1     Running   0          19m
kube-system   kube-scheduler-kbm5z                                     1/1     Running   0          19m
kube-system   kube-scheduler-klsmp                                     1/1     Running   0          19m
kube-system   packet-cloud-controller-manager-77cd8c9c7c-cdzfv         1/1     Running   0          20m
kube-system   pod-checkpointer-4szh6                                   1/1     Running   0          19m
kube-system   pod-checkpointer-4szh6-talos-metal-control-plane-j29lb   1/1     Running   0          19m
kube-system   pod-checkpointer-k7w8h                                   1/1     Running   0          19m
kube-system   pod-checkpointer-k7w8h-talos-metal-control-plane-lk9f2   1/1     Running   0          19m
kube-system   pod-checkpointer-m5wrh                                   1/1     Running   0          19m
kube-system   pod-checkpointer-m5wrh-talos-metal-control-plane-h9v4j   1/1     Running   0          19m
</code></pre><p><span>With the cluster live, it's now ready for workloads to be deployed!</span></p></section></div></div><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="talos-configuration"><span class="firn-headline-text"><span>Talos Configuration</span></span></h1><section><p><span>In order to manage Talos Nodes outside of Kubernetes, we need to create and set up configuration to use.</span></p><p><span>Create the directory for the config:</span></p><pre class="language-tmate"><code class="language-tmate">  mkdir -p $HOME/.talos
</code></pre><p><span>Discover the IP for the first controlPlane:</span></p><pre class="language-tmate"><code class="language-tmate">  export TALOS_ENDPOINT=$(kubectl -n "$CLUSTER_NAME" \
    get machines \
    $(kubectl -n "$CLUSTER_NAME" \
      get machines -l cluster.x-k8s.io/control-plane='' \
      --no-headers --output=jsonpath='{.items[0].metadata.name}') \
      -o=jsonpath="{.status.addresses[?(@.type=='ExternalIP')].address}" | awk '{print $2}')
</code></pre><p><span>Fetch the </span><code>talosconfig</code><span> from the existing cluster:</span></p><pre class="language-tmate"><code class="language-tmate">  kubectl get talosconfig \
    -n $CLUSTER_NAME \
    -l cluster.x-k8s.io/cluster-name=$CLUSTER_NAME \
    -o yaml -o jsonpath='{.items[0].status.talosConfig}' > $HOME/.talos/"$CLUSTER_NAME"-management-plane-talosconfig.yaml
</code></pre><p><span>Write in the configuration the endpoint IP and node IP:</span></p><pre class="language-tmate"><code class="language-tmate">  talosctl \
    --talosconfig $HOME/.talos/"$CLUSTER_NAME"-management-plane-talosconfig.yaml \
    config endpoint $TALOS_ENDPOINT
  talosctl \
    --talosconfig $HOME/.talos/"$CLUSTER_NAME"-management-plane-talosconfig.yaml \
    config node $TALOS_ENDPOINT
</code></pre><p><span>Now that the </span><code>talosconfig</code><span> has been written, try listing all containers:</span></p><pre class="language-bash"><code class="language-bash">  <<cluster-config-env-name>>
  # removing ip; omit ` | sed ...` for regular use
  talosctl --talosconfig $HOME/.talos/"$CLUSTER_NAME"-management-plane-talosconfig.yaml containers | sed -r 's/(\b[0-9]{1,3}\.){3}[0-9]{1,3}\b'/"x.x.x.x      "/
</code></pre><p><span>Here's the containers running on this particular node, in containerd (not k8s related):</span></p><pre class="language-bash"><code class="language-bash">NODE            NAMESPACE   ID         IMAGE                                  PID    STATUS
x.x.x.x         system      apid       talos/apid                             3046   RUNNING
x.x.x.x         system      etcd       gcr.io/etcd-development/etcd:v3.4.14   3130   RUNNING
x.x.x.x         system      networkd   talos/networkd                         2879   RUNNING
x.x.x.x         system      routerd    talos/routerd                          2888   RUNNING
x.x.x.x         system      timed      talos/timed                            2976   RUNNING
x.x.x.x         system      trustd     talos/trustd                           3047   RUNNING
</code></pre></section></div><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="clean-up"><span class="firn-headline-text"><span>Clean up</span></span></h1><section><p><span>Tearing down the entire cluster and resources associated with it, can be achieved by</span></p><p><span>i. Deleting the cluster:</span></p><pre class="language-tmate"><code class="language-tmate">  kubectl -n "$CLUSTER_NAME" delete cluster "$CLUSTER_NAME"
</code></pre><p><span>ii. Deleting the namespace:</span></p><pre class="language-tmate"><code class="language-tmate">  kubectl delete ns "$CLUSTER_NAME"
</code></pre><p><span>iii. Removing local configurations:</span></p><pre class="language-tmate"><code class="language-tmate">  rm \
    $HOME/.talos/"$CLUSTER_NAME"-management-plane-talosconfig.yaml \
    $HOME/.kube/"$CLUSTER_NAME"
</code></pre></section></div><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="what-have-i-learned-from-this?"><span class="firn-headline-text"><span>What have I learned from this?</span></span></h1><section><ul><li><p><span>(always learning) how wonderful the Kubernetes community is :: there are so many knowledgable individuals who are so ready for collaboration and adoption - it doesn't matter the SIG or group.</span></p></li><li><p><span>how modular Cluster-API is :: Cluster-API components (bootstrap, controlPlane, core, infrastructure) can be swapped out and meshed together in very cool ways.</span></p></li></ul></section></div><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="credits"><span class="firn-headline-text"><span>Credits</span></span></h1><section><p><span>Integrating Talos into this project would not be possible without help from </span><a class="firn-external" href="https://github.com/andrewrynhard" target="_blank">Andrew Rynhard (Talos Systems)</a><span>, huge thanks to him for reaching out for pairing and co-authoring.</span></p></section></div><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="notes-and-references"><span class="firn-headline-text"><span>Notes and references</span></span></h1><section><ul><li><p><span>with the new cluster's controlPlane live and available for deployment, the iPXE server could be moved into that cluster - meaning that new servers boot from the cluster that they'll join, making it almost self-contained</span></p></li><li><p><span>cluster configuration as based off of </span><a class="firn-external" href="https://github.com/kubernetes-sigs/cluster-api-provider-packet/blob/479faf06e1337b1e979cb624ca8be015b2a89cde/templates/cluster-template.yaml" target="_blank">cluster-template.yaml from the cluster-api-provider-packet repo</a></p></li><li><p><span>this post has been made to </span><a class="firn-external" href="https://blog.calebwoodbine.com/deploying-talos-and-kubernetes-with-cluster-api-on-equinix-metal" target="_blank">blog.calebwoodine.com</a><span>, and </span><a class="firn-external" href="https://ii.coop/deploying-talos-and-kubernetes-with-cluster-api-on-equinix-metal/" target="_blank">talos-system.com/blog</a><span>, but is also available as an </span><a class="firn-external" href="https://github.com/ii/org/blob/master/ii/equinix-metal-capi-talos-kubernetes/README.org" target="_blank">Org file</a></p></li></ul><hr /><p><span>Hope you've enjoyed the output of this project!
Thank you!</span></p></section></div><div class="firn-headline-section firn-headline-section-1"><h1 class="firn-headline firn-headline-1" id="footnotes"><span class="firn-headline-text"><span>Footnotes</span></span></h1><section></section></div></div></div></article><section id="recent"><h2>Recent Posts</h2><ul><li><a class="title" href="deploying-talos-to-equinix">Cluster-API + Talos + Equinix Metal</a><p>A guide to  launching a highly-available cluster with Equinix and Talos</p><p><em>2021-01-21</em></p></li><li><a class="title" href="working-with-orgmode-tables">Working with Org-Mode Tables: Basics</a><p>An introduction to how to create/edit tables with Org-Mode.</p><p><em>2019-03-09</em></p></li></ul></section></main></body></html>