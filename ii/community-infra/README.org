#+TITLE: Community Infra

#+begin_quote
ii's local Bay of Plenty community infrastucture for learning, developing, and pairing with locals.
#+end_quote

* Plan
Provide local infrastructure, through servers that run Kubernetes; For an automated, reproducable, and accessible way to those in the local community.

We will begin using Kubeadm for setting up Kubernetes, but will end up using Talos as the way to bring up Kubernetes.

* Network
There are currently three servers installed, their IPs are:
- 10.8.11.201
- 10.8.11.202
- 10.8.11.203

* Set up Kubernetes
** Prepare
The set of commands to prepare the Ubuntu installs for Kubernetes
#+begin_src shell :tangle ./preKubeadmCommands.sh
#!/bin/bash
KUBERNETES_VERSION='1.21.2'

PACKAGES=(
  apt-transport-https
  ca-certificates
  cloud-utils
  containerd
  dnsutils
  ebtables
  gettext-base
  git
  jq
  kitty-terminfo
  prips
  socat
)

pwd
cd $(dirname $0)

# ensure mounts
sed -ri '/\\sswap\\s/s/^#?/#/' /etc/fstab
swapoff -a
mount -a

# install required packages
apt-get -y update
DEBIAN_FRONTEND=noninteractive apt-get install -y apt-transport-https curl software-properties-common
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" > /etc/apt/sources.list.d/kubernetes.list
apt-get update -y
TRIMMED_KUBERNETES_VERSION=$(echo $KUBERNETES_VERSION | sed 's/\./\\./g' | sed 's/^v//')
RESOLVED_KUBERNETES_VERSION=$(apt-cache policy kubelet | awk -v VERSION=${TRIMMED_KUBERNETES_VERSION} '$1~ VERSION { print $1 }' | head -n1)
apt-get install -y ${PACKAGES[*]} \
  kubelet=${RESOLVED_KUBERNETES_VERSION} \
  kubeadm=${RESOLVED_KUBERNETES_VERSION} \
  kubectl=${RESOLVED_KUBERNETES_VERSION}
systemctl daemon-reload

# configure container runtime
cat <<EOF | tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
modprobe overlay
modprobe br_netfilter
mkdir -p /etc/containerd
rm /etc/containerd/config.toml
systemctl restart containerd
systemctl enable --now containerd
export CONTAINER_RUNTIME_ENDPOINT=/var/run/containerd/containerd.sock
echo $HOME
export HOME=$(getent passwd $(id -u) | cut -d ':' -f6)

# configure sysctls for Kubernetes
cat <<EOF | tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
sysctl --system
systemctl enable --now systemd-resolved
systemctl disable snapd.service snapd.socket
#+end_src

Copy the script
#+begin_src tmate :window community-infra
for i in {1..3}; do
  scp -P 2222 ./preKubeadmCommands.sh root@10.8.11.20${i}:~
done
#+end_src

PROCEED WITH CAUTION: Scrub drives
#+begin_src tmate :window community-infra
for NODE in $(kubectl get nodes -o=jsonpath='{.items[*].metadata.name}'); do
    NODE_INTERNAL_IP=$(kubectl get node "${NODE}" -o=jsonpath='{.status.addresses[?(@.type == "InternalIP")].address}')
    for DEV in {b..f}; do
        echo "${NODE} (${NODE_INTERNAL_IP}) /dev/sd${DEV}"
        ssh -p 2222 root@${NODE_INTERNAL_IP} env DEV=$DEV bash << \EOF
set -x
hostname
rm -rf /var/lib/rook
for VG in $(lvs | grep osd | awk  '{print $2}'); do
    lvm vgremove $VG --force
done
lvs
wipefs --all /dev/sd${DEV}
lsblk -f /dev/sd${DEV}
EOF
    done
done
#+end_src

** Install
*** Srv1
Prepare
#+begin_src tmate :window community-infra
ssh root@10.8.11.202 -p 2222 bash -x ./preKubeadmCommands.sh
#+end_src

Init
#+begin_src tmate :window community-infra
ssh root@10.8.11.202 -p 2222 kubeadm init \
    --pod-network-cidr 10.244.0.0/16 \
    --service-cidr 10.96.0.0/12
#+end_src

** Export variables
Get the CA cert hash from the first server
#+begin_src tmate :window community-infra
export CA_CERT_HASH=$(ssh root@10.8.11.201 -p 2222 openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //')
#+end_src

#+begin_src tmate :window community-infra
export JOIN_TOKEN=$(ssh root@10.8.11.201 -p 2222 kubeadm token list -o=jsonpath='{.token}')
#+end_src

** Srv2
Prepare
#+begin_src tmate :window community-infra
ssh root@10.8.11.202 -p 2222 bash -x ./preKubeadmCommands.sh
#+end_src

Join
#+begin_src tmate :window community-infra
ssh root@10.8.11.202 -p 2222 kubeadm join 10.8.11.201:6443 \
    --token "${JOIN_TOKEN}" \
    --discovery-token-ca-cert-hash "sha256:${CA_CERT_HASH}"
#+end_src

** Srv3
Prepare
#+begin_src tmate :window community-infra
ssh root@10.8.11.203 -p 2222 bash -x ./preKubeadmCommands.sh
#+end_src

Init
#+begin_src tmate :window community-infra
ssh root@10.8.11.203 -p 2222 \
    kubeadm join 10.8.11.201:6443 \
    --token "${JOIN_TOKEN}" \
    --discovery-token-ca-cert-hash "sha256:${CA_CERT_HASH}"
#+end_src

* Finalising
** Get the Kubeconfig
#+begin_src tmate :window community-infra
ssh root@10.8.11.201 -p 2222 cat /etc/kubernetes/admin.conf > ~/.kube/config-ii-community-infra
#+end_src

** Use the Kubeconfig
#+begin_src tmate :window community-infra
export KUBECONFIG=~/.kube/config-ii-community-infra
#+end_src

** Install a CNI
Prepare cilium
#+begin_src shell :results silent
helm repo add cilium https://helm.cilium.io/
helm template cilium cilium/cilium --version 1.10.4 \
  --namespace kube-system > ./cilium.yaml
#+end_src

Install cilium
#+begin_src tmate :window community-infra
kubectl apply -f cilium.yaml
#+end_src

** Untaint master for scheduling
#+begin_src tmate :window community-infra
kubectl taint node --all node-role.kubernetes.io/master-
#+end_src

* Installing core services
** Rook+Ceph
*** Prepare
#+begin_src tmate :dir . :window community-infra
kubectl create ns rook-ceph --dry-run=client -o yaml | \
    kubectl apply -f -

curl -s -L -o ./rook-ceph-common.yaml https://github.com/rook/rook/raw/v1.7.2/cluster/examples/kubernetes/ceph/common.yaml
curl -s -L -o ./rook-ceph-crds.yaml https://github.com/rook/rook/raw/v1.7.2/cluster/examples/kubernetes/ceph/crds.yaml
curl -s -L -o ./rook-ceph-operator.yaml https://github.com/rook/rook/raw/v1.7.2/cluster/examples/kubernetes/ceph/operator.yaml
#+end_src

*** Configure
#+begin_src yaml :tangle ./rook-ceph-cluster.yaml
#################################################################################################################
# Define the settings for the rook-ceph cluster with common settings for a production cluster.
# All nodes with available raw devices will be used for the Ceph cluster. At least three nodes are required
# in this example. See the documentation for more details on storage settings available.

# For example, to create the cluster:
#   kubectl create -f crds.yaml -f common.yaml -f operator.yaml
#   kubectl create -f cluster.yaml
#################################################################################################################

apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph # namespace:cluster
spec:
  cephVersion:
    # The container image used to launch the Ceph daemon pods (mon, mgr, osd, mds, rgw).
    # v13 is mimic, v14 is nautilus, and v15 is octopus.
    # RECOMMENDATION: In production, use a specific version tag instead of the general v14 flag, which pulls the latest release and could result in different
    # versions running within the cluster. See tags available at https://hub.docker.com/r/ceph/ceph/tags/.
    # If you want to be more precise, you can always use a timestamp tag such ceph/ceph:v15.2.8-20201217
    # This tag might not contain a new Ceph version, just security fixes from the underlying operating system, which will reduce vulnerabilities
    image: ceph/ceph:v16.2.5
    # Whether to allow unsupported versions of Ceph. Currently `nautilus` and `octopus` are supported.
    # Future versions such as `pacific` would require this to be set to `true`.
    # Do not set to true in production.
    allowUnsupported: false
  # The path on the host where configuration files will be persisted. Must be specified.
  # Important: if you reinstall the cluster, make sure you delete this directory from each host or else the mons will fail to start on the new cluster.
  # In Minikube, the '/data' directory is configured to persist across reboots. Use "/data/rook" in Minikube environment.
  dataDirHostPath: /var/lib/rook
  # Whether or not upgrade should continue even if a check fails
  # This means Ceph's status could be degraded and we don't recommend upgrading but you might decide otherwise
  # Use at your OWN risk
  # To understand Rook's upgrade process of Ceph, read https://rook.io/docs/rook/master/ceph-upgrade.html#ceph-version-upgrades
  skipUpgradeChecks: false
  # Whether or not continue if PGs are not clean during an upgrade
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  mon:
    # Set the number of mons to be started. Must be an odd number, and is generally recommended to be 3.
    count: 3
    # The mons should be on unique nodes. For production, at least 3 nodes are recommended for this reason.
    # Mons should only be allowed on the same node for test environments where data loss is acceptable.
    allowMultiplePerNode: false
  mgr:
    modules:
    # Several modules should not need to be included in this list. The "dashboard" and "monitoring" modules
    # are already enabled by other settings in the cluster CR.
    - name: pg_autoscaler
      enabled: true
  # enable the ceph dashboard for viewing cluster status
  dashboard:
    enabled: true
    # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
    # urlPrefix: /ceph-dashboard
    # serve the dashboard at the given port.
    # port: 8443
    # serve the dashboard using SSL
    ssl: true
  # enable prometheus alerting for cluster
  monitoring:
    # requires Prometheus to be pre-installed
    enabled: false
    # namespace to deploy prometheusRule in. If empty, namespace of the cluster will be used.
    # Recommended:
    # If you have a single rook-ceph cluster, set the rulesNamespace to the same namespace as the cluster or keep it empty.
    # If you have multiple rook-ceph clusters in the same k8s cluster, choose the same namespace (ideally, namespace with prometheus
    # deployed) to set rulesNamespace for all the clusters. Otherwise, you will get duplicate alerts with multiple alert definitions.
    rulesNamespace: rook-ceph
  network:
    # enable host networking
    #provider: host
    # EXPERIMENTAL: enable the Multus network provider
    #provider: multus
    #selectors:
      # The selector keys are required to be `public` and `cluster`.
      # Based on the configuration, the operator will do the following:
      #   1. if only the `public` selector key is specified both public_network and cluster_network Ceph settings will listen on that interface
      #   2. if both `public` and `cluster` selector keys are specified the first one will point to 'public_network' flag and the second one to 'cluster_network'
      #
      # In order to work, each selector value must match a NetworkAttachmentDefinition object in Multus
      #
      #public: public-conf --> NetworkAttachmentDefinition object name in Multus
      #cluster: cluster-conf --> NetworkAttachmentDefinition object name in Multus
    # Provide internet protocol version. IPv6, IPv4 or empty string are valid options. Empty string would mean IPv4
    #ipFamily: "IPv6"
  # enable the crash collector for ceph daemon crash collection
  crashCollector:
    disable: false
  # enable log collector, daemons will log on files and rotate
  # logCollector:
  #   enabled: true
  #   periodicity: 24h # SUFFIX may be 'h' for hours or 'd' for days.
  # automate [data cleanup process](https://github.com/rook/rook/blob/master/Documentation/ceph-teardown.md#delete-the-data-on-hosts) in cluster destruction.
  cleanupPolicy:
    # Since cluster cleanup is destructive to data, confirmation is required.
    # To destroy all Rook data on hosts during uninstall, confirmation must be set to "yes-really-destroy-data".
    # This value should only be set when the cluster is about to be deleted. After the confirmation is set,
    # Rook will immediately stop configuring the cluster and only wait for the delete command.
    # If the empty string is set, Rook will not destroy any data on hosts during uninstall.
    confirmation: ""
    # sanitizeDisks represents settings for sanitizing OSD disks on cluster deletion
    sanitizeDisks:
      # method indicates if the entire disk should be sanitized or simply ceph's metadata
      # in both case, re-install is possible
      # possible choices are 'complete' or 'quick' (default)
      method: quick
      # dataSource indicate where to get random bytes from to write on the disk
      # possible choices are 'zero' (default) or 'random'
      # using random sources will consume entropy from the system and will take much more time then the zero source
      dataSource: zero
      # iteration overwrite N times instead of the default (1)
      # takes an integer value
      iteration: 1
    # allowUninstallWithVolumes defines how the uninstall should be performed
    # If set to true, cephCluster deletion does not wait for the PVs to be deleted.
    allowUninstallWithVolumes: false
  # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
  # The example under 'all' would have all services scheduled on kubernetes nodes labeled with 'role=storage-node' and
  # tolerate taints with a key of 'storage-node'.
#  placement:
#    all:
#      nodeAffinity:
#        requiredDuringSchedulingIgnoredDuringExecution:
#          nodeSelectorTerms:
#          - matchExpressions:
#            - key: role
#              operator: In
#              values:
#              - storage-node
#      podAffinity:
#      podAntiAffinity:
#      topologySpreadConstraints:
#      tolerations:
#      - key: storage-node
#        operator: Exists
# The above placement information can also be specified for mon, osd, and mgr components
#    mon:
# Monitor deployments may contain an anti-affinity rule for avoiding monitor
# collocation on the same node. This is a required rule when host network is used
# or when AllowMultiplePerNode is false. Otherwise this anti-affinity rule is a
# preferred rule with weight: 50.
#    osd:
#    mgr:
#    cleanup:
  annotations:
#    all:
#    mon:
#    osd:
#    cleanup:
#    prepareosd:
# If no mgr annotations are set, prometheus scrape annotations will be set by default.
#    mgr:
  labels:
#    all:
#    mon:
#    osd:
#    cleanup:
#    mgr:
#    prepareosd:
  resources:
# The requests and limits set here, allow the mgr pod to use half of one CPU core and 1 gigabyte of memory
#    mgr:
#      limits:
#        cpu: "500m"
#        memory: "1024Mi"
#      requests:
#        cpu: "500m"
#        memory: "1024Mi"
# The above example requests/limits can also be added to the mon and osd components
#    mon:
#    osd:
#    prepareosd:
#    crashcollector:
#    logcollector:
#    cleanup:
  # The option to automatically remove OSDs that are out and are safe to destroy.
  removeOSDsIfOutAndSafeToRemove: false
#  priorityClassNames:
#    all: rook-ceph-default-priority-class
#    mon: rook-ceph-mon-priority-class
#    osd: rook-ceph-osd-priority-class
#    mgr: rook-ceph-mgr-priority-class
  storage: # cluster level storage configuration and selection
    useAllNodes: true
    useAllDevices: false
    deviceFilter: "^sd[bdef]"
    # config:
      # crushRoot: "custom-root" # specify a non-default root label for the CRUSH map
      # metadataDevice: "md0" # specify a non-rotational storage so ceph-volume will use it as block db device of bluestore.
      # databaseSizeMB: "1024" # uncomment if the disks are smaller than 100 GB
      # journalSizeMB: "1024"  # uncomment if the disks are 20 GB or smaller
      # osdsPerDevice: "1" # this value can be overridden at the node or device level
      # encryptedDevice: "true" # the default value for this option is "false"
# Individual nodes and their config can be specified as well, but 'useAllNodes' above must be set to false. Then, only the named
# nodes below will be used as storage resources.  Each node's 'name' field should match their 'kubernetes.io/hostname' label.
#    nodes:
#    - name: "172.17.4.201"
#      devices: # specific devices to use for storage can be specified for each node
#      - name: "sdb"
#      - name: "nvme01" # multiple osds can be created on high performance devices
#        config:
#          osdsPerDevice: "5"
#      - name: "/dev/disk/by-id/ata-ST4000DM004-XXXX" # devices can be specified using full udev paths
#      config: # configuration can be specified at the node level which overrides the cluster level config
#        storeType: filestore
#    - name: "172.17.4.301"
#      deviceFilter: "^sd."
  # The section for configuring management of daemon disruptions during upgrade or fencing.
  disruptionManagement:
    # If true, the operator will create and manage PodDisruptionBudgets for OSD, Mon, RGW, and MDS daemons. OSD PDBs are managed dynamically
    # via the strategy outlined in the [design](https://github.com/rook/rook/blob/master/design/ceph/ceph-managed-disruptionbudgets.md). The operator will
    # block eviction of OSDs by default and unblock them safely when drains are detected.
    managePodBudgets: false
    # A duration in minutes that determines how long an entire failureDomain like `region/zone/host` will be held in `noout` (in addition to the
    # default DOWN/OUT interval) when it is draining. This is only relevant when  `managePodBudgets` is `true`. The default value is `30` minutes.
    osdMaintenanceTimeout: 30
    # A duration in minutes that the operator will wait for the placement groups to become healthy (active+clean) after a drain was completed and OSDs came back up.
    # Operator will continue with the next drain if the timeout exceeds. It only works if `managePodBudgets` is `true`.
    # No values or 0 means that the operator will wait until the placement groups are healthy before unblocking the next drain.
    pgHealthCheckTimeout: 0
    # If true, the operator will create and manage MachineDisruptionBudgets to ensure OSDs are only fenced when the cluster is healthy.
    # Only available on OpenShift.
    manageMachineDisruptionBudgets: false
    # Namespace in which to watch for the MachineDisruptionBudgets.
    machineDisruptionBudgetNamespace: openshift-machine-api

  # healthChecks
  # Valid values for daemons are 'mon', 'osd', 'status'
  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s
    # Change pod liveness probe, it works for all mon,mgr,osd daemons
    livenessProbe:
      mon:
        disabled: false
      mgr:
        disabled: false
      osd:
        disabled: false
#+end_src
#+begin_src yaml :tangle ./rook-ceph-pool-storageclass.yaml
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  failureDomain: host
  replicated:
    size: 3
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: rook-ceph-block
   annotations:
     storageclass.kubernetes.io/is-default-class: "true"
# Change "rook-ceph" provisioner prefix to match the operator namespace if needed
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
    # clusterID is the namespace where the rook cluster is running
    clusterID: rook-ceph
    # Ceph pool into which the RBD image shall be created
    pool: replicapool

    # (optional) mapOptions is a comma-separated list of map options.
    # For krbd options refer
    # https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options
    # For nbd options refer
    # https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options
    # mapOptions: lock_on_read,queue_depth=1024

    # (optional) unmapOptions is a comma-separated list of unmap options.
    # For krbd options refer
    # https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options
    # For nbd options refer
    # https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options
    # unmapOptions: force

    # RBD image format. Defaults to "2".
    imageFormat: "2"

    # RBD image features. Available for imageFormat: "2". CSI RBD currently supports only `layering` feature.
    imageFeatures: layering

    # The secrets contain Ceph admin credentials.
    csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
    csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
    csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
    csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

    # Specify the filesystem type of the volume. If not specified, csi-provisioner
    # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
    # in hyperconverged settings where the volume is mounted on the same node as the osds.
    csi.storage.k8s.io/fstype: ext4

# Delete the rbd volume when a PVC is deleted
reclaimPolicy: Delete
#+end_src
#+begin_src yaml :tangle ./rook-ceph-dashboard.yaml
apiVersion: v1
kind: Service
metadata:
  name: rook-ceph-mgr-dashboard-external-https
  namespace: rook-ceph
  labels:
    app: rook-ceph-mgr
    rook_cluster: rook-ceph
spec:
  ports:
  - name: dashboard
    port: 8443
    protocol: TCP
    targetPort: 8443
  selector:
    app: rook-ceph-mgr
    rook_cluster: rook-ceph
  sessionAffinity: None
  type: NodePort
#+end_src

*** Install
Install the Operator
#+begin_src tmate :dir . :window community-infra
kubectl apply -f ./rook-ceph-crds.yaml -f ./rook-ceph-common.yaml -f ./rook-ceph-operator.yaml
#+end_src

Create a cluster
#+begin_src tmate :dir . :window community-infra
kubectl apply -f ./rook-ceph-cluster.yaml
#+end_src

Expose the cluster as a StorageClass
#+begin_src tmate :dir . :window community-infra
kubectl apply -f ./rook-ceph-pool-storageclass.yaml
#+end_src

Create an NodePort Service for the dashboard
#+begin_src tmate :dir . :window community-infra
kubectl apply -f ./rook-ceph-dashboard.yaml
#+end_src

Show the credentials and access point for the dashboard
#+begin_src tmate :dir . :window community-infra
echo https://$(kubectl get node -o wide $(kubectl -n rook-ceph get pod -o wide | grep mgr | awk '{print $7}') | awk '{print $6}' | tail -1):$(kubectl -n rook-ceph get svc rook-ceph-mgr-dashboard-external-https -o=jsonpath='{.spec.ports[0].nodePort}')
echo admin :: $(kubectl -n rook-ceph get secrets rook-ceph-dashboard-password -o=jsonpath='{.data.password}' | base64 -d)
#+end_src

*** Debug
Logs for rook-ceph-operator
#+begin_src tmate :dir . :window community-infra
kubectl -n rook-ceph logs -l app=rook-ceph-operator -f --tail=100
#+end_src

#+begin_src yaml :tangle ./rook-ceph-toolbox.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: rook-ceph-tools
  namespace: rook-ceph
  labels:
    app: rook-ceph-tools
spec:
  selector:
    matchLabels:
      app: rook-ceph-tools
  template:
    metadata:
      labels:
        app: rook-ceph-tools
    spec:
      dnsPolicy: ClusterFirstWithHostNet
      containers:
      - name: rook-ceph-tools
        image: rook/ceph:v1.7.2
        command: ["/tini"]
        args: ["-g", "--", "/usr/local/bin/toolbox.sh"]
        imagePullPolicy: IfNotPresent
        env:
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                name: rook-ceph-mon
                key: ceph-username
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                name: rook-ceph-mon
                key: ceph-secret
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
        volumeMounts:
          - mountPath: /etc/ceph
            name: ceph-config
          - name: mon-endpoint-volume
            mountPath: /etc/rook
      volumes:
        - name: mon-endpoint-volume
          configMap:
            name: rook-ceph-mon-endpoints
            items:
            - key: data
              path: mon-endpoints
        - name: ceph-config
          emptyDir: {}
      tolerations:
        - key: "node.kubernetes.io/unreachable"
          operator: "Exists"
          effect: "NoExecute"
          tolerationSeconds: 5
#+end_src

#+begin_src tmate :dir . :window community-infra
kubectl apply -f ./rook-ceph-toolbox.yaml
#+end_src

#+begin_src tmate :dir . :window community-infra
kubectl -n rook-ceph exec -it daemonset/rook-ceph-tools -- bash
#+end_src

#+begin_src yaml :tangle ./rook-ceph-pvc-test.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rook-ceph-pvc-test
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 500Gi
  storageClassName: rook-ceph-block
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rook-ceph-pvc-test
spec:
  strategy:
    type: Recreate
  selector:
    matchLabels:
      name: rook-ceph-pvc-test
  template:
    metadata:
      labels:
        name: rook-ceph-pvc-test
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: name
                    operator: In
                    values:
                      - rook-ceph-pvc-test
              topologyKey: "kubernetes.io/hostname"
      containers:
        - name: rook-ceph-pvc-test
          image: alpine:3.12
          command:
            - sleep
            - infinity
          volumeMounts:
            - name: rook-ceph-pvc-test
              mountPath: /mnt
      volumes:
        - name: rook-ceph-pvc-test
          persistentVolumeClaim:
            claimName: rook-ceph-pvc-test
#+end_src

Add the test PVC and Deployment
#+begin_src tmate :dir . :window community-infra
kubectl -n default apply -f ./rook-ceph-pvc-test.yaml
#+end_src

Schedule to a new node
#+begin_src tmate :dir . :window community-infra
kubectl -n default rollout restart deployment rook-ceph-pvc-test
#+end_src

Get a shell
#+begin_src tmate :dir . :window community-infra
kubectl -n default exec -it deployment/rook-ceph-pvc-test -- sh
#+end_src

#+begin_src yaml :tangle ./rook-ceph-pvc-shared-test.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rook-ceph-pvc-shared-test
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 500Gi
  storageClassName: rook-ceph-shared
#+end_src

#+begin_src tmate :dir . :window community-infra
kubectl -n default apply -f ./rook-ceph-pvc-shared-test.yaml
#+end_src

#+begin_src tmate :dir . :window community-infra
kubectl -n default describe pvc rook-ceph-pvc-shared-test
#+end_src

** Helm-Operator
*** Prepare
#+begin_src tmate :window community-infra
kubectl create namespace helm-operator --dry-run=client -o yaml | \
    kubectl apply -f -
#+end_src
*** Install
#+begin_src tmate :window community-infra
kubectl apply \
    -f https://github.com/sharingio/.sharing.io/raw/main/cluster-api/manifests/helm-operator-crds.yaml \
    -f https://github.com/sharingio/.sharing.io/raw/main/cluster-api/manifests/helm-operator.yaml
#+end_src
* Install apps
** Humacs
*** Prepare
#+begin_src tmate :window community-infra
kubectl create namespace humacs --dry-run=client -o yaml | \
    kubectl apply -f -
#+end_src

*** Configure
#+begin_src yaml :tangle ./humacs.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: humacs-home-ii
  namespace: humacs
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Gi
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: humacs
  namespace: humacs
spec:
  releaseName: humacs
  chart:
    git: https://github.com/humacs/humacs
    ref: eaf562e067faa086d3165aba659fa52b727662d8
    path: chart/humacs
  values:
    initContainers:
      - name: humacs-home-ii-fix-permissions
        image: alpine:3.12
        command:
          - sh
          - -c
          - chown 1000:1000 -R /home/ii && chown 1000 /run/containerd/containerd.sock
        volumeMounts:
          - mountPath: /home/ii
            name: home-ii
          - name: run-containerd-containerd-sock
            mountPath: /run/containerd/containerd.sock
    image:
      repository: registry.gitlab.com/humacs/humacs/ii
      tag: 2021.09.10.1346
    options:
      hostDockerSocket: false
      hostTmp: true
      timezone: Pacific/Auckland
      gitName: CloudNative.NZ
      gitEmail: cloudnativenz-humacs-test@ii.coop
      profile: ""
      repos:
        - https://github.com/ii/org
        - https://gitlab.com/ii/nz
      preinitScript: |
        git clone "https://github.com/sharingio/.sharing.io" || \
          git clone https://github.com/sharingio/.sharing.io
        . /home/ii/.sharing.io/sharingio-pair-preinit-script.sh
    extraEnvVars:
      - name: SHARINGIO_PAIR_NAME
        value: "community-infra"
      - name: SHARINGIO_PAIR_USER
        value: "$SHARINGIO_PAIR_INSTANCE_SETUP_USER"
      - name: SHARINGIO_PAIR_GUEST_NAMES
      - name: SHARINGIO_PAIR_KUBERNETES_CONTROLPLANE_ENDPOINT
      - name: SHARINGIO_PAIR_LOAD_BALANCER_IP
      - name: HUMACS_DEBUG
        value: "true"
      - name: REINIT_HOME_FOLDER
        value: "true"
      - name: SHARINGIO_PAIR_BASE_DNS_NAME
        value: "$SHARINGIO_PAIR_INSTANCE_SETUP_BASEDNSNAME"
      - name: GITHUB_TOKEN
        value: "$SHARINGIO_PAIR_INSTANCE_SETUP_GITHUBOAUTHTOKEN"
      - name: CONTAINER_RUNTIME_ENDPOINT
        value: unix:///run/containerd/containerd.sock
      - name: CONTAINER_ADDRESS
        value: /run/containerd/containerd.sock
      - name: CONTAINERD_NAMESPACE
        value: k8s.io
      - name: K8S_NODE
        valueFrom:
          fieldRef:
            fieldPath: spec.nodeName
    extraVolumes:
      - name: home-ii
        hostPath:
          path: /home/ii
      - name: host
        hostPath:
          path: /
      - name: run-containerd-containerd-sock
        hostPath:
          path: /run/containerd/containerd.sock
    extraVolumeMounts:
      - name: home-ii
        mountPath: /home/ii
      - name: host
        mountPath: /var/run/host
#+end_src

*** Install
#+begin_src tmate :window community-infra
kubectl -n humacs apply -f humacs.yaml
#+end_src

* Tear down
#+begin_src tmate :window community-infra
for NODE in $(kubectl get nodes -o=jsonpath='{.items[*].metadata.name}'); do
    NODE_INTERNAL_IP=$(kubectl get node "${NODE}" -o=jsonpath='{.status.addresses[?(@.type == "InternalIP")].address}')
    for DEV in {b..f}; do
        echo "${NODE} (${NODE_INTERNAL_IP}) /dev/sd${DEV}"
        ssh -p 2222 root@${NODE_INTERNAL_IP} env DEV=$DEV bash << \EOF
set -x
hostname
yes | kubeadm reset
rm /etc/cni/net.d/*
rm -rf /var/lib/rook
for VG in $(lvs | grep osd | awk  '{print $2}'); do
    lvm vgremove $VG --force
done
lvs
wipefs --all /dev/sd${DEV}
lsblk -f /dev/sd${DEV}
EOF
    done
done
#+end_src
