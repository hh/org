#+TITLE: Local cluster
#+PROPERTY: header-args:diff+ :comments none
#+PROPERTY: header-args:dockerfile+ :comments none
#+PROPERTY: header-args:shell+ :prologue "( " :epilogue " ) 2>&1 ; :" :comments none
#+PROPERTY: header-args:text+ :comments none
#+PROPERTY: header-args:tmate+ :comments none
#+PROPERTY: header-args:yaml+ :comments none

Setting up a local cluster.

* Prologue
* Prepare
** Save repo location
#+begin_src tmate :window prepare
export REPO_ROOT="${PWD}"
#+end_src

** Downloading a Talos RPi image
Download the Talos image to flash to a MicroSD card from GitHub
#+begin_src tmate :window prepare
cd $(mktemp -d)
curl -O -L \
  https://github.com/talos-systems/talos/releases/download/v0.10.4/metal-rpi_4-arm64.img.xz
export TALOS_METAL_RPI_IMG=${PWD}/*
#+end_src

Some Pis may require having the EEPROM updated, check [[https://www.talos.dev/docs/v0.10/single-board-computers/rpi_4/#updating-the-eeprom][the Talos docs]].

** Prepare MicroSD cards
Write the image to a MicroSD card
#+begin_src tmate :window prepare
export DISK_TO_USE_DEFAULT=/dev/sdb && \
  read -p "Enter the disk to use (default: '${DISK_TO_USE}'): " DISK_TO_USE && \
  sudo dd \
    if=${TALOS_METAL_RPI_IMG} \
    of="${DISK_TO_USE-$DISK_TO_USE_DEFAULT}" \
    status=progress \
    conv=fsync \
    bs=4M
#+end_src

** Install =talosctl=
To manage Talos on each node, =talosctl= is used to provision and manage
#+begin_src tmate :window prepare
curl -o ~/bin/talosctl -L \
  https://github.com/talos-systems/talos/releases/download/v0.11.0/talosctl-$(uname | tr '[:upper:]' '[:lower:]')-amd64
chmod +x ~/bin/talosctl
#+end_src

* Set up
** kind
Declare the kind config
#+begin_src yaml :tangle ./kind-config.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
 - role: control-plane
   extraPortMappings:
   - containerPort: 67
     hostPort: 67
     protocol: UDP
   - containerPort: 69
     hostPort: 69
     protocol: UDP
   - containerPort: 80
     hostPort: 80
     protocol: TCP
   - containerPort: 443
     hostPort: 443
     protocol: TCP
   - containerPort: 4011
     hostPort: 4011
     protocol: UDP
   - containerPort: 7472
     hostPort: 7472
     protocol: UDP
   - containerPort: 8081
     hostPort: 8081
     protocol: TCP
   kubeadmConfigPatches:
   - |
     apiVersion: kubeadm.k8s.io/v1beta2
     kind: InitConfiguration
     nodeRegistration:
       kubeletExtraArgs:
         node-labels: "ingress-ready=true"
#+end_src

Using kind /v0.11.1/ or greater, launch the cluster
#+begin_src tmate :window prepare
export HOST_IP=$(ip a | grep 192.168 | awk '{print $2}' | cut -d '/' -f1 | head -n 1)
kind create cluster --config <(envsubst < ./kind-config.yaml)
#+end_src

#+begin_src tmate :window prepare
export HOST_IP=$(ip a | grep 192.168 | awk '{print $2}' | cut -d '/' -f1 | head -n 1)
talosctl cluster create \
  -p 67:67/udp,69:69/udp,80:80/tcp,443:443/tcp,4011:4011/udp,7472:7472/tcp,8081:8081/tcp \
  --workers 0 \
  --config-patch '[{"op": "add", "path": "/cluster/allowSchedulingOnMasters", "value": true}]' \
  --endpoint "${HOST_IP}"
#+end_src

Install nginx-ingress-controller
#+begin_src shell
VERSION=$(curl -s https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/stable.txt)
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/${VERSION}/deploy/static/provider/kind/deploy.yaml
#+end_src

#+RESULTS:
#+begin_example
namespace/ingress-nginx created
serviceaccount/ingress-nginx created
configmap/ingress-nginx-controller created
clusterrole.rbac.authorization.k8s.io/ingress-nginx created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created
role.rbac.authorization.k8s.io/ingress-nginx created
rolebinding.rbac.authorization.k8s.io/ingress-nginx created
service/ingress-nginx-controller-admission created
service/ingress-nginx-controller created
deployment.apps/ingress-nginx-controller created
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created
serviceaccount/ingress-nginx-admission created
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
role.rbac.authorization.k8s.io/ingress-nginx-admission created
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
job.batch/ingress-nginx-admission-create created
job.batch/ingress-nginx-admission-patch created
#+end_example

Set /NODE_ADDRS/ var
#+begin_src tmate :window prepare
export NODE_ADDRS=$(ip a | grep 192.168 | awk '{print $2}' | cut -d '/' -f1 | head -n 1)
#+end_src

** a Talos node
*** Discover node IPs
Each node that comes up will, of course, have an IP address.
I'm checking what the router says.

*** Determine that nodes are live
#+begin_src tmate :window prepare
#export NODE_ADDRS=(192.168.1.111 192.168.1.127 192.168.1.234)
export NODE_ADDRS=(192.168.1.234)
#+end_src

*** Ensure nodes are live
#+begin_src tmate :window prepare
for IP in ${NODE_ADDRS[*]}; do
    echo "Checking ${IP}:50000"
    nc -zv "${IP}" "50000"
done
#+end_src

*** Generating the configuration
#+begin_src tmate :window prepare
talosctl gen config \
    ii-nz \
    https://192.168.1.100:6443 \
    --output-dir talos/ \
    --additional-sans k8s.ii.nz \
    --install-disk /dev/mmcblk0 \
    --install-image ghcr.io/talos-systems/installer:v0.10.3
#+end_src

*** Modify the configuration
#+begin_src diff :tangle talos-config-patches.patch :comment none
diff --git a/talos/controlplane.yaml b/talos/controlplane.yaml
index bc87738..cf17a8a 100644
--- a/talos/controlplane.yaml
+++ b/talos/controlplane.yaml
@@ -35,7 +35,12 @@ machine:
     #         - rw

     # Provides machine specific network configuration options.
-    network: {}
+    network:
+      interfaces:
+        - interface: eth0
+          dhcp: true
+          vip:
+            ip: 192.168.1.100
     # # `interfaces` is used to define the network interface configuration.
     # interfaces:
     #     - interface: eth0 # The interface name.
@@ -214,6 +219,7 @@ machine:
     #               slot: 0 # Key slot number for luks2 encryption.
 # Provides cluster specific configuration options.
 cluster:
+    allowSchedulingOnMasters: true
     # Provides control plane specific configuration options.
     controlPlane:
         endpoint: https://192.168.1.100:6443 # Endpoint is the canonical controlplane endpoint, which can be an IP address or a DNS hostname.
diff --git a/talos/init.yaml b/talos/init.yaml
index ad6d34e..46bdafd 100644
--- a/talos/init.yaml
+++ b/talos/init.yaml
@@ -35,7 +35,12 @@ machine:
     #         - rw

     # Provides machine specific network configuration options.
-    network: {}
+    network:
+      interfaces:
+        - interface: eth0
+          dhcp: true
+          vip:
+            ip: 192.168.1.100
     # # `interfaces` is used to define the network interface configuration.
     # interfaces:
     #     - interface: eth0 # The interface name.
@@ -214,6 +219,7 @@ machine:
     #               slot: 0 # Key slot number for luks2 encryption.
 # Provides cluster specific configuration options.
 cluster:
+    allowSchedulingOnMasters: true
     # Provides control plane specific configuration options.
     controlPlane:
         endpoint: https://192.168.1.100:6443 # Endpoint is the canonical controlplane endpoint, which can be an IP address or a DNS hostname.

#+end_src

Apply patches
#+begin_src tmate :window prepare
patch -ruN -d talos/ < "${REPO_ROOT}/talos-config-patches.patch"
#+end_src

*** Use talosconfig
#+NAME: export-talosconfig
#+begin_src tmate :window prepare
export TALOSCONFIG=$PWD/talos/talosconfig
#+end_src

Write the endpoint
#+begin_src tmate :window prepare
talosctl config endpoint 192.168.1.100
#+end_src

*** Provisioning the first node
#+begin_src tmate :window prepare
talosctl apply-config --insecure --nodes "${NODE_ADDRS[0]}" --file talos/init.yaml
#+end_src

**** Ensure that the node is active
#+begin_src tmate :window prepare
talosctl health -e "${NODE_ADDRS[0]}" -n "${NODE_ADDRS[0]}"
#+end_src

*** Provision all the nodes
#+begin_src tmate :window prepare
for IP in ${NODE_ADDRS[*]}; do
    talosctl apply-config --insecure --nodes "${IP}" --file talos/controlplane.yaml
done
#+end_src

**** Watch the health of all nodes, as they become active
#+begin_src tmate :window prepare
talosctl health -e "${NODE_ADDRS[0]}" -n "${NODE_ADDRS[0]}"
#+end_src

*** Get kubeconfig
#+begin_src tmate :window prepare
talosctl kubeconfig -e 192.168.1.100 -n 192.168.1.100
#+end_src

*** Get nodes
#+begin_src shell
kubectl get nodes
#+end_src

#+RESULTS:
#+begin_example
NAME                  STATUS   ROLES                  AGE     VERSION
talos-192-168-1-111   Ready    control-plane,master   16m     v1.21.1
talos-192-168-1-127   Ready    control-plane,master   8m2s    v1.21.1
talos-192-168-1-234   Ready    control-plane,master   7m43s   v1.21.1
#+end_example

* Validate
** Get pods
#+begin_src shell
kubectl get pods -A
#+end_src

#+RESULTS:
#+begin_example
NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE
kube-system   coredns-fcc4c97fb-br6rd                       1/1     Running   0          17m
kube-system   coredns-fcc4c97fb-cfstz                       1/1     Running   0          17m
kube-system   kube-apiserver-talos-192-168-1-111            1/1     Running   0          14m
kube-system   kube-apiserver-talos-192-168-1-127            1/1     Running   0          7m23s
kube-system   kube-apiserver-talos-192-168-1-234            1/1     Running   0          7m55s
kube-system   kube-controller-manager-talos-192-168-1-111   1/1     Running   3          15m
kube-system   kube-controller-manager-talos-192-168-1-127   1/1     Running   0          7m23s
kube-system   kube-controller-manager-talos-192-168-1-234   1/1     Running   0          7m55s
kube-system   kube-flannel-5stx9                            1/1     Running   0          8m16s
kube-system   kube-flannel-9kcx2                            1/1     Running   0          7m56s
kube-system   kube-flannel-wxn5m                            1/1     Running   0          16m
kube-system   kube-proxy-6dzrl                              1/1     Running   0          7m56s
kube-system   kube-proxy-pb42s                              1/1     Running   0          8m16s
kube-system   kube-proxy-w5q56                              1/1     Running   0          16m
kube-system   kube-scheduler-talos-192-168-1-111            1/1     Running   3          15m
kube-system   kube-scheduler-talos-192-168-1-127            1/1     Running   0          7m23s
kube-system   kube-scheduler-talos-192-168-1-234            1/1     Running   0          7m55s
#+end_example

* Ensure set up
** Upload talos folder into Kubernetes secret
#+begin_src tmate :window prepare
kubectl -n kube-system create secret generic "talos-config" --from-file=talos/
#+end_src

Ensure that the files exist in the secret
#+begin_src shell
kubectl -n kube-system get secret talos-config -o yaml | yq e '.data | keys | .[]' -P -
#+end_src

#+RESULTS:
#+begin_example
controlplane.yaml
init.yaml
join.yaml
talosconfig
#+end_example

** Fetch Talos configs
Create a new temp directory
#+begin_src tmate :window prepare
cd $(mktemp -d)
#+end_src

Extract talos-config into directory
#+begin_src tmate :window prepare :noweb yes
TALOS_CONFIGS="$(mktemp -t talos-config-XXXXX)"
kubectl -n kube-system get secret talos-config -o yaml > "${TALOS_CONFIGS}"

mkdir -p talos/
for FILE in $(cat "${TALOS_CONFIGS}" | yq e '.data | keys | .[]' -P -); do
  echo $FILE
  cat "${TALOS_CONFIGS}" | yq e ".data.\"${FILE}\"" -P - | base64 --decode > "talos/${FILE}"
done
<<export-talosconfig>>
#+end_src

** Get node IPs from the cluster
#+begin_src tmate :window prepare
export NODE_ADDRS=$(kubectl get nodes -o yaml | yq e '.items[].status.addresses[] | select(.type=="InternalIP") | .address' -P -)
#+end_src

** Get the TalosConfig
#+begin_src tmate :window prepare
export TALOSCONFIG=$(mktemp /tmp/tmp.XXXXX)
kubectl -n local-clusters get talosconfig -l cluster.x-k8s.io/cluster-name=local-cluster-mgmt -o=jsonpath='{.items[0].status.talosConfig}' > "${TALOSCONFIG}"
#+end_src

** Get machinetype
#+begin_src tmate :window prepare
talosctl -e 192.168.1.100 -n "$(echo ${NODE_ADDRS} | tr ' ' ',')" get machinetype
#+end_src

** Shutdown RPis
#+begin_src tmate :window prepare
for IP in ${NODE_ADDRS[*]}; do
    talosctl shutdown -e 192.168.1.100 -n "${IP}"
done
#+end_src

** Reset all nodes to uninitialised Talos
#+begin_src tmate :window prepare
read -p "Are you sure you want to reset all nodes, effectively destroying the cluster? [Enter|C-c] " && \
(
  for IP in ${NODE_ADDRS[*]}; do
      talosctl -e "${IP}" -n "${IP}" reset --graceful=false --reboot --system-labels-to-wipe=EPHEMERAL,STATE
  done
)
#+end_src

* Workloads
** metallb
*** Prepare
Create a directory for the manifests and a namespace for the resources
#+begin_src shell :results silent
mkdir -p metallb
curl -o metallb/namespace.yaml -L https://raw.githubusercontent.com/metallb/metallb/v0.9.6/manifests/namespace.yaml
curl -o metallb/metallb.yaml -L https://raw.githubusercontent.com/metallb/metallb/v0.9.6/manifests/metallb.yaml
#+end_src

*** Configure
Using layer2 for ARP capabilities and provide a very sufficient 10 IP address range in a part of the network that is configure to not be used by DHCP.
#+begin_src yaml :tangle ./metallb/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - 192.168.1.20-192.168.1.30
#+end_src

*** Install
#+begin_src shell
kubectl apply -f metallb/namespace.yaml
kubectl -n metallb-system get secret memberlist 2> /dev/null \
    || kubectl -n metallb-system create secret generic memberlist --from-literal=secretkey="$(openssl rand -base64 128)"
kubectl -n metallb-system apply -f ./metallb/config.yaml
kubectl -n metallb-system apply -f ./metallb/metallb.yaml
#+end_src

#+RESULTS:
#+begin_example
namespace/metallb-system created
secret/memberlist created
configmap/config created
Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
podsecuritypolicy.policy/controller created
podsecuritypolicy.policy/speaker created
serviceaccount/controller created
serviceaccount/speaker created
clusterrole.rbac.authorization.k8s.io/metallb-system:controller created
clusterrole.rbac.authorization.k8s.io/metallb-system:speaker created
role.rbac.authorization.k8s.io/config-watcher created
role.rbac.authorization.k8s.io/pod-lister created
clusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller created
clusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker created
rolebinding.rbac.authorization.k8s.io/config-watcher created
rolebinding.rbac.authorization.k8s.io/pod-lister created
daemonset.apps/speaker created
deployment.apps/controller created
#+end_example

** Helm-Operator
Unfortunately the Helm-Operator project by FluxCD is both in maintenance mode and unsupported on arm64. Here in the prepare stage, I'm patching the current state of how things are to build an arm64 image. Ideally, this is all in a single Dockerfile and does not use Make scripts. I'm unsure what the future of Helm-Operator is, but I'd like to see and help support for architectures outta-the-box.

*** Prepare
Create a directory for the manifests and a namespace for the resources
#+begin_src shell :results silent
mkdir -p helm-operator
kubectl create namespace helm-operator --dry-run=client -o yaml \
  | kubectl apply -f -
#+end_src

*** Configure
Create local manifests to apply in the cluster
#+begin_src shell :results silent
curl -o ./helm-operator/helm-operator-crds.yaml -L https://raw.githubusercontent.com/fluxcd/helm-operator/1.2.0/deploy/crds.yaml

helm repo add fluxcd https://charts.fluxcd.io
helm template helm-operator --create-namespace fluxcd/helm-operator \
    --namespace helm-operator \
    --set helm.versions=v3 \
    --set image.repository=registry.gitlab.com/bobymcbobs/container-images/helm-operator \
    --set image.tag=1.2.0 \
      > ./helm-operator/helm-operator.yaml
#+end_src

*** Install
#+begin_src shell
kubectl apply -f ./helm-operator/helm-operator-crds.yaml
kubectl -n helm-operator apply -f ./helm-operator/helm-operator.yaml
#+end_src

#+RESULTS:
#+begin_example
Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition
customresourcedefinition.apiextensions.k8s.io/helmreleases.helm.fluxcd.io configured
serviceaccount/helm-operator unchanged
secret/helm-operator-git-deploy unchanged
configmap/helm-operator-kube-config unchanged
Warning: rbac.authorization.k8s.io/v1beta1 ClusterRole is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRole
clusterrole.rbac.authorization.k8s.io/helm-operator unchanged
Warning: rbac.authorization.k8s.io/v1beta1 ClusterRoleBinding is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRoleBinding
clusterrolebinding.rbac.authorization.k8s.io/helm-operator unchanged
service/helm-operator unchanged
deployment.apps/helm-operator configured
#+end_example

** nginx-ingress controller
*** Prepare

Create a directory for the manifests and a namespace for the resources
#+begin_src shell :results silent
mkdir -p nginx-ingress
kubectl create namespace nginx-ingress --dry-run=client -o yaml \
  | kubectl apply -f -
#+end_src

*** Configure
Ensuring that remote IP addresses will be forwarded as headers in the requests, using the fields in the /.spec.values.controller.service/ field.
Preferring that each nginx-ingress pod runs on a different node.
#+begin_src yaml :tangle ./nginx-ingress/nginx-ingress.yaml
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: nginx-ingress
  namespace: nginx-ingress
spec:
  releaseName: nginx-ingress
  chart:
    repository: https://kubernetes.github.io/ingress-nginx
    name: ingress-nginx
    version: 3.30.0
  values:
    controller:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                      - ingress-nginx
              topologyKey: "kubernetes.io/hostname"
      service:
        type: LoadBalancer
        externalTrafficPolicy: Local
    defaultBackend:
      enabled: false
#+end_src

*** Install
#+begin_src shell
kubectl -n nginx-ingress apply -f nginx-ingress/nginx-ingress.yaml
#+end_src

#+RESULTS:
#+begin_example
helmrelease.helm.fluxcd.io/nginx-ingress created
#+end_example

** local-path-provisioner
Currently used, to get-the-job-done.
My end goal is to use Rook+Ceph in-place, but I'm starting with this.

*** Prepare
Create a directory for the manifests and a namespace for the resources.
#+begin_src shell :results silent
mkdir -p local-path-provisioner
curl -o local-path-provisioner/local-path-provisioner.yaml -L https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
#+end_src

*** Install
#+begin_src shell
kubectl apply -f local-path-provisioner/local-path-provisioner.yaml
#+end_src

#+RESULTS:
#+begin_example
namespace/local-path-storage created
serviceaccount/local-path-provisioner-service-account created
clusterrole.rbac.authorization.k8s.io/local-path-provisioner-role created
clusterrolebinding.rbac.authorization.k8s.io/local-path-provisioner-bind created
deployment.apps/local-path-provisioner created
storageclass.storage.k8s.io/local-path created
configmap/local-path-config created
#+end_example

*** Finalise
Ensuring that local-path is the default StorageClass.
#+begin_src shell
kubectl patch storageclasses.storage.k8s.io local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
#+end_src

#+RESULTS:
#+begin_example
storageclass.storage.k8s.io/local-path patched
#+end_example

** CAPI + Sidero
Links:
- https://www.sidero.dev/docs/v0.3/getting-started/install-clusterapi/
- https://www.sidero.dev/docs/v0.3/guides/rpi4-as-servers/#rpi4-boot-process

*** Configure
#+begin_src yaml :tangle ./sidero-controller-manager-debug.yaml
apiVersion: v1
kind: Pod
metadata:
  name: sidero-debug
  namespace: sidero-system
spec:
  hostNetwork: true
  containers:
  - image: alpine:3.12
    name: sidero-debug
    securityContext:
      privileged: true
    volumeMounts:
      - mountPath: /var/lib/sidero/tftp
        name: tftp-folder
    command:
      - sh
      - -c
      - apk add tar && sleep infinity
  volumes:
    - name: tftp-folder
      persistentVolumeClaim:
        claimName: sidero-tftp
#+end_src
#+begin_src yaml :tangle ./sidero-controller-manager-tftp-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: sidero-tftp
  namespace: sidero-system
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
#+end_src
#+begin_src yaml :tangle ./sidero-controller-manager-patch.yaml
spec:
  template:
    spec:
      volumes:
        - name: tftp-folder
          persistentVolumeClaim:
            claimName: sidero-tftp
      containers:
      - name: manager
        volumeMounts:
          - mountPath: /var/lib/sidero/tftp
            name: tftp-folder
#+end_src
- TODO Sidero TFTP, for UEFI boot
  - share the /var/lib/sidero/tftp folder as a PVC with a alpine pod
- TODO copy UEFI boot into TFTP folder and RPI_EFI.fd from SD card

*** Install
#+begin_src tmate :window prepare
export SIDERO_METADATA_SERVER_HOST_NETWORK=true \
  SIDERO_METADATA_SERVER_PORT=9091 \
  SIDERO_CONTROLLER_MANAGER_API_ENDPOINT="${HOST_IP:-192.168.1.21}" \
  SIDERO_CONTROLLER_MANAGER_AUTO_ACCEPT_SERVERS=true \
  SIDERO_CONTROLLER_MANAGER_HOST_NETWORK=false \
  SIDERO_CONTROLLER_MANAGER_BOOT_FROM_DISK_METHOD=http-404

clusterctl init -b talos -c talos -i sidero:v0.3.0
#+end_src

*** Finalise
(metal-only) Assign a virtal IP to the sidero-http service
#+begin_src shell
kubectl -n sidero-system patch service sidero-http -p '{"spec":{"type":"LoadBalancer"}}'
#+end_src

#+RESULTS:
#+begin_example
service/sidero-http patched
#+end_example

(kind-only) Assign an external IP to sidero-http
#+begin_src shell
export KIND_IP="$(docker inspect kind-control-plane -f '{{.NetworkSettings.Networks.kind.IPAddress}}')"
kubectl -n sidero-system patch service sidero-http -p "{\"spec\":{\"externalIPs\":[\"${KIND_IP}\"]}}"
#+end_src

#+RESULTS:
#+begin_example
service/sidero-http patched
#+end_example

Check the IP address
#+begin_src shell
kubectl -n sidero-system get svc
#+end_src

#+RESULTS:
#+begin_example
NAME                                        TYPE           CLUSTER-IP       EXTERNAL-IP    PORT(S)          AGE
sidero-controller-manager-metrics-service   ClusterIP      10.100.182.164   <none>         8443/TCP         77s
sidero-http                                 LoadBalancer   10.105.234.143   192.168.1.21   8081:30367/TCP   77s
sidero-tftp                                 ClusterIP      10.100.74.148    <none>         69/UDP           77s
#+end_example

Expose Sidero-HTTP as a HTTPs Ingress
#+begin_src yaml :tangle ./ingress-boot-ii-nz.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: boot-ii-nz
  namespace: sidero-system
spec:
  rules:
  - host: boot.ii.nz
    http:
      paths:
      - backend:
          service:
            name: sidero-http
            port:
              number: 8081
        path: /
        pathType: ImplementationSpecific
#+end_src

Apply the ingress
#+begin_src shell
kubectl apply -f ./ingress-boot-ii-nz.yaml
#+end_src

#+RESULTS:
#+begin_example
ingress.networking.k8s.io/boot-ii-nz created
#+end_example

Create a PVC for the TFTP folder
#+begin_src shell
# TODO figure out how to use dnsmasq/dhcp for just PXE and Sidero CM for TFTP
kubectl apply -f ./sidero-controller-manager-tftp-pvc.yaml
kubectl -n sidero-system patch deployment sidero-controller-manager --patch-file ./sidero-controller-manager-patch.yaml
kubectl -n sidero-system delete pod -l app=sidero 2> /dev/null
#+end_src

#+RESULTS:
#+begin_example
persistentvolumeclaim/sidero-tftp created
deployment.apps/sidero-controller-manager patched
pod "caps-controller-manager-5948c84db7-vbwhm" deleted
pod "sidero-controller-manager-565796bc46-9xzhx" deleted
pod "sidero-controller-manager-76c76bdc8d-s2f7q" deleted
#+end_example

Create a Pod that's also got the TFTP mount
#+begin_src shell
kubectl -n sidero-system delete pod sidero-debug 2> /dev/null
kubectl apply -f ./sidero-controller-manager-debug.yaml
#+end_src

#+RESULTS:
#+begin_example
pod/sidero-debug created
#+end_example

Czech the content
#+begin_src shell
kubectl -n sidero-system exec -it sidero-debug -- ls -alh /var/lib/sidero/tftp/
#+end_src

#+RESULTS:
#+begin_example
Unable to use a TTY - input is not a terminal or the right kind of file
total 2M
drwxrwxrwx    2 root     root           6 Jul  8 20:49 .
drwxr-xr-x    3 root     root           3 Jul  8 20:49 ..
-rw-r--r--    1 root     root      968.6K Jul  8 20:49 ipxe-arm64.efi
-rw-r--r--    1 root     root      996.5K Jul  8 20:49 ipxe.efi
-rw-r--r--    1 root     root       81.0K Jul  8 20:49 undionly.kpxe
-rw-r--r--    1 root     root       81.0K Jul  8 20:49 undionly.kpxe.0
#+end_example

Copy assets in-place
#+begin_src tmate :window prepare
kubectl -n sidero-system cp sidero-debug:/var/lib/sidero /tmp/
#+end_src
(this will be used for uploading the TFTP root for DNSMASQ)

*** Debug
Logs
#+begin_src tmate :window sidero
kubectl -n sidero-system logs -l app=sidero -f
#+end_src

Scale to zero
#+begin_src shell
kubectl -n sidero-system scale deployment sidero-controller-manager --replicas=0
#+end_src

#+RESULTS:
#+begin_example
deployment.apps/sidero-controller-manager scaled
#+end_example

Scale to one
#+begin_src shell
kubectl -n sidero-system scale deployment sidero-controller-manager --replicas=1
#+end_src

*** Remove
#+begin_src tmate :window prepare
clusterctl delete --all
#+end_src
(useful for iterating)

** PXE boot server (dnsmasq)
*** Prepare
#+begin_src shell :results silent
mkdir -p dnsmasq
kubectl create namespace dnsmasq --dry-run=client -o yaml | \
    kubectl apply -f -
#+end_src

*** Configure
Configure dnsmasq
#+begin_src text :tangle ./dnsmasq/dnsmasq.conf :comments none
#dnsmasq config, for a complete example, see:
#  http://oss.segetech.com/intra/srv/dnsmasq.conf

port=0
dhcp-range=${DHCP_RANGE},proxy
pxe-service=0,"Raspberry Pi Boot"
pxe-prompt="PXE booting Talos from Sidero in",0
dhcp-boot=ipxe-arm64.efi,sidero
log-queries
log-dhcp

enable-tftp=*
tftp-root=/var/lib/sidero/tftp
#+end_src

Configure the container
#+begin_src dockerfile :tangle ./dnsmasq/Dockerfile :comments none
FROM alpine:3.12 AS final
RUN apk add --no-cache tcpdump curl dnsmasq-dnssec gettext bash
# TODO run as non-root
RUN mkdir -p /etc/default/ && \
  echo -e "ENABLED=1\nIGNORE_RESOLVCONF=yes" > /etc/default/dnsmasq
ENTRYPOINT ["dnsmasq","--no-daemon"]
#+end_src

TFTP PVC
#+begin_src yaml :tangle ./dnsmasq/dnsmasq-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: dnsmasq-tftp
  namespace: dnsmasq
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
#+end_src

Configure the deployment
#+begin_src yaml :tangle ./dnsmasq/dnsmasq.yaml :comments none
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dnsmasq
  namespace: dnsmasq
  labels:
    nz.ii: dnsmasq
    app: dnsmasq
spec:
  strategy:
    type: Recreate
  replicas: 1
  selector:
    matchLabels:
      nz.ii: dnsmasq
  template:
    metadata:
      annotations:
        nz.ii/dnsmasq.conf-sha256sum: "${DNSMASQ_CONF_HASH}"
        nz.ii/dockerfile-sha256sum: "${DOCKERFILE_HASH}"
      labels:
        nz.ii: dnsmasq
        app: dnsmasq
    spec:
      hostNetwork: true
      containers:
      - name: dnsmasq
        image: registry.gitlab.com/ii/nz/dnsmasq:latest
        imagePullPolicy: Always
        volumeMounts:
          - name: config
            mountPath: /etc/dnsmasq
          - name: tftp-folder
            mountPath: /var/lib/sidero/tftp
        env:
          - name: DHCP_RANGE
            value: "${DHCP_RANGE}"
        command:
          - bash
          - -x
          - -c
          - dnsmasq --no-daemon -C <(envsubst < /etc/dnsmasq/dnsmasq.conf)
        securityContext:
          capabilities:
            add:
              - NET_ADMIN
              - NET_RAW
              - SYS_ADMIN
        ports:
        - containerPort: 67
          hostPort: 67
          protocol: UDP
        - containerPort: 4011
          hostPort: 4011
          protocol: UDP
        - containerPort: 7472
          hostPort: 7472
          protocol: UDP
      volumes:
      - name: config
        configMap:
          name: dnsmasq-config
      - name: tftp-folder
        persistentVolumeClaim:
          claimName: dnsmasq-tftp
#+end_src

*** Build
**** Build for the target architecture
#+begin_src tmate :window dnsmasq
kubectl build \
    --destination registry.gitlab.com/ii/nz/dnsmasq:latest \
    --snapshotMode=redo \
    --context=$PWD \
    --dockerfile ./dnsmasq/Dockerfile
#+end_src

**** Build for /amd64/ and /arm64/
Prepare (1/2): prepare binfmt files
#+begin_src tmate :window dnsmasq
docker run --privileged --rm tonistiigi/binfmt --install all
#+end_src

Prepare (2/2): create a builder
#+begin_src tmate :window dnsmasq
docker buildx create --use
#+end_src

Build
#+begin_src tmate :window dnsmasq
docker buildx build \
    --platform linux/arm64/v8,linux/amd64 \
    --push \
    --tag registry.gitlab.com/ii/nz/dnsmasq:latest \
    --file ./dnsmasq/Dockerfile \
    ./dnsmasq/
#+end_src
(dependencies: docker-ce=>20.10.6, docker-buildx=>0.3.1, qemu-user-static, binfmt-support)

*** Install
#+begin_src shell
kubectl -n dnsmasq create configmap dnsmasq-config --from-file=dnsmasq/dnsmasq.conf --dry-run=client -o yaml | \
    kubectl apply -f -
export DNSMASQ_CONF_HASH="$(sha256sum ./dnsmasq/dnsmasq.conf | awk '{print $1}')"
export DOCKERFILE_HASH="$(sha256sum ./dnsmasq/Dockerfile | awk '{print $1}')"
if [ "$(kubectl config current-context)" = "kind-kind" ]; then
    DHCP_RANGE="$(docker network inspect -f '{{json .IPAM.Config}}' kind | jq -r .[0].Subnet | cut -d / -f1)"
fi
export DHCP_RANGE="${DHCP_RANGE:-192.168.1.0}"
echo "DHCP_RANGE: ${DHCP_RANGE}"
kubectl apply -f ./dnsmasq/dnsmasq-pvc.yaml
envsubst < ./dnsmasq/dnsmasq.yaml | kubectl apply -f -
#+end_src

#+RESULTS:
#+begin_example
configmap/dnsmasq-config configured
DHCP_RANGE: 192.168.1.0
persistentvolumeclaim/dnsmasq-tftp unchanged
deployment.apps/dnsmasq unchanged
#+end_example

*** Validate
Get logs
#+begin_src tmate :window dnsmasq
kubectl -n dnsmasq logs -l app=dnsmasq --prefix -f
#+end_src

#+begin_src tmate :window prepare
PORTS=(67 69)
for IP in ${NODE_ADDRS[*]}; do
    for PORT in ${PORTS[*]}; do
        echo "Checking ${IP}:${PORT}"
        nc -zvu "${IP}" "${PORT}" || "  port '${PORT}' inaccessible"
    done
done
#+end_src

#+begin_src yaml :tangle ./dnsmasq/debug-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: dnsmasq-debug
  name: dnsmasq-debug
  namespace: dnsmasq
spec:
  hostNetwork: true
  containers:
  - image: alpine:3.12
    name: dnsmasq-debug
    securityContext:
      privileged: true
    command:
      - sleep
      - infinity
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - weight: 1
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: app
                  operator: In
                  values:
                    - dnsmasq
            topologyKey: "kubernetes.io/hostname"
  dnsPolicy: ClusterFirst
  restartPolicy: Always
#+end_src

#+begin_src shell
kubectl delete -f ./dnsmasq/debug-pod.yaml
#+end_src

Drop a shell
#+begin_src tmate :window tcpdump
kubectl -n dnsmasq exec -it dnsmasq-debug -- sh
#+end_src

Install tcpdump
#+begin_src tmate :window tcpdump
apk add tcpdump
#+end_src

List interfaces
#+begin_src tmate :window tcpdump
ip a
#+end_src

We'll use eth0, since that's the host network for the Pi

Scale to zero
#+begin_src shell
kubectl -n dnsmasq scale deployment dnsmasq --replicas=0
#+end_src

#+RESULTS:
#+begin_example
deployment.apps/dnsmasq scaled
#+end_example

Scale to one
#+begin_src shell
kubectl -n dnsmasq scale deployment dnsmasq --replicas=1
#+end_src

#+RESULTS:
#+begin_example
deployment.apps/dnsmasq scaled
#+end_example

*** Prepare assets
Download UEFI assets
#+begin_src tmate :window prepare
FILES="start4.elf fixup4.dat config.txt ipxe.efi ipxe-arm64.efi firmware overlays bcm2711-rpi-4-b.dtb"

cd /var/lib/sidero/tftp
for UUID in ${UUIDs}; do
    echo "${UUID}:"
    mkdir -p ${UUID}
    cd ${UUID}
    rm *
    for FILE in $FILES; do
        echo "- ${FILE}"
        ln -sf ../$FILE ./$FILE
    done
    cd -
done
VERSION=v1.28
ASSET=RPi4_UEFI_Firmware_${VERSION}.zip
EXTRACTED_DIR=/tmp/tftp
rm -rf "${EXTRACTED_DIR}"
if [ ! -f "${HOME}/Downloads/${ASSET}" ]; then
    curl -o ${HOME}/Downloads/${ASSET} -L https://github.com/pftf/RPi4/releases/download/${VERSION}/${ASSET}
fi
mkdir -p "${EXTRACTED_DIR}"
unzip -o "${HOME}/Downloads/${ASSET}" -d "${EXTRACTED_DIR}"

for _SERIAL in servers/*; do
    SERIAL="${_SERIAL/servers\//}"
    echo "${SERIAL}:"
    mkdir -p "${EXTRACTED_DIR}/${SERIAL}"
    cp -f "${_SERIAL}/RPI_EFI.fd" "${EXTRACTED_DIR}/${SERIAL}/"
    cp -a tftp-root/SERIAL/* /tmp/tftp/${SERIAL}/
done
ls -alhR /tmp/tftp
#+end_src

*** Copy TFTP contents to dnsmasq

Copy TFTP folder from sidero-controller-manager into dnsmasq's TFTP folder
#+begin_src shell
# echo "Copying TFTP out from Sidero Controller Manager"
# SCM_TFTP_FOLDER=/tmp
# kubectl -n sidero-system cp sidero-debug:/var/lib/sidero/tftp "${SCM_TFTP_FOLDER}/tftp"
# echo "Local contents of ${SCM_TFTP_FOLDER}/tftp"
# ls -alh "${SCM_TFTP_FOLDER}/tftp/"

echo "Copying local contents to dnsmasq"
DNSMASQ_POD_NAME="$(kubectl -n dnsmasq get pods -l app=dnsmasq -o=jsonpath='{.items[0].metadata.name}')"
kubectl -n dnsmasq exec -it "${DNSMASQ_POD_NAME}" -- rm -rf /var/lib/sidero/tftp/*
kubectl -n dnsmasq cp "/tmp/tftp" "${DNSMASQ_POD_NAME}":/var/lib/sidero/
kubectl -n dnsmasq exec -it "${DNSMASQ_POD_NAME}" -- ls -alh /var/lib/sidero/tftp
#+end_src

#+RESULTS:
#+begin_example
Copying local contents to dnsmasq
Unable to use a TTY - input is not a terminal or the right kind of file
Unable to use a TTY - input is not a terminal or the right kind of file
total 2M
drwxrwxrwx   17 root     root          29 Jul  9 04:26 .
drwxr-xr-x    3 root     root           3 Jul  8 20:50 ..
drwxr-xr-x    2 root     root          11 Jul  9 04:26 136c6fe1
drwxr-xr-x    2 root     root          11 Jul  9 04:26 1f8570e2
drwxr-xr-x    2 root     root          11 Jul  9 04:26 2bbd241a
drwxr-xr-x    2 root     root          11 Jul  9 04:26 2cb186c5
drwxr-xr-x    2 root     root          11 Jul  9 04:26 407d7434
drwxr-xr-x    2 root     root          11 Jul  9 04:26 4b1fcf44
-rw-r--r--    1 1000     1000        1.9M Jul  9 04:26 RPI_EFI.fd
-rw-r--r--    1 1000     1000        5.3K Jul  9 04:26 Readme.md
drwxr-xr-x    2 root     root          11 Jul  9 04:26 bc3ebf28
drwxr-xr-x    2 root     root          11 Jul  9 04:26 bc3ef28
-rw-r--r--    1 1000     1000       48.1K Jul  9 04:26 bcm2711-rpi-4-b.dtb
-rw-r--r--    1 1000     1000       48.1K Jul  9 04:26 bcm2711-rpi-400.dtb
-rw-r--r--    1 1000     1000       48.7K Jul  9 04:26 bcm2711-rpi-cm4.dtb
drwxr-xr-x    2 root     root          11 Jul  9 04:26 bf267951
drwxr-xr-x    2 root     root          11 Jul  9 04:26 c3052218
-rw-r--r--    1 1000     1000         206 Jul  9 04:26 config.txt
drwxr-xr-x    2 root     root          11 Jul  9 04:26 d997b14e
drwxr-xr-x    2 root     root          11 Jul  9 04:26 dd24784d
drwxr-xr-x    2 root     root          11 Jul  9 04:26 ebc28a3f
drwxr-xr-x    3 root     root           5 Jul  9 04:26 firmware
-rw-r--r--    1 1000     1000        5.3K Jul  9 04:26 fixup4.dat
-rw-rw-r--    1 1000     1000      968.6K Jul  8 23:19 ipxe-arm64.efi
-rw-rw-r--    1 1000     1000      996.5K Jul  8 23:19 ipxe.efi
drwxr-xr-x    2 root     root           3 Jul  9 04:26 overlays
-rw-r--r--    1 1000     1000        2.1M Jul  9 04:26 start4.elf
-rw-rw-r--    1 1000     1000       81.0K Jul  8 23:19 undionly.kpxe
-rw-rw-r--    1 1000     1000       81.0K Jul  8 23:19 undionly.kpxe.0
#+end_example

* Preparing RPis for network booting
The following steps must be performed on each RPi

** Flash the latest network EEPROM firmware
1. Fetch the latest release of EEPROM (network) from the [[https://github.com/raspberrypi/rpi-eeprom/releases][GitHub Repo]]
2. Write the contents of the zip file to a fat32 formatted microSD card
3. Insert and boot the microSD card on the RPi
4. Wait until the green screen before unplugging the power for the RPi

** Bring up and configure the UEFI firmware
Into the non-networked target RPi, have spare keyboard and display plugged in,
1. Fetch a release of RPI4_UEFI firmware (currently using /v1.28/) from the [[https://github.com/pftf/RPi4/releases][GitHub Repo]]
2. Write the contents of the zip file to a fat32 formatted microSD card
3. Insert and boot the microSD card
4. Enter the UEFI set up by hitting /Esc/

*** Configure the UEFI firmware
1. Remove Memory limit: In /Device Manager -> Raspberry Pi Configuration -> Advanced Configuration/, set /Limit RAM to 3 GB/ to /Disabled/; F10 + Y to save.
2. Max out CPU clock: In /Device Manager -> Raspberry Pi Configuration -> CPU Configuration/, set /CPU clock/ to /Max/; F10 + Y to save.
3. Declare the iPXE HTTP boot URI: In /Device Manager -> Network Device List -> <MAC ADDRESS> -> HTTP Boot Configuration/, set /Input the description/ to /boot.ii.nz/ and /Boot URI/ to http://boot.ii.nz/tftp/ipxe-arm64.efi; F10 + Y to save.
4. Tidy up the boot order: In /Boot Maintenance Manager -> Boot Options -> Delete Boot Option/, ensure that the following options remain (in no specific order):
   - /boot.ii.nz/
   - /SD/MMC .../
   /Commit Changes and Exit/ to save.
4. Restructure the boot order: In /Boot Maintenance Manager -> Boot Options -> Change Boot Order/, set the order to:
   - /boot.ii.nz/
   - /SD/MMC .../
   /Commit Changes and Exit/ to save.

Once complete, /Esc/ the entire way out to the main menu and hit reset. When the RPi starts booting again, unplug from power before it reaches a source to boot from.

Now, on the SD card with the UEFI firmware, it the file /RPI_EFI.fd/ must be copied into the [[./servers][./servers]] folder, by the board serial number.
Is it useful to find the serial number when the RPi is booted with no network or SD card (located on /board: <...> <SERIAL NUMBER> .../).

* Bringing up servers with Sidero
Declare some common configuration
#+begin_src yaml :tangle ./sidero/local-cluster-rpi-template.yaml
apiVersion: cluster.x-k8s.io/v1alpha3
kind: Cluster
metadata:
  name: ${CLUSTER_NAME}
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 10.244.0.0/16
    services:
      cidrBlocks:
        - 10.96.0.0/12
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
    kind: MetalCluster
    name: ${CLUSTER_NAME}
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1alpha3
    kind: TalosControlPlane
    name: ${CLUSTER_NAME}-cp
---
apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
kind: MetalCluster
metadata:
  name: ${CLUSTER_NAME}
spec:
  controlPlaneEndpoint:
    host: ${CONTROL_PLANE_ENDPOINT}
    port: ${CONTROL_PLANE_PORT}
---
apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
kind: MetalMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-cp
spec:
  template:
    spec:
      serverClassRef:
        apiVersion: metal.sidero.dev/v1alpha1
        kind: ServerClass
        name: ${CONTROL_PLANE_SERVERCLASS}
---
apiVersion: controlplane.cluster.x-k8s.io/v1alpha3
kind: TalosControlPlane
metadata:
  name: ${CLUSTER_NAME}-cp
spec:
  version: ${KUBERNETES_VERSION}
  replicas: ${CONTROL_PLANE_MACHINE_COUNT}
  infrastructureTemplate:
    kind: MetalMachineTemplate
    apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
    name: ${CLUSTER_NAME}-cp
  controlPlaneConfig:
    init:
      generateType: init
      talosVersion: ${TALOS_VERSION}
      configPatches:
        - op: add
          path: /machine/network
          value:
            interfaces:
              - interface: eth0
                dhcp: true
                vip:
                  ip: ${CONTROL_PLANE_ENDPOINT}
    controlplane:
      generateType: controlplane
      talosVersion: ${TALOS_VERSION}
      configPatches:
        - op: add
          path: /machine/network
          value:
            interfaces:
              - interface: eth0
                dhcp: true
                vip:
                  ip: ${CONTROL_PLANE_ENDPOINT}
---
apiVersion: bootstrap.cluster.x-k8s.io/v1alpha3
kind: TalosConfigTemplate
metadata:
  name: ${CLUSTER_NAME}-workers
spec:
  template:
    spec:
      generateType: join
      talosVersion: ${TALOS_VERSION}
---
apiVersion: cluster.x-k8s.io/v1alpha3
kind: MachineDeployment
metadata:
  name: ${CLUSTER_NAME}-workers
spec:
  clusterName: ${CLUSTER_NAME}
  replicas: ${WORKER_MACHINE_COUNT}
  selector:
    matchLabels: null
  template:
    spec:
      version: ${KUBERNETES_VERSION}
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1alpha3
          kind: TalosConfigTemplate
          name: ${CLUSTER_NAME}-workers
      clusterName: ${CLUSTER_NAME}
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
        kind: MetalMachineTemplate
        name: ${CLUSTER_NAME}-workers
---
apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
kind: MetalMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-workers
spec:
  template:
    spec:
      serverClassRef:
        apiVersion: metal.sidero.dev/v1alpha1
        kind: ServerClass
        name: ${WORKER_SERVERCLASS}
#+end_src

Declare the Environment for the RPis
#+begin_src yaml :tangle ./sidero/rpi-environment.yaml
apiVersion: metal.sidero.dev/v1alpha1
kind: Environment
metadata:
  name: raspberrypi4-servers
spec:
  initrd:
    url: https://github.com/talos-systems/talos/releases/download/v0.11.0/initramfs-arm64.xz
  kernel:
    args:
    - console=tty0
    - console=ttyS0
    - consoleblank=0
    - earlyprintk=ttyS0
    - ima_appraise=fix
    - ima_hash=sha512
    - ima_template=ima-ng
    - init_on_alloc=1
    - initrd=initramfs.xz
    - nvme_core.io_timeout=4294967295
    - printk.devkmsg=on
    - pti=on
    - random.trust_cpu=on
    - slab_nomerge=
    - talos.config=http://192.168.1.21:8081/configdata?uuid=${uuid}
    - talos.platform=metal
    url: https://github.com/talos-systems/talos/releases/download/v0.11.0/vmlinuz-arm64
#+end_src

Declare the ServerClass to use for RPis
#+begin_src yaml :tangle ./sidero/rpi-serverclass.yaml
apiVersion: metal.sidero.dev/v1alpha1
kind: ServerClass
metadata:
  name: raspberrypi4-servers
spec:
  environmentRef:
    name: raspberrypi4-servers
  configPatches:
    # - op: add
    #   path: /cluster/allowSchedulingOnMasters
    #   value: true
    - op: replace
      path: /machine/install
      value:
        disk: /dev/mmcblk1
        image: ghcr.io/talos-systems/installer:v0.11.0
        bootloader: true
        wipe: false
        force: false
  qualifiers:
    cpu:
      - manufacturer: Broadcom
        version: "BCM2711 (ARM Cortex-A72)"
    systemInformation:
      - manufacturer: "Raspberry Pi Foundation"
        productName: "Raspberry Pi 4 Model B"
#+end_src

Apply the ServerClass and Environment
#+begin_src shell :results silent
kubectl apply \
    -f ./sidero/rpi-serverclass.yaml \
    -f ./sidero/rpi-environment.yaml
#+end_src

Create a namespace for the clusters
#+begin_src shell :results silent
kubectl create ns local-clusters
#+end_src

Generate config
#+begin_src shell :results silent
export \
    CONTROL_PLANE_ENDPOINT=192.168.1.31 \
    CONTROL_PLANE_PORT=6443 \
    CONTROL_PLANE_SERVERCLASS=raspberrypi4-servers \
    KUBERNETES_VERSION=v1.21.2 \
    TALOS_VERSION=v1.11.0 \
    WORKER_SERVERCLASS=raspberrypi4-servers \
    WORKER_MACHINE_COUNT=5 \
    CONTROL_PLANE_MACHINE_COUNT=1
clusterctl config cluster -n local-clusters local-cluster-mgmt --from ./sidero/local-cluster-rpi-template.yaml > ./sidero/local-clusters/local-cluster-mgmt.yaml
#+end_src

Bring up the workload cluster
#+begin_src shell
kubectl apply -f ./sidero/local-clusters/local-cluster-mgmt.yaml
#+end_src

#+RESULTS:
#+begin_example
cluster.cluster.x-k8s.io/local-cluster-mgmt created
metalcluster.infrastructure.cluster.x-k8s.io/local-cluster-mgmt created
metalmachinetemplate.infrastructure.cluster.x-k8s.io/local-cluster-mgmt-cp created
taloscontrolplane.controlplane.cluster.x-k8s.io/local-cluster-mgmt-cp created
talosconfigtemplate.bootstrap.cluster.x-k8s.io/local-cluster-mgmt-workers created
machinedeployment.cluster.x-k8s.io/local-cluster-mgmt-workers created
metalmachinetemplate.infrastructure.cluster.x-k8s.io/local-cluster-mgmt-workers created
#+end_example

*** Debug
See all things CAPI and Sidero
#+begin_src shell
kubectl get $(kubectl api-resources | grep -E 'x-k8s|sidero' | awk '{print $1}' | xargs | tr ' ' ',') -A
#+end_src

#+RESULTS:
#+begin_example
NAMESPACE        NAME                                                                      AGE
local-clusters   talosconfig.bootstrap.cluster.x-k8s.io/local-cluster-mgmt-cp-pxtxg        94m
local-clusters   talosconfig.bootstrap.cluster.x-k8s.io/local-cluster-mgmt-workers-hk9q5   94m
local-clusters   talosconfig.bootstrap.cluster.x-k8s.io/local-cluster-mgmt-workers-hlttg   94m
local-clusters   talosconfig.bootstrap.cluster.x-k8s.io/local-cluster-mgmt-workers-m7snc   14m
local-clusters   talosconfig.bootstrap.cluster.x-k8s.io/local-cluster-mgmt-workers-nd65c   94m
local-clusters   talosconfig.bootstrap.cluster.x-k8s.io/local-cluster-mgmt-workers-qmsxw   94m

NAMESPACE        NAME                                                                        AGE
local-clusters   talosconfigtemplate.bootstrap.cluster.x-k8s.io/local-cluster-mgmt-workers   94m

NAMESPACE        NAME                                          PHASE
local-clusters   cluster.cluster.x-k8s.io/local-cluster-mgmt   Provisioned

NAMESPACE        NAME                                                            PHASE     REPLICAS   READY   UPDATED   UNAVAILABLE
local-clusters   machinedeployment.cluster.x-k8s.io/local-cluster-mgmt-workers   Running   5          5       5

NAMESPACE        NAME                                                                   PROVIDERID                                      PHASE     VERSION
local-clusters   machine.cluster.x-k8s.io/local-cluster-mgmt-cp-l5bnd                   sidero://00c03111-0000-0000-0000-dca63203f4f8   Running   v1.21.2
local-clusters   machine.cluster.x-k8s.io/local-cluster-mgmt-workers-647744d6cd-2hdrs   sidero://00c03111-0000-0000-0000-dca632487ab4   Running   v1.21.2
local-clusters   machine.cluster.x-k8s.io/local-cluster-mgmt-workers-647744d6cd-67kxr   sidero://00c03112-0000-0000-0000-dca6327dcbba   Running   v1.21.2
local-clusters   machine.cluster.x-k8s.io/local-cluster-mgmt-workers-647744d6cd-p6zrd   sidero://00c03111-0000-0000-0000-dca63203f59a   Running   v1.21.2
local-clusters   machine.cluster.x-k8s.io/local-cluster-mgmt-workers-647744d6cd-pljkq   sidero://00c03111-0000-0000-0000-dca6321c2b8a   Running   v1.21.2
local-clusters   machine.cluster.x-k8s.io/local-cluster-mgmt-workers-647744d6cd-wn7bv   sidero://00c03111-0000-0000-0000-dca6321c36d1   Running   v1.21.2

NAMESPACE        NAME                                                                REPLICAS   AVAILABLE   READY
local-clusters   machineset.cluster.x-k8s.io/local-cluster-mgmt-workers-647744d6cd   5          5           5

NAMESPACE       NAME                                                         TYPE                     PROVIDER      VERSION   WATCH NAMESPACE
cabpt-system    provider.clusterctl.cluster.x-k8s.io/bootstrap-talos         BootstrapProvider        talos         v0.2.0
cacppt-system   provider.clusterctl.cluster.x-k8s.io/control-plane-talos     ControlPlaneProvider     talos         v0.1.0
capi-system     provider.clusterctl.cluster.x-k8s.io/cluster-api             CoreProvider             cluster-api   v0.3.20
sidero-system   provider.clusterctl.cluster.x-k8s.io/infrastructure-sidero   InfrastructureProvider   sidero        v0.3.0

NAMESPACE        NAME                                                                    READY   INITIALIZED   REPLICAS   READY REPLICAS   UNAVAILABLE REPLICAS
local-clusters   taloscontrolplane.controlplane.cluster.x-k8s.io/local-cluster-mgmt-cp   true    true          1          1

NAMESPACE        NAME                                                              CLUSTER              READY
local-clusters   metalcluster.infrastructure.cluster.x-k8s.io/local-cluster-mgmt   local-cluster-mgmt   true

NAMESPACE        NAME                                                                            READY
local-clusters   metalmachine.infrastructure.cluster.x-k8s.io/local-cluster-mgmt-cp-sdpgq        true
local-clusters   metalmachine.infrastructure.cluster.x-k8s.io/local-cluster-mgmt-workers-7lg59   true
local-clusters   metalmachine.infrastructure.cluster.x-k8s.io/local-cluster-mgmt-workers-jwnz9   true
local-clusters   metalmachine.infrastructure.cluster.x-k8s.io/local-cluster-mgmt-workers-lt82n   true
local-clusters   metalmachine.infrastructure.cluster.x-k8s.io/local-cluster-mgmt-workers-nx7pg   true
local-clusters   metalmachine.infrastructure.cluster.x-k8s.io/local-cluster-mgmt-workers-qmnhp   true

NAMESPACE        NAME                                                                              AGE
local-clusters   metalmachinetemplate.infrastructure.cluster.x-k8s.io/local-cluster-mgmt-cp        94m
local-clusters   metalmachinetemplate.infrastructure.cluster.x-k8s.io/local-cluster-mgmt-workers   94m

NAMESPACE   NAME                                                                                 READY
            serverbinding.infrastructure.cluster.x-k8s.io/00c03111-0000-0000-0000-dca63203f4f8   true
            serverbinding.infrastructure.cluster.x-k8s.io/00c03111-0000-0000-0000-dca63203f59a   true
            serverbinding.infrastructure.cluster.x-k8s.io/00c03111-0000-0000-0000-dca6321c2b8a   true
            serverbinding.infrastructure.cluster.x-k8s.io/00c03111-0000-0000-0000-dca6321c36d1   true
            serverbinding.infrastructure.cluster.x-k8s.io/00c03111-0000-0000-0000-dca632487ab4   true
            serverbinding.infrastructure.cluster.x-k8s.io/00c03112-0000-0000-0000-dca6327dcbba   true

NAMESPACE   NAME                                                KERNEL                                                                           INITRD                                                                                READY
            environment.metal.sidero.dev/default                https://github.com/talos-systems/talos/releases/download/v0.10.3/vmlinuz-amd64   https://github.com/talos-systems/talos/releases/download/v0.10.3/initramfs-amd64.xz   True
            environment.metal.sidero.dev/raspberrypi4-servers   https://github.com/talos-systems/talos/releases/download/v0.11.0/vmlinuz-arm64   https://github.com/talos-systems/talos/releases/download/v0.11.0/initramfs-arm64.xz   True

NAMESPACE   NAME                                                AVAILABLE   IN USE
            serverclass.metal.sidero.dev/any                    []          ["00c03111-0000-0000-0000-dca63203f4f8","00c03111-0000-0000-0000-dca63203f59a","00c03111-0000-0000-0000-dca6321c2b8a","00c03111-0000-0000-0000-dca6321c36d1","00c03111-0000-0000-0000-dca632487ab4","00c03112-0000-0000-0000-dca6327dcbba"]
            serverclass.metal.sidero.dev/raspberrypi4-servers   []          ["00c03111-0000-0000-0000-dca63203f4f8","00c03111-0000-0000-0000-dca63203f59a","00c03111-0000-0000-0000-dca6321c2b8a","00c03111-0000-0000-0000-dca6321c36d1","00c03111-0000-0000-0000-dca632487ab4","00c03112-0000-0000-0000-dca6327dcbba"]

NAMESPACE   NAME                                                           HOSTNAME   ACCEPTED   ALLOCATED   CLEAN   POWER
            server.metal.sidero.dev/00c03111-0000-0000-0000-dca63203f4f8   Pi0        true       true        false   on
            server.metal.sidero.dev/00c03111-0000-0000-0000-dca63203f59a   Pi7        true       true        false   on
            server.metal.sidero.dev/00c03111-0000-0000-0000-dca6321c2b8a   Pi4        true       true        false   on
            server.metal.sidero.dev/00c03111-0000-0000-0000-dca6321c36d1   Pi2        true       true        false   on
            server.metal.sidero.dev/00c03111-0000-0000-0000-dca632487ab4   Pi5        true       true        false   on
            server.metal.sidero.dev/00c03112-0000-0000-0000-dca6327dcbba   Pi1        true       true        false   on
#+end_example

*** Deleting the cluster
Delete the servers managed by CAPI
#+begin_src shell
CLUSTER_NAME=local-cluster-mgmt
NAMESPACE=local-clusters
SERVERS="$(kubectl -n ${NAMESPACE} get metalmachines -l cluster.x-k8s.io/cluster-name=${CLUSTER_NAME} -o=jsonpath='{range .items[*]}server/{.spec.serverRef.name} {end}')"
MACHINES="$(kubectl -n ${NAMESPACE} get machines -l cluster.x-k8s.io/cluster-name=${CLUSTER_NAME} -o=jsonpath='{range .items[*]}machine/{.metadata.name} {end}')"
METALMACHINES="$(kubectl -n ${NAMESPACE} get metalmachines -l cluster.x-k8s.io/cluster-name=${CLUSTER_NAME} -o=jsonpath='{range .items[*]}metalmachine/{.metadata.name} {end}')"
kubectl -n "${NAMESPACE}" delete ${METALMACHINES} ${SERVER} ${MACHINES}
#+end_src

#+RESULTS:
#+begin_example
metalmachine.infrastructure.cluster.x-k8s.io "local-cluster-mgmt-cp-4tfx9" deleted
metalmachine.infrastructure.cluster.x-k8s.io "local-cluster-mgmt-workers-bmj8c" deleted
metalmachine.infrastructure.cluster.x-k8s.io "local-cluster-mgmt-workers-hp7jd" deleted
metalmachine.infrastructure.cluster.x-k8s.io "local-cluster-mgmt-workers-hxg7f" deleted
metalmachine.infrastructure.cluster.x-k8s.io "local-cluster-mgmt-workers-vdglk" deleted
metalmachine.infrastructure.cluster.x-k8s.io "local-cluster-mgmt-workers-xz9cc" deleted
machine.cluster.x-k8s.io "local-cluster-mgmt-cp-k6rvc" deleted
machine.cluster.x-k8s.io "local-cluster-mgmt-workers-6f4b4cbf84-7vhfs" deleted
machine.cluster.x-k8s.io "local-cluster-mgmt-workers-6f4b4cbf84-fg7pf" deleted
machine.cluster.x-k8s.io "local-cluster-mgmt-workers-6f4b4cbf84-gtcc5" deleted
machine.cluster.x-k8s.io "local-cluster-mgmt-workers-6f4b4cbf84-px7bz" deleted
machine.cluster.x-k8s.io "local-cluster-mgmt-workers-6f4b4cbf84-tjct4" deleted
#+end_example

(optional) Remove servers with no status
#+begin_src shell
kubectl delete server $(kubectl -n local-clusters get server -o=json | jq -r '.items[] | select(.status==null) | .metadata.name')
#+end_src

#+RESULTS:
#+begin_example
kubectl delete server
#+end_example

Remove the cluster
#+begin_src shell
kubectl delete -f ./sidero/local-clusters/local-cluster-mgmt.yaml
#+end_src

#+RESULTS:
#+begin_example
Error from server (NotFound): error when deleting "./sidero/local-clusters/local-cluster-mgmt.yaml": clusters.cluster.x-k8s.io "local-cluster-mgmt" not found
Error from server (NotFound): error when deleting "./sidero/local-clusters/local-cluster-mgmt.yaml": metalclusters.infrastructure.cluster.x-k8s.io "local-cluster-mgmt" not found
Error from server (NotFound): error when deleting "./sidero/local-clusters/local-cluster-mgmt.yaml": metalmachinetemplates.infrastructure.cluster.x-k8s.io "local-cluster-mgmt-cp" not found
Error from server (NotFound): error when deleting "./sidero/local-clusters/local-cluster-mgmt.yaml": taloscontrolplanes.controlplane.cluster.x-k8s.io "local-cluster-mgmt-cp" not found
Error from server (NotFound): error when deleting "./sidero/local-clusters/local-cluster-mgmt.yaml": talosconfigtemplates.bootstrap.cluster.x-k8s.io "local-cluster-mgmt-workers" not found
Error from server (NotFound): error when deleting "./sidero/local-clusters/local-cluster-mgmt.yaml": machinedeployments.cluster.x-k8s.io "local-cluster-mgmt-workers" not found
Error from server (NotFound): error when deleting "./sidero/local-clusters/local-cluster-mgmt.yaml": metalmachinetemplates.infrastructure.cluster.x-k8s.io "local-cluster-mgmt-workers" not found
#+end_example

** Get TalosConfig
#+begin_src tmate :window mgm-cluster
export TALOSCONFIG=/tmp/local-cluster-mgmt-talosconfig
kubectl -n local-clusters get talosconfig -l cluster.x-k8s.io/cluster-name=local-cluster-mgmt -o=jsonpath='{.items[0].status.talosConfig}' > "${TALOSCONFIG}"
#+end_src

** Get KubeConfig
#+begin_src tmate :window mgm-cluster
export KUBECONFIG=/tmp/local-cluster-mgmt-kubeconfig
talosctl --talosconfig "${TALOSCONFIG}" -e 192.168.1.31 -n 192.168.1.31 kubeconfig "${KUBECONFIG}"
#+end_src

** Check health of Nodes
#+begin_src shell :wrap "SRC text"
export KUBECONFIG=/tmp/local-cluster-mgmt-kubeconfig
kubectl get nodes
#+end_src

#+RESULTS:
#+begin_SRC text
NAME   STATUS   ROLES    AGE     VERSION
pi0    Ready    <none>   82m     v1.21.2
pi1    Ready    <none>   82m     v1.21.2
pi2    Ready    <none>   82m     v1.21.2
pi4    Ready    <none>   5m12s   v1.21.2
pi5    Ready    <none>   5m36s   v1.21.2
pi7    Ready    <none>   82m     v1.21.2
#+end_SRC

** Fetch a list of all of the Pods
#+begin_src shell :wrap "SRC text"
export KUBECONFIG=/tmp/local-cluster-mgmt-kubeconfig
kubectl get pods -A
#+end_src

#+RESULTS:
#+begin_SRC text
NAMESPACE     NAME                          READY   STATUS    RESTARTS   AGE
kube-system   coredns-6ff77786fb-cdr8s      1/1     Running   5          83m
kube-system   coredns-6ff77786fb-jx4t2      1/1     Running   5          83m
kube-system   kube-apiserver-pi0            1/1     Running   4          81m
kube-system   kube-controller-manager-pi0   1/1     Running   11         82m
kube-system   kube-flannel-c4l95            1/1     Running   5          82m
kube-system   kube-flannel-hrflr            1/1     Running   10         82m
kube-system   kube-flannel-mqvq2            1/1     Running   10         82m
kube-system   kube-flannel-n47hn            1/1     Running   0          5m45s
kube-system   kube-flannel-x6b65            1/1     Running   4          82m
kube-system   kube-flannel-zqz22            1/1     Running   0          6m9s
kube-system   kube-proxy-br6pp              1/1     Running   5          82m
kube-system   kube-proxy-fdmlg              1/1     Running   0          6m9s
kube-system   kube-proxy-jxqmm              1/1     Running   5          82m
kube-system   kube-proxy-khmbb              1/1     Running   4          82m
kube-system   kube-proxy-mrhnb              1/1     Running   0          5m45s
kube-system   kube-proxy-nlz8k              1/1     Running   5          82m
kube-system   kube-scheduler-pi0            1/1     Running   11         81m
#+end_SRC
