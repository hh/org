#+TITLE: Local cluster
#+PROPERTY: header-args:shell+ :prologue "( " :epilogue " ) 2>&1 ; :"
#+PROPERTY: header-args:yaml+ :comments none
#+PROPERTY: header-args:patch+ :comments none

Setting up a local cluster.

* Prologue


* Prepare
** Save repo location
#+begin_src tmate :window prepare
export REPO_ROOT="${PWD}"
#+end_src

** Downloading a Talos RPi image
Download the Talos image to flash to a MicroSD card from GitHub
#+begin_src tmate :window prepare
cd $(mktemp -d)
curl -O -L \
  https://github.com/talos-systems/talos/releases/download/v0.10.3/metal-rpi_4-arm64.img.xz
export TALOS_METAL_RPI_IMG=${PWD}/*
#+end_src

Some Pis may require having the EEPROM updated, check [[https://www.talos.dev/docs/v0.10/single-board-computers/rpi_4/#updating-the-eeprom][the Talos docs]].

** Prepare MicroSD cards
Write the image to a MicroSD card
#+begin_src tmate :window prepare
export DISK_TO_USE_DEFAULT=/dev/sdb && \
  read -p "Enter the disk to use (default: '${DISK_TO_USE}'): " DISK_TO_USE && \
  sudo dd \
    if=${TALOS_METAL_RPI_IMG} \
    of="${DISK_TO_USE-$DISK_TO_USE_DEFAULT}" \
    status=progress \
    conv=fsync \
    bs=4M
#+end_src

** Install =talosctl=
To manage Talos on each node, =talosctl= is used to provision and manage
#+begin_src tmate :window prepare
curl -o ~/bin/talosctl -L \
  https://github.com/talos-systems/talos/releases/download/v0.10.3/talosctl-$(uname | tr '[:upper:]' '[:lower:]')-amd64
#+end_src

* Set up
** Discover node IPs
Each node that comes up will, of course, have an IP address.
I'm checking what the router says.

** Determine that nodes are live
#+begin_src tmate :window prepare
#export NODE_ADDRS=(192.168.1.111 192.168.1.127 192.168.1.234)
export NODE_ADDRS=(192.168.1.234)
#+end_src

** Ensure nodes are live
#+begin_src tmate :window prepare
for IP in ${NODE_ADDRS[*]}; do
    echo "Checking ${IP}:50000"
    nc -zv "${IP}" "50000"
done
#+end_src

** Generating the configuration
#+begin_src tmate :window prepare
talosctl gen config \
    ii-nz \
    https://192.168.1.100:6443 \
    --output-dir talos/ \
    --additional-sans k8s.ii.nz \
    --install-disk /dev/mmcblk0 \
    --install-image ghcr.io/talos-systems/installer:v0.10.3
#+end_src

** Modify the configuration
#+begin_src diff :tangle talos-config-patches.patch :comment none
diff --git a/talos/controlplane.yaml b/talos/controlplane.yaml
index bc87738..cf17a8a 100644
--- a/talos/controlplane.yaml
+++ b/talos/controlplane.yaml
@@ -35,7 +35,12 @@ machine:
     #         - rw

     # Provides machine specific network configuration options.
-    network: {}
+    network:
+      interfaces:
+        - interface: eth0
+          dhcp: true
+          vip:
+            ip: 192.168.1.100
     # # `interfaces` is used to define the network interface configuration.
     # interfaces:
     #     - interface: eth0 # The interface name.
@@ -214,6 +219,7 @@ machine:
     #               slot: 0 # Key slot number for luks2 encryption.
 # Provides cluster specific configuration options.
 cluster:
+    allowSchedulingOnMasters: true
     # Provides control plane specific configuration options.
     controlPlane:
         endpoint: https://192.168.1.100:6443 # Endpoint is the canonical controlplane endpoint, which can be an IP address or a DNS hostname.
diff --git a/talos/init.yaml b/talos/init.yaml
index ad6d34e..46bdafd 100644
--- a/talos/init.yaml
+++ b/talos/init.yaml
@@ -35,7 +35,12 @@ machine:
     #         - rw

     # Provides machine specific network configuration options.
-    network: {}
+    network:
+      interfaces:
+        - interface: eth0
+          dhcp: true
+          vip:
+            ip: 192.168.1.100
     # # `interfaces` is used to define the network interface configuration.
     # interfaces:
     #     - interface: eth0 # The interface name.
@@ -214,6 +219,7 @@ machine:
     #               slot: 0 # Key slot number for luks2 encryption.
 # Provides cluster specific configuration options.
 cluster:
+    allowSchedulingOnMasters: true
     # Provides control plane specific configuration options.
     controlPlane:
         endpoint: https://192.168.1.100:6443 # Endpoint is the canonical controlplane endpoint, which can be an IP address or a DNS hostname.

#+end_src

Apply patches
#+begin_src tmate :window prepare
patch -ruN -d talos/ < "${REPO_ROOT}/talos-config-patches.patch"
#+end_src

** Use talosconfig
#+NAME: export-talosconfig
#+begin_src tmate :window prepare
export TALOSCONFIG=$PWD/talos/talosconfig
#+end_src

Write the endpoint
#+begin_src tmate :window prepare
talosctl config endpoint 192.168.1.100
#+end_src

** Provisioning the first node
#+begin_src tmate :window prepare
talosctl apply-config --insecure --nodes "${NODE_ADDRS[0]}" --file talos/init.yaml
#+end_src

*** Ensure that the node is active
#+begin_src tmate :window prepare
talosctl health -e "${NODE_ADDRS[0]}" -n "${NODE_ADDRS[0]}"
#+end_src

** Provision all the nodes
#+begin_src tmate :window prepare
for IP in ${NODE_ADDRS[*]}; do
    talosctl apply-config --insecure --nodes "${IP}" --file talos/controlplane.yaml
done
#+end_src

*** Watch the health of all nodes, as they become active
#+begin_src tmate :window prepare
talosctl health -e "${NODE_ADDRS[0]}" -n "${NODE_ADDRS[0]}"
#+end_src

** Get kubeconfig
#+begin_src tmate :window prepare
talosctl kubeconfig -e 192.168.1.100 -n 192.168.1.100
#+end_src

** Get nodes
#+begin_src shell
kubectl get nodes
#+end_src

#+RESULTS:
#+begin_example
NAME                  STATUS   ROLES                  AGE     VERSION
talos-192-168-1-111   Ready    control-plane,master   16m     v1.21.1
talos-192-168-1-127   Ready    control-plane,master   8m2s    v1.21.1
talos-192-168-1-234   Ready    control-plane,master   7m43s   v1.21.1
#+end_example

* Validate
** Get pods
#+begin_src shell
kubectl get pods -A
#+end_src

#+RESULTS:
#+begin_example
NAMESPACE     NAME                                          READY   STATUS    RESTARTS   AGE
kube-system   coredns-fcc4c97fb-br6rd                       1/1     Running   0          17m
kube-system   coredns-fcc4c97fb-cfstz                       1/1     Running   0          17m
kube-system   kube-apiserver-talos-192-168-1-111            1/1     Running   0          14m
kube-system   kube-apiserver-talos-192-168-1-127            1/1     Running   0          7m23s
kube-system   kube-apiserver-talos-192-168-1-234            1/1     Running   0          7m55s
kube-system   kube-controller-manager-talos-192-168-1-111   1/1     Running   3          15m
kube-system   kube-controller-manager-talos-192-168-1-127   1/1     Running   0          7m23s
kube-system   kube-controller-manager-talos-192-168-1-234   1/1     Running   0          7m55s
kube-system   kube-flannel-5stx9                            1/1     Running   0          8m16s
kube-system   kube-flannel-9kcx2                            1/1     Running   0          7m56s
kube-system   kube-flannel-wxn5m                            1/1     Running   0          16m
kube-system   kube-proxy-6dzrl                              1/1     Running   0          7m56s
kube-system   kube-proxy-pb42s                              1/1     Running   0          8m16s
kube-system   kube-proxy-w5q56                              1/1     Running   0          16m
kube-system   kube-scheduler-talos-192-168-1-111            1/1     Running   3          15m
kube-system   kube-scheduler-talos-192-168-1-127            1/1     Running   0          7m23s
kube-system   kube-scheduler-talos-192-168-1-234            1/1     Running   0          7m55s
#+end_example

* Ensure set up
** Upload talos folder into Kubernetes secret
#+begin_src tmate :window prepare
kubectl -n kube-system create secret generic "talos-config" --from-file=talos/
#+end_src

Ensure that the files exist in the secret
#+begin_src shell
kubectl -n kube-system get secret talos-config -o yaml | yq e '.data | keys | .[]' -P -
#+end_src

#+RESULTS:
#+begin_example
controlplane.yaml
init.yaml
join.yaml
talosconfig
#+end_example

** Fetch Talos configs
Create a new temp directory
#+begin_src tmate :window prepare
cd $(mktemp -d)
#+end_src

Extract talos-config into directory
#+begin_src tmate :window prepare :noweb yes
TALOS_CONFIGS="$(mktemp -t talos-config-XXXXX)"
kubectl -n kube-system get secret talos-config -o yaml > "${TALOS_CONFIGS}"

mkdir -p talos/
for FILE in $(cat "${TALOS_CONFIGS}" | yq e '.data | keys | .[]' -P -); do
  echo $FILE
  cat "${TALOS_CONFIGS}" | yq e ".data.\"${FILE}\"" -P - | base64 --decode > "talos/${FILE}"
done
<<export-talosconfig>>
#+end_src

** Get node IPs from the cluster
#+begin_src tmate :window prepare
export NODE_ADDRS=$(kubectl get nodes -o yaml | yq e '.items[].status.addresses[] | select(.type=="InternalIP") | .address' -P -)
#+end_src

** Get the TalosConfig
#+begin_src tmate :window prepare
export TALOSCONFIG=$(mktemp /tmp/tmp.XXXXX)
kubectl -n local-clusters get talosconfig -l cluster.x-k8s.io/cluster-name=local-cluster-mgmt -o=jsonpath='{.items[0].status.talosConfig}' > "${TALOSCONFIG}"
#+end_src

** Get machinetype
#+begin_src tmate :window prepare
talosctl -e 192.168.1.100 -n "$(echo ${NODE_ADDRS} | tr ' ' ',')" get machinetype
#+end_src

** Shutdown RPis
#+begin_src tmate :window prepare
for IP in ${NODE_ADDRS[*]}; do
    talosctl shutdown -e 192.168.1.100 -n "${IP}"
done
#+end_src

** Reset all nodes to uninitialised Talos
#+begin_src tmate :window prepare
read -p "Are you sure you want to reset all nodes, effectively destroying the cluster? [Enter|C-c] " && \
(
  for IP in ${NODE_ADDRS[*]}; do
      talosctl -e "${IP}" -n "${IP}" reset --graceful=false --reboot --system-labels-to-wipe=EPHEMERAL,STATE
  done
)
#+end_src

* Workloads
** metallb
*** Prepare
Create a directory for the manifests and a namespace for the resources
#+begin_src shell :results silent
mkdir -p metallb
curl -o metallb/namespace.yaml -L https://raw.githubusercontent.com/metallb/metallb/v0.9.6/manifests/namespace.yaml
curl -o metallb/metallb.yaml -L https://raw.githubusercontent.com/metallb/metallb/v0.9.6/manifests/metallb.yaml
#+end_src

*** Configure
Using layer2 for ARP capabilities and provide a very sufficient 10 IP address range in a part of the network that is configure to not be used by DHCP.
#+begin_src yaml :tangle ./metallb/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - 192.168.1.20-192.168.1.30
#+end_src

*** Install
#+begin_src shell
kubectl apply -f metallb/namespace.yaml
kubectl -n metallb-system get secret memberlist 2> /dev/null \
    || kubectl -n metallb-system create secret generic memberlist --from-literal=secretkey="$(openssl rand -base64 128)"
kubectl -n metallb-system apply -f ./metallb/config.yaml
kubectl -n metallb-system apply -f ./metallb/metallb.yaml
#+end_src

#+RESULTS:
#+begin_example
namespace/metallb-system created
secret/memberlist created
configmap/config created
Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
podsecuritypolicy.policy/controller created
podsecuritypolicy.policy/speaker created
serviceaccount/controller created
serviceaccount/speaker created
clusterrole.rbac.authorization.k8s.io/metallb-system:controller created
clusterrole.rbac.authorization.k8s.io/metallb-system:speaker created
role.rbac.authorization.k8s.io/config-watcher created
role.rbac.authorization.k8s.io/pod-lister created
clusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller created
clusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker created
rolebinding.rbac.authorization.k8s.io/config-watcher created
rolebinding.rbac.authorization.k8s.io/pod-lister created
daemonset.apps/speaker created
deployment.apps/controller created
#+end_example

** Helm-Operator
Unfortunately the Helm-Operator project by FluxCD is both in maintenance mode and unsupported on arm64. Here in the prepare stage, I'm patching the current state of how things are to build an arm64 image. Ideally, this is all in a single Dockerfile and does not use Make scripts. I'm unsure what the future of Helm-Operator is, but I'd like to see and help support for architectures outta-the-box.

*** Prepare
Create a directory for the manifests and a namespace for the resources
#+begin_src shell :results silent
mkdir -p helm-operator
kubectl create namespace helm-operator --dry-run=client -o yaml \
  | kubectl apply -f -
#+end_src

*** Configure
Create local manifests to apply in the cluster
#+begin_src shell :results silent
curl -o ./helm-operator/helm-operator-crds.yaml -L https://raw.githubusercontent.com/fluxcd/helm-operator/1.2.0/deploy/crds.yaml

helm repo add fluxcd https://charts.fluxcd.io
helm template helm-operator --create-namespace fluxcd/helm-operator \
    --namespace helm-operator \
    --set helm.versions=v3 \
    --set image.repository=registry.gitlab.com/bobymcbobs/container-images/helm-operator \
    --set image.tag=1.2.0 \
      > ./helm-operator/helm-operator.yaml
#+end_src

*** Install
#+begin_src shell
kubectl apply -f ./helm-operator/helm-operator-crds.yaml
kubectl -n helm-operator apply -f ./helm-operator/helm-operator.yaml
#+end_src

#+RESULTS:
#+begin_example
Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition
customresourcedefinition.apiextensions.k8s.io/helmreleases.helm.fluxcd.io created
serviceaccount/helm-operator created
secret/helm-operator-git-deploy created
configmap/helm-operator-kube-config created
Warning: rbac.authorization.k8s.io/v1beta1 ClusterRole is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRole
clusterrole.rbac.authorization.k8s.io/helm-operator created
Warning: rbac.authorization.k8s.io/v1beta1 ClusterRoleBinding is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRoleBinding
clusterrolebinding.rbac.authorization.k8s.io/helm-operator created
service/helm-operator created
deployment.apps/helm-operator created
#+end_example

** nginx-ingress controller
*** Prepare

Create a directory for the manifests and a namespace for the resources
#+begin_src shell :results silent
mkdir -p nginx-ingress
kubectl create namespace nginx-ingress --dry-run=client -o yaml \
  | kubectl apply -f -
#+end_src

*** Configure
Ensuring that remote IP addresses will be forwarded as headers in the requests, using the fields in the /.spec.values.controller.service/ field.
Preferring that each nginx-ingress pod runs on a different node.
#+begin_src yaml :tangle ./nginx-ingress/nginx-ingress.yaml
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: nginx-ingress
  namespace: nginx-ingress
spec:
  releaseName: nginx-ingress
  chart:
    repository: https://kubernetes.github.io/ingress-nginx
    name: ingress-nginx
    version: 3.30.0
  values:
    controller:
      autoscaling:
        enabled: true
        minReplicas: 3
        maxReplicas: 10
        targetCPUUtilizationPercentage: 80
      service:
        type: LoadBalancer
        externalTrafficPolicy: Local
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                      - ingress-nginx
              topologyKey: "kubernetes.io/hostname"
    defaultBackend:
      enabled: false
#+end_src

*** Install
#+begin_src shell
kubectl -n nginx-ingress apply -f nginx-ingress/nginx-ingress.yaml
#+end_src

#+RESULTS:
#+begin_example
helmrelease.helm.fluxcd.io/nginx-ingress created
#+end_example

** local-path-provisioner
Currently used, to get-the-job-done.
My end goal is to use Rook+Ceph in-place, but I'm starting with this.

*** Prepare
Create a directory for the manifests and a namespace for the resources.
#+begin_src shell :results silent
mkdir -p local-path-provisioner
curl -o local-path-provisioner/local-path-provisioner.yaml -L https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
#+end_src

*** Install
#+begin_src shell
kubectl apply -f local-path-provisioner/local-path-provisioner.yaml
#+end_src

#+RESULTS:
#+begin_example
namespace/local-path-storage created
serviceaccount/local-path-provisioner-service-account created
clusterrole.rbac.authorization.k8s.io/local-path-provisioner-role created
clusterrolebinding.rbac.authorization.k8s.io/local-path-provisioner-bind created
deployment.apps/local-path-provisioner created
storageclass.storage.k8s.io/local-path created
configmap/local-path-config created
#+end_example

*** Finalise
Ensuring that local-path is the default StorageClass.
#+begin_src shell
kubectl patch storageclasses.storage.k8s.io local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
#+end_src

#+RESULTS:
#+begin_example
storageclass.storage.k8s.io/local-path patched
#+end_example

** CAPI + Sidero
Links:
- https://www.sidero.dev/docs/v0.3/getting-started/install-clusterapi/
- https://www.sidero.dev/docs/v0.3/guides/rpi4-as-servers/#rpi4-boot-process

*** Configure
#+begin_src yaml :tangle ./sidero-controller-manager-debug.yaml
apiVersion: v1
kind: Pod
metadata:
  name: sidero-debug
  namespace: sidero-system
spec:
  hostNetwork: true
  containers:
  - image: alpine:3.12
    name: sidero-debug
    securityContext:
      privileged: true
    volumeMounts:
      - mountPath: /var/lib/sidero/tftp
        name: tftp-folder
    command:
      - sh
      - -c
      - apk add tar && sleep infinity
  volumes:
    - name: tftp-folder
      persistentVolumeClaim:
        claimName: sidero-tftp
#+end_src
#+begin_src yaml :tangle ./sidero-controller-manager-tftp-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: sidero-tftp
  namespace: sidero-system
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
#+end_src
#+begin_src yaml :tangle ./sidero-controller-manager-patch.yaml
spec:
  template:
    spec:
      volumes:
        - name: tftp-folder
          persistentVolumeClaim:
            claimName: sidero-tftp
      containers:
      - name: manager
        volumeMounts:
          - mountPath: /var/lib/sidero/tftp
            name: tftp-folder
#+end_src
- TODO Sidero TFTP, for UEFI boot
  - share the /var/lib/sidero/tftp folder as a PVC with a alpine pod
- TODO copy UEFI boot into TFTP folder and RPI_EFI.fd from SD card

*** Install
#+begin_src tmate :window prepare
export SIDERO_METADATA_SERVER_HOST_NETWORK=true \
  SIDERO_METADATA_SERVER_PORT=9091 \
  SIDERO_CONTROLLER_MANAGER_API_ENDPOINT=192.168.1.21 \
  SIDERO_CONTROLLER_MANAGER_AUTO_ACCEPT_SERVERS=true \
  SIDERO_CONTROLLER_MANAGER_HOST_NETWORK=false \
  SIDERO_CONTROLLER_MANAGER_BOOT_FROM_DISK_METHOD=ipxe-sanboot

clusterctl init -b talos -c talos -i sidero:v0.3.0
#+end_src

*** Finalise
Patch the sidero-http service an IP
#+begin_src shell
kubectl -n sidero-system patch service sidero-http -p '{"spec":{"type":"LoadBalancer"}}'
#+end_src

#+RESULTS:
#+begin_example
service/sidero-http patched
#+end_example

Patch the sidero-tftp service an IP
#+begin_src shell
kubectl -n sidero-system patch service sidero-tftp -p '{"spec":{"type":"LoadBalancer"}}'
#+end_src

#+RESULTS:
#+begin_example
service/sidero-tftp patched
#+end_example

(assign a virtual IP across the network)

Check the IP address
#+begin_src shell
kubectl -n sidero-system get svc
#+end_src

#+RESULTS:
#+begin_example
NAME                                        TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)          AGE
sidero-controller-manager-metrics-service   ClusterIP      10.101.66.253   <none>         8443/TCP         7m4s
sidero-http                                 LoadBalancer   10.103.54.118   192.168.1.21   8081:31916/TCP   7m4s
sidero-tftp                                 LoadBalancer   10.96.159.209   192.168.1.22   69:30513/UDP     7m3s
#+end_example

Expose Sidero-HTTP as a HTTPs Ingress
#+begin_src yaml :tangle ./ingress-boot-ii-nz.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: boot-ii-nz
  namespace: sidero-system
spec:
  rules:
  - host: boot.ii.nz
    http:
      paths:
      - backend:
          service:
            name: sidero-http
            port:
              number: 8081
        path: /
        pathType: ImplementationSpecific
#+end_src

Apply the ingress
#+begin_src shell
kubectl apply -f ./ingress-boot-ii-nz.yaml
#+end_src

#+RESULTS:
#+begin_example
ingress.networking.k8s.io/boot-ii-nz configured
#+end_example

Create a PVC for the TFTP folder
#+begin_src shell
# TODO figure out how to use dnsmasq/dhcp for just PXE and Sidero CM for TFTP
kubectl apply -f ./sidero-controller-manager-tftp-pvc.yaml
kubectl -n sidero-system patch deployment sidero-controller-manager --patch-file ./sidero-controller-manager-patch.yaml
kubectl -n sidero-system delete pod -l app=sidero 2> /dev/null
#+end_src

#+RESULTS:
#+begin_example
persistentvolumeclaim/sidero-tftp unchanged
deployment.apps/sidero-controller-manager patched (no change)
pod "caps-controller-manager-5948c84db7-tfjvd" deleted
pod "sidero-controller-manager-68cd57f8db-476rk" deleted
pod "sidero-controller-manager-8694596b44-9sqfp" deleted
#+end_example

Create a Pod that's also got the TFTP mount
#+begin_src shell
kubectl -n sidero-system delete pod sidero-debug 2> /dev/null
kubectl apply -f ./sidero-controller-manager-debug.yaml
#+end_src

#+RESULTS:
#+begin_example
pod "sidero-debug" deleted
pod/sidero-debug created
#+end_example

Czech the content
#+begin_src shell
kubectl -n sidero-system exec -it sidero-debug -- ls -alh /var/lib/sidero/tftp/
#+end_src

#+RESULTS:
#+begin_example
Unable to use a TTY - input is not a terminal or the right kind of file
total 2M
drwxrwxrwx    2 root     root          88 Jul  2 06:27 .
drwxr-xr-x    3 root     root          18 Jul  2 06:26 ..
-rw-r--r--    1 root     root      968.6K Jul  4 22:12 ipxe-arm64.efi
-rw-r--r--    1 root     root      996.5K Jul  4 22:12 ipxe.efi
-rw-r--r--    1 root     root       81.0K Jul  4 22:12 undionly.kpxe
-rw-r--r--    1 root     root       81.0K Jul  4 22:12 undionly.kpxe.0
#+end_example

Copy assets in-place
#+begin_src tmate :window prepare
kubectl -n sidero-system cp sidero-debug:/var/lib/sidero /tmp/
#+end_src
(this will be used for uploading the TFTP root for DNSMASQ)

*** Debug
Logs
#+begin_src tmate :window sidero
kubectl -n sidero-system logs -l app=sidero -f
#+end_src

*** Remove
#+begin_src tmate :window prepare
clusterctl delete --all
#+end_src
(useful for iterating)

** PXE boot server (dnsmasq)
*** Prepare
#+begin_src shell :results silent
mkdir -p dnsmasq
kubectl create namespace dnsmasq --dry-run=client -o yaml | \
    kubectl apply -f -
#+end_src

*** Configure
Configure dnsmasq
#+begin_src text :tangle ./dnsmasq/dnsmasq.conf :comments none
#dnsmasq config, for a complete example, see:
#  http://oss.segetech.com/intra/srv/dnsmasq.conf

port=0
dhcp-range=192.168.1.0,proxy
pxe-service=0,"Raspberry Pi Boot"
pxe-prompt="PXE booting Talos from Sidero in",0
dhcp-boot=ipxe-arm64.efi,sidero
log-queries
log-dhcp

enable-tftp=*
tftp-root=/var/lib/sidero/tftp
#+end_src

Configure the container
#+begin_src dockerfile :tangle ./dnsmasq/Dockerfile :comments none
FROM alpine:3.12 AS final
RUN apk add --no-cache tcpdump curl dnsmasq-dnssec
# TODO run as non-root
RUN mkdir -p /etc/default/ && \
  echo -e "ENABLED=1\nIGNORE_RESOLVCONF=yes" > /etc/default/dnsmasq
ENTRYPOINT ["dnsmasq","--no-daemon"]
#+end_src

TFTP PVC
#+begin_src yaml :tangle ./dnsmasq/dnsmasq-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: dnsmasq-tftp
  namespace: dnsmasq
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
#+end_src

Configure the deployment
#+begin_src yaml :tangle ./dnsmasq/dnsmasq.yaml :comments none
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dnsmasq
  namespace: dnsmasq
  labels:
    nz.ii: dnsmasq
    app: dnsmasq
spec:
  strategy:
    type: Recreate
  replicas: 1
  selector:
    matchLabels:
      nz.ii: dnsmasq
  template:
    metadata:
      annotations:
        nz.ii/dnsmasq.conf-sha256sum: "${DNSMASQ_CONF_HASH}"
        nz.ii/dockerfile-sha256sum: "${DOCKERFILE_HASH}"
      labels:
        nz.ii: dnsmasq
        app: dnsmasq
    spec:
      hostNetwork: true
      containers:
      - name: dnsmasq
        image: registry.gitlab.com/ii/nz/dnsmasq:latest
        imagePullPolicy: Always
        volumeMounts:
          - name: config
            mountPath: /etc/dnsmasq
          - name: tftp-folder
            mountPath: /var/lib/sidero/tftp
        args:
          - -C
          - /etc/dnsmasq/dnsmasq.conf
        securityContext:
          capabilities:
            add:
              - NET_ADMIN
              - NET_RAW
              - SYS_ADMIN
        ports:
        - containerPort: 67
          hostPort: 67
          protocol: UDP
        - containerPort: 4011
          hostPort: 4011
          protocol: UDP
        - containerPort: 7472
          hostPort: 7472
          protocol: UDP
      volumes:
      - name: config
        configMap:
          name: dnsmasq-config
      - name: tftp-folder
        persistentVolumeClaim:
          claimName: dnsmasq-tftp
#+end_src

*** Build
#+begin_src tmate :window dnsmasq
kubectl build \
    --destination registry.gitlab.com/ii/nz/dnsmasq:latest \
    --snapshotMode=redo \
    --context=$PWD \
    --dockerfile ./dnsmasq/Dockerfile
#+end_src

*** Install
#+begin_src shell
kubectl -n dnsmasq create configmap dnsmasq-config --from-file=dnsmasq/dnsmasq.conf --dry-run=client -o yaml | \
    kubectl apply -f -
export DNSMASQ_CONF_HASH="$(sha256sum ./dnsmasq/dnsmasq.conf | awk '{print $1}')"
export DOCKERFILE_HASH="$(sha256sum ./dnsmasq/Dockerfile | awk '{print $1}')"
kubectl apply -f ./dnsmasq/dnsmasq-pvc.yaml
envsubst < ./dnsmasq/dnsmasq.yaml | kubectl apply -f -
#+end_src

#+RESULTS:
#+begin_example
configmap/dnsmasq-config configured
persistentvolumeclaim/dnsmasq-tftp unchanged
deployment.apps/dnsmasq unchanged
#+end_example

*** Validate
Get logs
#+begin_src tmate :window dnsmasq
kubectl -n dnsmasq logs -l app=dnsmasq --prefix -f
#+end_src

#+begin_src tmate :window prepare
for IP in ${NODE_ADDRS[*]}; do
    echo "Checking ${IP}:67"
    nc -zvu "${IP}" "67"
done
#+end_src

#+begin_src yaml :tangle ./dnsmasq/debug-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: dnsmasq-debug
  name: dnsmasq-debug
  namespace: dnsmasq
spec:
  hostNetwork: true
  containers:
  - image: alpine:3.12
    name: dnsmasq-debug
    securityContext:
      privileged: true
    command:
      - sleep
      - infinity
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - weight: 1
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: app
                  operator: In
                  values:
                    - dnsmasq
            topologyKey: "kubernetes.io/hostname"
  dnsPolicy: ClusterFirst
  restartPolicy: Always
#+end_src

#+begin_src shell
kubectl delete -f ./dnsmasq/debug-pod.yaml
#+end_src

Drop a shell
#+begin_src tmate :window tcpdump
kubectl -n dnsmasq exec -it dnsmasq-debug -- sh
#+end_src

Install tcpdump
#+begin_src tmate :window tcpdump
apk add tcpdump
#+end_src

List interfaces
#+begin_src tmate :window tcpdump
ip a
#+end_src

We'll use eth0, since that's the host network for the Pi

*** More
Scale to zero
#+begin_src shell
kubectl -n dnsmasq scale deployment dnsmasq --replicas=0
#+end_src

#+RESULTS:
#+begin_example
deployment.apps/dnsmasq scaled
#+end_example

Scale to one
#+begin_src shell
kubectl -n dnsmasq scale deployment dnsmasq --replicas=1
#+end_src

#+RESULTS:
#+begin_example
deployment.apps/dnsmasq scaled
#+end_example

*** Prepare assets
Download UEFI assets
#+begin_src tmate :window prepare
FILES="start4.elf fixup4.dat config.txt ipxe.efi ipxe-arm64.efi firmware overlays bcm2711-rpi-4-b.dtb"

cd /var/lib/sidero/tftp
for UUID in ${UUIDs}; do
    echo "${UUID}:"
    mkdir -p ${UUID}
    cd ${UUID}
    rm *
    for FILE in $FILES; do
        echo "- ${FILE}"
        ln -sf ../$FILE ./$FILE
    done
    cd -
done
VERSION=v1.28
ASSET=RPi4_UEFI_Firmware_${VERSION}.zip
EXTRACTED_DIR=/tmp/tftp
if [ ! -f "${HOME}/Downloads/${ASSET}" ]; then
    curl -o ${HOME}/Downloads/${ASSET} -L https://github.com/pftf/RPi4/releases/download/${VERSION}/${ASSET}
fi
mkdir -p "${EXTRACTED_DIR}"
unzip -o "${HOME}/Downloads/${ASSET}" -d "${EXTRACTED_DIR}"

for _SERIAL in servers/*; do
    SERIAL="${_SERIAL/servers\//}"
    echo "${SERIAL}:"
    mkdir -p "${EXTRACTED_DIR}/${SERIAL}"
    cp -f "${_SERIAL}/RPI_EFI.fd" "${EXTRACTED_DIR}/${SERIAL}/"
    cp -a tftp-root/SERIAL/* /tmp/tftp/${SERIAL}/
done
#+end_src

*** HOLD Copy TFTP contents to dnsmasq

Copy TFTP folder from sidero-controller-manager into dnsmasq's TFTP folder
#+begin_src shell
# echo "Copying TFTP out from Sidero Controller Manager"
# SCM_TFTP_FOLDER=/tmp
# kubectl -n sidero-system cp sidero-debug:/var/lib/sidero/tftp "${SCM_TFTP_FOLDER}/tftp"
# echo "Local contents of ${SCM_TFTP_FOLDER}/tftp"
# ls -alh "${SCM_TFTP_FOLDER}/tftp/"

echo "Copying local contents to dnsmasq"
DNSMASQ_POD_NAME="$(kubectl -n dnsmasq get pods -l app=dnsmasq -o=jsonpath='{.items[0].metadata.name}')"
kubectl -n dnsmasq cp "/tmp/tftp" "${DNSMASQ_POD_NAME}":/var/lib/sidero/
kubectl -n dnsmasq exec -it "${DNSMASQ_POD_NAME}" -- ls -alh /var/lib/sidero/tftp
#+end_src

#+RESULTS:
#+begin_example
Copying local contents to dnsmasq
Unable to use a TTY - input is not a terminal or the right kind of file
total 6M
drwxrwxrwx   11 root     root        4.0K Jul  6 02:53 .
drwxr-xr-x    3 root     root          18 Jul  6 01:33 ..
drwxr-xr-x    2 root     root         175 Jul  6 02:54 2bbd241a
drwxr-xr-x    2 root     root         175 Jul  6 02:54 2cb186c5
-rw-r--r--    1 1000     1000        1.9M Jul  6 02:53 RPI_EFI.fd
-rw-r--r--    1 1000     1000        5.3K Jul  6 02:53 Readme.md
drwxr-xr-x    2 root     root         175 Jul  6 02:54 bc3ebf28
drwxr-xr-x    2 root     root         175 Jul  6 02:54 bc3ef28
-rw-r--r--    1 1000     1000       48.1K Jul  6 02:53 bcm2711-rpi-4-b.dtb
-rw-r--r--    1 1000     1000       48.1K Jul  6 02:53 bcm2711-rpi-400.dtb
-rw-r--r--    1 1000     1000       48.7K Jul  6 02:53 bcm2711-rpi-cm4.dtb
drwxr-xr-x    2 root     root         175 Jul  6 02:54 c3052218
-rw-r--r--    1 1000     1000         206 Jul  6 02:53 config.txt
drwxr-xr-x    2 root     root         175 Jul  6 02:54 dd24784d
drwxr-xr-x    2 root     root         175 Jul  6 02:54 ebc28a3f
drwxr-xr-x    3 root     root          55 Jul  6 02:53 firmware
-rw-r--r--    1 1000     1000        5.3K Jul  6 02:53 fixup4.dat
-rw-r--r--    1 1000     1000      968.6K Jul  5 04:04 ipxe-arm64.efi
-rw-r--r--    1 1000     1000      996.5K Jul  5 04:04 ipxe.efi
drwxr-xr-x    2 root     root          30 Jul  6 02:53 overlays
-rw-r--r--    1 1000     1000        2.1M Jul  6 02:54 start4.elf
-rw-r--r--    1 1000     1000       81.0K Jul  5 04:04 undionly.kpxe
-rw-r--r--    1 1000     1000       81.0K Jul  5 04:04 undionly.kpxe.0
#+end_example

* Preparing RPis for network booting
The following steps must be performed on each RPi

** Flash the latest network EEPROM firmware
1. Fetch the latest release of EEPROM (network) from the [[https://github.com/raspberrypi/rpi-eeprom/releases][GitHub Repo]]
2. Write the contents of the zip file to a fat32 formatted microSD card
3. Insert and boot the microSD card on the RPi
4. Wait until the green screen before unplugging the power for the RPi

** Bring up and configure the UEFI firmware
With a spare keyboard plugged into the target RPi,
1. Fetch a release of RPI4_UEFI firmware (currently using /v1.28/) from the [[https://github.com/pftf/RPi4/releases][GitHub Repo]]
2. Write the contents of the zip file to a fat32 formatted microSD card
3. Insert and boot the microSD card
4. Enter the UEFI set up by hitting /Esc/

*** Configure the UEFI firmware
1. Remove Memory limit: In /Device Manager -> Raspberry Pi Configuration -> Advanced Configuration/, set /Limit RAM to 3 GB/ to /Disabled/; F10 + Y to save.
2. Max out CPU clock: In /Device Manager -> Raspberry Pi Configuration -> CPU Configuration/, set /CPU clock/ to /Max/; F10 + Y to save.
3. Declare the iPXE HTTP boot URI: In /Device Manager -> Network Device List -> <MAC ADDRESS> -> HTTP Boot Configuration/, set /Input the description/ to /boot.ii.nz/ and /Boot URI/ to http://boot.ii.nz/tftp/ipxe-arm64.efi; F10 + Y to save.
4. Tidy up the boot order: In /Boot Maintenance Manager -> Boot Options -> Delete Boot Option/, ensure that the following options remain (in no specific order):
   - /boot.ii.nz/
   - /SD/MMC .../
   /Commit Changes and Exit/ to save.
4. Restructure the boot order: In /Boot Maintenance Manager -> Boot Options -> Change Boot Order/, set the order to:
   - /boot.ii.nz/
   - /SD/MMC .../
   /Commit Changes and Exit/ to save.

Once complete, /Esc/ the entire way out to the main menu and hit reset. When the RPi starts booting again, unplug from power before it reaches a source to boot from.

Now, on the SD card with the UEFI firmware, it the file /RPI_EFI.fd/ must be copied into the [[./servers][./servers]] folder, by the board serial number.
Is it useful to find the serial number when the RPi is booted with no network or SD card (located on /board: <...> <SERIAL NUMBER> .../).

* Bringing up servers with Sidero
Declare some common configuration
#+begin_src yaml :tangle ./sidero/local-cluster-rpi-template.yaml
apiVersion: cluster.x-k8s.io/v1alpha3
kind: Cluster
metadata:
  name: ${CLUSTER_NAME}
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 10.244.0.0/16
    services:
      cidrBlocks:
        - 10.96.0.0/12
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
    kind: MetalCluster
    name: ${CLUSTER_NAME}
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1alpha3
    kind: TalosControlPlane
    name: ${CLUSTER_NAME}-cp
---
apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
kind: MetalCluster
metadata:
  name: ${CLUSTER_NAME}
spec:
  controlPlaneEndpoint:
    host: ${CONTROL_PLANE_ENDPOINT}
    port: ${CONTROL_PLANE_PORT}
---
apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
kind: MetalMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-cp
spec:
  template:
    spec:
      serverClassRef:
        apiVersion: metal.sidero.dev/v1alpha1
        kind: ServerClass
        name: ${CONTROL_PLANE_SERVERCLASS}
---
apiVersion: controlplane.cluster.x-k8s.io/v1alpha3
kind: TalosControlPlane
metadata:
  name: ${CLUSTER_NAME}-cp
spec:
  version: ${KUBERNETES_VERSION}
  replicas: ${CONTROL_PLANE_MACHINE_COUNT}
  infrastructureTemplate:
    kind: MetalMachineTemplate
    apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
    name: ${CLUSTER_NAME}-cp
  controlPlaneConfig:
    init:
      generateType: init
      talosVersion: ${TALOS_VERSION}
      configPatches:
        - op: add
          path: /machine/network
          value:
            interfaces:
              - interface: eth0
                dhcp: true
                vip:
                  ip: ${CONTROL_PLANE_ENDPOINT}
    controlplane:
      generateType: controlplane
      talosVersion: ${TALOS_VERSION}
      configPatches:
        - op: add
          path: /machine/network
          value:
            interfaces:
              - interface: eth0
                dhcp: true
                vip:
                  ip: ${CONTROL_PLANE_ENDPOINT}
---
apiVersion: bootstrap.cluster.x-k8s.io/v1alpha3
kind: TalosConfigTemplate
metadata:
  name: ${CLUSTER_NAME}-workers
spec:
  template:
    spec:
      generateType: join
      talosVersion: ${TALOS_VERSION}
---
apiVersion: cluster.x-k8s.io/v1alpha3
kind: MachineDeployment
metadata:
  name: ${CLUSTER_NAME}-workers
spec:
  clusterName: ${CLUSTER_NAME}
  replicas: ${WORKER_MACHINE_COUNT}
  selector:
    matchLabels: null
  template:
    spec:
      version: ${KUBERNETES_VERSION}
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1alpha3
          kind: TalosConfigTemplate
          name: ${CLUSTER_NAME}-workers
      clusterName: ${CLUSTER_NAME}
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
        kind: MetalMachineTemplate
        name: ${CLUSTER_NAME}-workers
---
apiVersion: infrastructure.cluster.x-k8s.io/v1alpha3
kind: MetalMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-workers
spec:
  template:
    spec:
      serverClassRef:
        apiVersion: metal.sidero.dev/v1alpha1
        kind: ServerClass
        name: ${WORKER_SERVERCLASS}
#+end_src

Declare the Environment for the RPis
#+begin_src yaml :tangle ./sidero/rpi-environment.yaml
apiVersion: metal.sidero.dev/v1alpha1
kind: Environment
metadata:
  name: raspberrypi4-servers
spec:
  initrd:
    url: https://github.com/talos-systems/talos/releases/download/v0.10.3/initramfs-arm64.xz
  kernel:
    args:
    - console=tty0
    - console=ttyS0
    - consoleblank=0
    - earlyprintk=ttyS0
    - ima_appraise=fix
    - ima_hash=sha512
    - ima_template=ima-ng
    - init_on_alloc=1
    - initrd=initramfs.xz
    - nvme_core.io_timeout=4294967295
    - printk.devkmsg=on
    - pti=on
    - random.trust_cpu=on
    - slab_nomerge=
    - talos.config=http://192.168.1.21:8081/configdata?uuid=${uuid}
    - talos.platform=metal
    url: https://github.com/talos-systems/talos/releases/download/v0.10.3/vmlinuz-arm64
#+end_src

Declare the ServerClass to use for RPis
#+begin_src yaml :tangle ./sidero/rpi-serverclass.yaml
apiVersion: metal.sidero.dev/v1alpha1
kind: ServerClass
metadata:
  name: raspberrypi4-servers
spec:
  environmentRef:
    name: raspberrypi4-servers
  configPatches:
    - op: add
      path: /cluster/allowSchedulingOnMasters
      value: true
    - op: replace
      path: /machine/install
      value:
        disk: /dev/mmcblk1
        image: ghcr.io/talos-systems/installer:v0.10.3
        bootloader: true
        wipe: false
        force: false
  qualifiers:
    cpu:
      - manufacturer: Broadcom
        version: "BCM2711 (ARM Cortex-A72)"
    systemInformation:
      - manufacturer: "Raspberry Pi Foundation"
        productName: "Raspberry Pi 4 Model B"
#+end_src

Apply the ServerClass and Environment
#+begin_src shell :results silent
kubectl apply \
    -f ./sidero/rpi-serverclass.yaml \
    -f ./sidero/rpi-environment.yaml
#+end_src

Create a namespace for the clusters
#+begin_src shell :results silent
kubectl create ns local-clusters
#+end_src

Generate config
#+begin_src shell :results silent
export \
    CONTROL_PLANE_ENDPOINT=192.168.1.31 \
    CONTROL_PLANE_PORT=6443 \
    CONTROL_PLANE_SERVERCLASS=raspberrypi4-servers \
    KUBERNETES_VERSION=v1.20.1 \
    TALOS_VERSION=v1.10.3 \
    WORKER_SERVERCLASS=raspberrypi4-server
clusterctl config cluster -n local-clusters local-cluster-mgmt --from ./sidero/local-cluster-rpi-template.yaml > ./sidero/local-clusters/local-cluster-mgmt.yaml
#+end_src

Bring up the workload cluster
#+begin_src shell
kubectl apply -f ./sidero/local-clusters/local-cluster-mgmt.yaml
#+end_src

#+RESULTS:
#+begin_example
cluster.cluster.x-k8s.io/local-cluster-mgmt created
metalcluster.infrastructure.cluster.x-k8s.io/local-cluster-mgmt created
metalmachinetemplate.infrastructure.cluster.x-k8s.io/local-cluster-mgmt-cp created
taloscontrolplane.controlplane.cluster.x-k8s.io/local-cluster-mgmt-cp created
talosconfigtemplate.bootstrap.cluster.x-k8s.io/local-cluster-mgmt-workers created
machinedeployment.cluster.x-k8s.io/local-cluster-mgmt-workers created
metalmachinetemplate.infrastructure.cluster.x-k8s.io/local-cluster-mgmt-workers created
#+end_example

*** Debug
See the provisioning
#+begin_src shell
kubectl get servers,machines,clusters,serverclasses -A
#+end_src

#+RESULTS:
#+begin_example
NAME                                                           HOSTNAME   ACCEPTED   ALLOCATED   CLEAN   POWER
server.metal.sidero.dev/00c03111-0000-0000-0000-dca6321c2b8a   Pi4        true
server.metal.sidero.dev/00c03111-0000-0000-0000-dca632487ab4   Pi5        true       false       false   on

NAMESPACE        NAME                                                   PROVIDERID   PHASE          VERSION
local-clusters   machine.cluster.x-k8s.io/local-cluster-mgmt-cp-v8kgn                Provisioning   v1.20.1

NAMESPACE        NAME                                          PHASE
local-clusters   cluster.cluster.x-k8s.io/local-cluster-mgmt   Provisioned

NAMESPACE   NAME                                                AVAILABLE                                  IN USE
            serverclass.metal.sidero.dev/any                    ["00c03111-0000-0000-0000-dca632487ab4"]   ["00c03111-0000-0000-0000-dca63203f59a"]
            serverclass.metal.sidero.dev/raspberrypi4-servers   ["00c03111-0000-0000-0000-dca632487ab4"]   ["00c03111-0000-0000-0000-dca63203f59a"]
#+end_example

See all things CAPI and Sidero
#+begin_src shell
kubectl get $(kubectl api-resources | grep -E 'x-k8s|sidero' | awk '{print $1}' | xargs | tr ' ' ',') -A
#+end_src

#+RESULTS:
#+begin_example
NAMESPACE        NAME                                                                 AGE
local-clusters   talosconfig.bootstrap.cluster.x-k8s.io/local-cluster-mgmt-cp-dztg7   144m

NAMESPACE        NAME                                                                        AGE
local-clusters   talosconfigtemplate.bootstrap.cluster.x-k8s.io/local-cluster-mgmt-workers   145m

NAMESPACE        NAME                                          PHASE
local-clusters   cluster.cluster.x-k8s.io/local-cluster-mgmt   Provisioned

NAMESPACE        NAME                                                            PHASE     REPLICAS   READY   UPDATED   UNAVAILABLE
local-clusters   machinedeployment.cluster.x-k8s.io/local-cluster-mgmt-workers   Running

NAMESPACE        NAME                                                   PROVIDERID                                      PHASE     VERSION
local-clusters   machine.cluster.x-k8s.io/local-cluster-mgmt-cp-k9rhs   sidero://00c03111-0000-0000-0000-dca63203f59a   Running   v1.20.1

NAMESPACE        NAME                                                                REPLICAS   AVAILABLE   READY
local-clusters   machineset.cluster.x-k8s.io/local-cluster-mgmt-workers-6f4b4cbf84

NAMESPACE       NAME                                                         TYPE                     PROVIDER      VERSION   WATCH NAMESPACE
cabpt-system    provider.clusterctl.cluster.x-k8s.io/bootstrap-talos         BootstrapProvider        talos         v0.2.0
cacppt-system   provider.clusterctl.cluster.x-k8s.io/control-plane-talos     ControlPlaneProvider     talos         v0.1.0
capi-system     provider.clusterctl.cluster.x-k8s.io/cluster-api             CoreProvider             cluster-api   v0.3.20
sidero-system   provider.clusterctl.cluster.x-k8s.io/infrastructure-sidero   InfrastructureProvider   sidero        v0.3.0

NAMESPACE        NAME                                                                    READY   INITIALIZED   REPLICAS   READY REPLICAS   UNAVAILABLE REPLICAS
local-clusters   taloscontrolplane.controlplane.cluster.x-k8s.io/local-cluster-mgmt-cp   true    true          1          1

NAMESPACE        NAME                                                              CLUSTER              READY
local-clusters   metalcluster.infrastructure.cluster.x-k8s.io/local-cluster-mgmt   local-cluster-mgmt   true

NAMESPACE        NAME                                                                       READY
local-clusters   metalmachine.infrastructure.cluster.x-k8s.io/local-cluster-mgmt-cp-sg7b8   true

NAMESPACE        NAME                                                                              AGE
local-clusters   metalmachinetemplate.infrastructure.cluster.x-k8s.io/local-cluster-mgmt-cp        145m
local-clusters   metalmachinetemplate.infrastructure.cluster.x-k8s.io/local-cluster-mgmt-workers   145m

NAMESPACE   NAME                                                                                 READY
            serverbinding.infrastructure.cluster.x-k8s.io/00c03111-0000-0000-0000-dca63203f59a   true

NAMESPACE   NAME                                                KERNEL                                                                           INITRD                                                                                READY
            environment.metal.sidero.dev/default                https://github.com/talos-systems/talos/releases/download/v0.10.3/vmlinuz-amd64   https://github.com/talos-systems/talos/releases/download/v0.10.3/initramfs-amd64.xz   True
            environment.metal.sidero.dev/raspberrypi4-servers   https://github.com/talos-systems/talos/releases/download/v0.10.3/vmlinuz-arm64   https://github.com/talos-systems/talos/releases/download/v0.10.3/initramfs-arm64.xz   True

NAMESPACE   NAME                                                AVAILABLE                                                                         IN USE
            serverclass.metal.sidero.dev/any                    ["00c03111-0000-0000-0000-dca6321c2b8a","00c03111-0000-0000-0000-dca632487ab4"]   ["00c03111-0000-0000-0000-dca63203f59a"]
            serverclass.metal.sidero.dev/raspberrypi4-servers   ["00c03111-0000-0000-0000-dca6321c2b8a","00c03111-0000-0000-0000-dca632487ab4"]   ["00c03111-0000-0000-0000-dca63203f59a"]

NAMESPACE   NAME                                                           HOSTNAME   ACCEPTED   ALLOCATED   CLEAN   POWER
            server.metal.sidero.dev/00c03111-0000-0000-0000-dca63203f59a   Pi7        true       true        false   on
            server.metal.sidero.dev/00c03111-0000-0000-0000-dca6321c2b8a   Pi4        true       false       false   on
            server.metal.sidero.dev/00c03111-0000-0000-0000-dca632487ab4   Pi5        true       false       true    on
#+end_example

** Get TalosConfig
#+begin_src tmate :window mgm-cluster
export TALOSCONFIG=$(mktemp /tmp/tmp.XXXXX)
kubectl -n local-clusters get talosconfig -l cluster.x-k8s.io/cluster-name=local-cluster-mgmt -o=jsonpath='{.items[0].status.talosConfig}' > "${TALOSCONFIG}"
#+end_src

** Get KubeConfig
#+begin_src tmate :window mgm-cluster
export KUBECONFIG=$(mktemp /tmp/tmp.XXXXX)
talosctl --talosconfig "${TALOSCONFIG}" -e 192.168.1.31 -n 192.168.1.31 kubeconfig "${KUBECONFIG}"
#+end_src
