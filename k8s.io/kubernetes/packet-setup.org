#+TITLE: Setup Kubernetes on Packet
#+AUTHOR: Hippie Hacker
#+EMAIL: hh@ii.coop
#+CREATOR: ii.coop
#+DATE: 19th of February, 2019
#+PROPERTY: header-args:shell :results output code verbatim replace
#+NOPROPERTY: header-args:shell+ :prologue ". /etc/profile.d/homedir-go-path.sh\n. /etc/profile.d/system-go-path.sh\nexec 2>&1\n"
#+PROPERTY: header-args:shell+ :epilogue ":\n"
#+PROPERTY: header-args:shell+ :wrap "EXAMPLE :noeval t"
#+PROPERTY: header-args:shell+ :dir "/ssh:root@139.178.88.146:/home/"
#+PROPERTY: header-args:tmate  :socket (symbol-value 'socket)
#+PROPERTY: header-args:tmate+ :session (concat (user-login-name) ":" (nth 4 (org-heading-components)))
#+NOPROPERTY: header-args:tmate+ :prologue (concat "cd " org-file-dir "\n")
#+REVEAL_ROOT: http://cdn.jsdelivr.net/reveal.js/3.0.0/
#+STARTUP: showeverything
* TLDR

[[/ssh:root@139.178.88.146:/root/]]

Comma b s
#+NAME: write remote config files
#+BEGIN_SRC elisp :results none
(org-babel-tangle)
#+END_SRC

#+NAME: read kubeadm config from existing cluster
#+BEGIN_SRC shell :wrap "SRC yaml" :eval never
kubectl -n kube-system get cm kubeadm-config -oyaml
#+END_SRC


#+NAME: reset master
#+BEGIN_SRC tmate :noweb yes
  <<Reset Master>> -f
  <<delete pvcs>>
#+END_SRC

#+NAME: install k8s
#+BEGIN_SRC tmate :noweb yes
  <<kubeadm pull images>>
  <<kubeadm init>>
  <<configure kubectl>>
#+END_SRC  

#+NAME: foo
#+BEGIN_SRC tmate :noweb yes
#+END_SRC

#+NAME: configure k8s
#+BEGIN_SRC tmate :noweb yes
  <<flannel setup>>
  <<create tiller service account>>
  <<helm init w/ tiller>>
  <<setup hostpath-provisioner>>
  <<install dashboard>>
  <<create admin service account>>
  <<create admin cluster role binding>>
  <<configure kubeexp>>
  k get pods --all-namespaces          
#+END_SRC

http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/

* Objectives
  This article runs through deploying a kubernetes cluster onto a remote machine running Ubuntu 18, and then deploying gitlab to that cluster.
  By the end of this you should be able to access your own instance of gitlab at a domain of your choosing, and know that it's running with the scale and resilience advantage of kubernetes.
* External Requirements
  - A remote machine running Ubuntu 18.
    - we are using machines created through [[packet.com][Packet]]
  - A local machine running the ii version of spacemacs.
    - You'll need our specific configuration, and packages, to be able to run the code commands in the rest of this article.
  - A domain name you own, or have admin rights to its registry.
  - Dev accounts set up with...mailgun, google, and gitlab? (this is for our secrets.env later, and am not sure where those secrets come from.)
* Document Setup
  This document is designed so you can simply tap on the code blocks you read here, and they will execute on your specific remote machine.
  You will also be able to _see_ the code executing (and answer any prompts that come up or errors that appear) through a convenient tmate session.  To make this work well, there's some pre-setup that you'll need to do.

You'll only do this setup at the creation of the machine, or start of a new project, you won't have to do this setup for just iterating over and refining this setup, or if you take down and bring k8s back up on the same machine.

** Install and setup tmate on your remote machine.
   
   First, we need to bring in tmate.
   
  #+NAME: Installing tmate on remote machine 
  #+BEGIN_EXAMPLE shell
  sudo apt install tmate xclip
  #+END_EXAMPLE
  
  Tmate requires having SSH keys that it can use just to setup the pairing sessions.  So generate some new ones now.
  (NOTE: Create new ones, dont' copy your personal SSH keys over.)
  
  #+NAME: Generate SSH keys
  #+BEGIN_EXAMPLE shell
  # on your remote machine 
  ssh-keygen -o -a 100 -t ed25519
  #+END_EXAMPLE

  QUESTION: How Secure does this need to be? We just did =ssh-keygen=, but I am unsure if that was just to quickly get to the meat of the project, or if that followed best practice.  I'd like whatever we document to best practice.

  ANSWER: This ssh-key will likely only be used by tmate. The public side is never stored any where for auth. It looks like it's nesessary as an artifact of the way tmate uses ssh to connect to the tmate.io (or pair.ii.nz) server.
  
  Then, let's check it works.
  #+NAME: Start up TMATE
  #+BEGIN_EXAMPLE shell
  # on your remote machine 
  tmate
  #+END_EXAMPLE
  
  This should bring up a tmux session, but with an ssh link shown at the bottom. Success!  Kill it by pressing =Ctrl-d=
  

** tmate config

We could use tangle + ssh shell's to setup the remote box.

#+NAME: tmate config
#+BEGIN_SRC sh :eval never
set-option -g set-clipboard on
set-option -g mouse on
set-option -g history-limit 50000
set -g tmate-identity ""
set -s escape-time 0
#+END_SRC

The default tmate.io server supports https/web urls. pair.ii.nz does not, but it's faster for us (and run by ii)

#+NAME: tmate config
#+BEGIN_SRC sh :eval never
;set -g tmate-server-host pair.ii.nz
;set -g tmate-server-port 22
;set -g tmate-server-rsa-fingerprint   "f9:af:d5:f2:47:8b:33:53:7b:fb:ba:81:ba:37:d3:b9"
;set -g tmate-server-ecdsa-fingerprint   "32:44:b3:bb:b3:0a:b8:20:05:32:73:f4:9a:fd:ee:a8"
#+END_SRC


** Add the IP address of your remote machine to this document.
You'll need to add the address to two parts of this document.  This will ensure the commands we run happen in the place you want.

- [[#+PROPERTY: header-args:shell+ :dir "/ssh:USERNAME@IP_ADDRESS:/home/"][#+PROPERTY: header-args:shell+ :dir "/ssh:USERNAME@IP_ADDRESS:/home/"]]
- [[# eval: (set (make-local-variable 'ssh-user-host) "root@139.178.88.146")][In the eval section at bottom of doc: (set (make-local-variable 'ssh-user-host) "USERNAME@IPADDRESS")]]

You can also add extra IPs to your Packet box, and enable them in /etc/network/interfaces
With a /29 you can allocate the next 4 ip addresses beyond your bond0 IP.

I was originally adding them to a loopback:

#+NAME: /etc/network/interfaces
#+BEGIN_SRC config
auto lo:1
iface lo:1 inet static
    address 139.178.88.147
    netmask 255.255.255.248
auto lo:2
iface lo:2 inet static
    address 139.178.88.148
    netmask 255.255.255.248
auto lo:3
iface lo:3 inet static
    address 139.178.88.149
    netmask 255.255.255.248
auto lo:4
iface lo:4 inet static
    address 139.178.88.150
    netmask 255.255.255.248
#+END_SRC

Second approach added aliases to bond interface

#+NAME: /etc/network/interfaces
#+BEGIN_SRC text
  auto bond0:1
  iface bond0:1 inet static
      address 139.178.88.147
      netmask 255.255.255.248
  auto bond0:2
  iface bond0:2 inet static
      address 139.178.88.148
      netmask 255.255.255.248
  auto bond0:3
  iface bond0:3 inet static
      address 139.178.88.149
      netmask 255.255.255.248
  auto bond0:4
  iface bond0:4 inet static
      address 139.178.88.150
      netmask 255.255.255.248
#+END_SRC

#+BEGIN_SRC shell :eval never
ifup bond0:1
ifup bond0:2
ifup bond0:3
ifup bond0:4
#+END_SRC
   
**  Refresh this document and test it works
*** Refresh Document
   The easiest way to refresh is to type =SPC SPC normal-mode= (alternately =M-x normal-mode=). Spacemacs should prompt you, asking if you want to evaluate the variables.  Say yes.  

Then, open a new terminal window and paste (however you paste on your machine).  You should see a command given to you like:

#+BEGIN_EXAMPLE shell
ssh -tAX root@REMOTEIP \
-L /tmp/USERNAME.packet-setup.iisocket:/tmp/USERNAME.packet-setup.iisocket \
tmate -S /tmp/zz.packet-setup.iisocket new-session -A \
-s zz -n main \"tmate wait tmate-ready \&\& tmate display \
-p \'#{tmate_ssh}\' \| xclip -i -sel p -f \| xclip -i -sel c \&\& bash --login\"
#+END_EXAMPLE

Press enter.  This will bring you into a tmate session on your remote machine.  Once this has started up, the sharable link to this session gets copied to your clipboard.  So you can paste that to a friend, if you are pairing.
*** Test Tmate Works
Lastly!  Let's make sure it works.  Run this code block by pressing =,,= while your cursor is anywhere on it.

#+NAME: Test that Tmate Works
#+BEGIN_SRC tmate
echo "it worked!"
pwd
#+END_SRC
 
Check your remote machine, you'll see a new window called "Check TMATe Worked" and you'll see the echo and pwd commands executed.

If that's the case, you're good to go!
* Pre-Kubernetes Sanity Checks
  Before we dive into installing Kubernetes, we want to double-check our box has available ports, so that all the various kubernetes pods can talk to each other (and we can talk to them)

** Check Required ports

 These are our required ports

| Protocol | Direction | Port Range | Purpose                 | Used By                 |
| TCP      | Inbound   |      6443* | Kubernetes API          | serverAll               |
| TCP      | Inbound   |  2379-2380 | etcd server client      | APIkube-apiserver, etcd |
| TCP      | Inbound   |      10250 | Kubelet API             | Self, Control plane     |
| TCP      | Inbound   |      10251 | kube-scheduler          | Self                    |
| TCP      | Inbound   |      10252 | kube-controller-manager | Self                    |

Run this netstat and check for software listening on these ports.
If you see the LISTENing ports that match the port ranges listed above, youll need to reconfigure the host or k8s.

  #+NAME: Check Required Ports
  #+BEGIN_SRC shell :results replace table drawer :wrap (symbol-value 'nil) :exports both
  netstat -lntu \
    | grep Proto\\\|LISTEN \
    | grep -v tcp6\\\|127.0.0 \
    | sed 's:Local.*Address:Local Foreign:'
  #+END_SRC

QUESTION: They don't for us, what is 6443* and 10250-10252 used for, and is it okay that they don't show up in this netstat command?
ANSWER: We don't want them to at this point, we want to make sure we can use those ports later.

  #+RESULTS: Check Required Ports
  #+BEGIN_RESULTS
  | Proto | Recv-Q | Send-Q |      Local | Foreign   | State  |
  | tcp   |      0 |      0 | 0.0.0.0:22 | 0.0.0.0:* | LISTEN |
  #+END_RESULTS

* install Kubernetes Tools
https://kubernetes.io/docs/setup/independent/install-kubeadm/

Before anything else, we want our remote machine to have kubeadm, kubelet, and kubectl.


** Install kubeadm, kubelet, and kubectl
   #+NAME: Install kubeadm, kubelet, and kubectl, disable swap
   #+BEGIN_SRC tmate
     apt-get update \
       && apt-get install -y apt-transport-https
     curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg \
       | apt-key add -
     echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" \
       >> /etc/apt/sources.list.d/kubernetes.list
     apt-get update \
       && apt-get install -y kubelet kubeadm kubectl kubernetes-cni
   #+END_SRC
   
   Verify it worked.  The results should largely match our example included below
   #+NAME: kubectl, kubeadm, kubelet versions
   #+BEGIN_SRC shell :results output verbatim
     echo "==kubelet=="
     kubelet --version
     echo "==kubectl=="
     kubectl version
     echo "==kubeadm=="
     kubeadm version
   #+END_SRC

   #+RESULTS: kubectl, kubeadm, kubelet versions
   #+BEGIN_EXAMPLE :noeval t
   ==kubelet==
   Kubernetes v1.13.3
   ==kubectl==
   Client Version: version.Info{Major:"1", Minor:"13", GitVersion:"v1.13.3", GitCommit:"721bfa751924da8d1680787490c54b9179b1fed0", GitTreeState:"clean", BuildDate:"2019-02-01T20:08:12Z", GoVersion:"go1.11.5", Compiler:"gc", Platform:"linux/amd64"}
   ==kubeadm==
   kubeadm version: &version.Info{Major:"1", Minor:"13", GitVersion:"v1.13.3", GitCommit:"721bfa751924da8d1680787490c54b9179b1fed0", GitTreeState:"clean", BuildDate:"2019-02-01T20:05:53Z", GoVersion:"go1.11.5", Compiler:"gc", Platform:"linux/amd64"}
   #+END_EXAMPLE

   #+NAME: Example Working versions of kubectl, kubeadm, kubelet
   #+BEGIN_EXAMPLE :noeval t
   ==kubelet==
   Kubernetes v1.13.3
   ==kubectl==
   Client Version: version.Info{Major:"1", Minor:"13", GitVersion:"v1.13.3", GitCommit:"721bfa751924da8d1680787490c54b9179b1fed0", GitTreeState:"clean", BuildDate:"2019-02-01T20:08:12Z", GoVersion:"go1.11.5", Compiler:"gc", Platform:"linux/amd64"}
   ==kubeadm==
   kubeadm version: &version.Info{Major:"1", Minor:"13", GitVersion:"v1.13.3", GitCommit:"721bfa751924da8d1680787490c54b9179b1fed0", GitTreeState:"clean", BuildDate:"2019-02-01T20:05:53Z", GoVersion:"go1.11.5", Compiler:"gc", Platform:"linux/amd64"}
   #+END_EXAMPLE

   NOTE: If these don't show anything, try running the install script again. It might have installed curl and then stopped...so try again now that curl is installed.
** Connfigure kubectl defaults / completion

#+BEGIN_SRC tmate
  # add autocomplete permanently to your bash shell.
  echo "source <(kubectl completion bash)" >> ~/.bashrc
  # use k as an alias to kubectl
  echo alias k=kubectl >> ~/.bashrc 
  echo complete -F __start_kubectl k >> ~/.bashrc 
#+END_SRC
** Disable Swap

#+BEGIN_SRC tmate
  # disable currently enabled swap
  swapoff -a
  # Comment out any swap to disable automounting
  sed -e '/swap/ s/^#*/#/' -i /etc/fstab
#+END_SRC
** reconfigure_ssh

Configure SSH to only listen on the primary IP

We need to be able to use port 22 on the other IPs for tmate/gitlab instances

#+BEGIN_SRC tmate
  sed -e '/ListenAddress 0.0.0.0/ s/0.0.0.0/139.178.88.146/' -i /etc/ssh/sshd_config
  grep ListenAddress /etc/ssh/sshd_config
#+END_SRC

#+NAME: ssh listening only on one IP
#+BEGIN_SRC shell
lsof -i -n -P | grep LISTEN | grep IPv4 | grep -v 127.0.0.1 | grep ssh
#+END_SRC

#+RESULTS: ssh listening only on one IP
#+BEGIN_EXAMPLE :noeval t
sshd      235392            root    3u  IPv4 8983782      0t0  TCP 139.178.88.146:22 (LISTEN)
#+END_EXAMPLE

** Install Docker

We are using docker.io from ubuntu, should possibly switch to upstream docker-ce

#+BEGIN_SRC tmate
  apt-get update \
    && apt-get install -y docker.io
  systemctl restart docker
  systemctl enable docker
  systemctl status docker
#+END_SRC

* Configure and Deploy Kubernetes
 Our setup is largely insired by [[https://www.packet.com/developers/guides/kubeless-on-packet-cloud/][Packet's Guide to deploying on kubernetes]]
** Reset Kubernetes
  This is here for iteration. 
#+NAME: Reset Master  
#+BEGIN_SRC tmate
kubeadm reset --force
#+END_SRC
** Delete PVCS
   Also for iteration loops. Not necessary if this is the first time, as you have not created any storage yet.
#+NAME: delete pvcs
#+BEGIN_SRC tmate
rm -rf /volumes/pvc-*
#+END_SRC

** Initialize Master K8s Node
   NOTE: This script is relevant as of k8s version 1.13.  If you get an error about mismatching versions, it's likely that there's a new stable version of k8s.  Look to waht that is and then adjust the last line in this script accordingly.
   
#+NAME: kubeadm pull images
#+BEGIN_SRC tmate
kubeadm config images pull \
 --kubernetes-version stable-1.13
#+END_SRC

We need to be able to specify a particular IP for services of type NodePort.
https://kubernetes.io/docs/concepts/services-networking/service/#nodeport
https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/#options

#+BEGIN_QUOTE
 If you want to specify particular IP(s) to proxy the port, you can set the
 --nodeport-addresses flag in kube-proxy to particular IP block(s)

  (which is supported since Kubernetes v1.10).

  A comma-delimited list of IP blocks (e.g. 10.0.0.0/8, 1.2.3.4/32) is used to
 filter addresses local to this node.
#+END_QUOTE

https://godoc.org/k8s.io/kubernetes/pkg/proxy/apis/config#KubeProxyConfiguration

#+BEGIN_QUOTE
nodePortAddresses is the --nodeport-addresses value for kube-proxy process.

Values must be valid IP blocks.

These values are as a parameter to select the interfaces where nodeport works.

In case someone would like to expose a service on localhost for local visit
 and some other interfaces for particular purpose,
 a list of IP blocks would do that.

If set it to "127.0.0.0/8", kube-proxy will only select the loopback interface for NodePort.

If set it to a non-zero IP block, kube-proxy will filter that down to just the IPs that applied to the node.

An empty string slice is meant to select all network interfaces.
  NodePortAddresses []string
#+END_QUOTE
#+BEGIN_QUOTE
Range of host ports (

beginPort-endPort,
single port or
beginPort+offset,
inclusive

)

that may be consumed in order to proxy service traffic.
If (unspecified, 0, or 0-0) then ports will be randomly chosen.
#+END_QUOTE

#+NAME: kubeadm defaults
#+BEGIN_SRC tmate
kubeadm config print init-defaults --component-configs KubeProxyConfiguration,KubeletConfiguration
#+END_SRC
[[https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta1#ClusterConfiguration]]
extraArgs, audit policy etc

#+NAME: kubeadm config
#+BEGIN_SRC yaml :tangle (concat "/ssh:" ssh-user-host ":kubeadm-config.yaml")
  apiVersion: kubeadm.k8s.io/v1beta1
  kind: InitConfiguration
  localAPIEndpoint:
    advertiseAddress: "139.178.88.146"
  nodeRegistration:
    taints: [] # defaults to NoSchedule on role=master
  ---
  apiVersion: kubeadm.k8s.io/v1beta1
  kind: ClusterConfiguration
  kubernetesVersion: v1.13.3
  controlPlaneEndpoint: ""
  networking:
    podSubnet: "10.244.0.0/16"
    serviceSubnet: "10.96.0.0/12"
  apiServer:
    extraArgs:
      service-node-port-range: "22-10000" # allow more ports via API
  ---
  apiVersion: kubeproxy.config.k8s.io/v1alpha1
  kind: KubeProxyConfiguration
  nodePortAddresses:
    - "139.178.88.144/29" # default is null
  portRange: "22-10000" # Proxy also needs port range to ensure we can use 22,80,443,and friends
#+END_SRC

#+NAME: kubeadm init
#+BEGIN_SRC tmate :eval ask
  kubeadm init --config kubeadm-config.yaml
#+END_SRC

Might be worth looking into kube-proxy IPVS mode:
https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/ipvs/README.md

#+NAME: Initialize Master
#+BEGIN_SRC tmate
kubeadm init \
 --pod-network-cidr=10.244.0.0/16 \
 --apiserver-advertise-address=$(\
   ip address show label bond0:1 | sed -n 's/[ ]*inet \([^\/]*\).*/\1/p') \
 --kubernetes-version stable-1.13
#+END_SRC

** Configure kubectl
When it is installed, you can check it with the following

#+NAME: configure kubectl
#+BEGIN_SRC tmate
mkdir -p $HOME/.kube
sudo cp /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
#+END_SRC
  https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/

Since we are using public IPs, we can now use the file we create here to manage this k8s from anywhere.

#+NAME: get kubectl working locally
#+BEGIN_SRC shell :eval never
export KUBECONFIG=~/kubeconfig
ssh root@139.178.88.146 kubectl config view --merge --minify --flatten > ~/kubeconfig
kubectl get nodes
#+END_SRC

* kubectl
  Let's check that kubectl works.  A good way to do that is to ask it to run commands against our cluster.
  
  #+NAME: Check Kubectl Works
  #+BEGIN_SRC shell
    kubectl get nodes 
  #+END_SRC

  #+RESULTS: Check Kubectl Works
  #+BEGIN_EXAMPLE :noeval t
  NAME         STATUS     ROLES    AGE     VERSION
  ci.ii.coop   NotReady   master   3m42s   v1.13.3
  #+END_EXAMPLE

Look inside the kube-proxy pod to see the rendered config file:

#+NAME: apiserver cmdline
#+BEGIN_SRC shell
kubectl exec -ti --namespace=kube-system \
  `kubectl get pods \
    --namespace=kube-system \
    -l component=kube-apiserver \
    -o jsonpath='{.items[].metadata.name}'` \
   cat /proc/1/cmdline \
   | sed s/--/\\n--/g \
   | sed s/.\$//g \
   | grep -C2 service-node-port-range
#+END_SRC 

#+RESULTS: apiserver cmdline
#+BEGIN_EXAMPLE :noeval t
kube-apiserver
--authorization-mode=Node,RBAC
--service-node-port-range=22-10000
--advertise-address=139.178.88.146
--allow-privileged=true
#+END_EXAMPLE

#+NAME: proxy-config 
#+BEGIN_SRC shell :wrap "SRC yaml"
kubectl exec -ti --namespace=kube-system \
  `kubectl get pods --namespace=kube-system \
    -l k8s-app=kube-proxy \
    -o jsonpath='{.items[].metadata.name}'` \
   cat /var/lib/kube-proxy/config.conf \
   | grep -C3 portRange\\\|nodePort
#+END_SRC 

#+RESULTS: proxy-config
#+BEGIN_SRC yaml
kind: KubeProxyConfiguration
metricsBindAddress: 127.0.0.1:10249
mode: ""
nodePortAddresses:
- 139.178.88.144/29
oomScoreAdj: -999
portRange: 22+10000
resourceContainer: /kube-proxy
udpIdleTimeout: 250ms
#+END_SRC

Might be good to show the taints, notready status via kubectl commands before untainting and applying network.

#+NAME: master node taints
#+BEGIN_SRC shell :wrap "SRC json" :prologue (symbol-value nil) :epilogue (symbol-value nil)
  kubectl get node \
    $(kubectl get node \
      -l node-role.kubernetes.io/master \
      --namespace=kube-system \
      -o jsonpath='{.items[*].metadata.name}')\
    -o json \
  | jq -M .spec.taints
  #+END_SRC

  #+RESULTS: master node taints
  #+BEGIN_SRC json
  [
    {
      "effect": "NoSchedule",
      "key": "node-role.kubernetes.io/master"
    },
    {
      "effect": "NoSchedule",
      "key": "node.kubernetes.io/not-ready"
    }
  ]
  #+END_SRC

Usually pods don't get scheduled on the master, due to the NoSchedule taint.
If we overrode the defalut ~nodeRegistration: taints: [{key: node-role.kubernetes.io/master}]~
Setting the default taints to an empty list.

  #+NAME: untaint the master
  #+BEGIN_SRC tmate
    kubectl taint nodes --all node-role.kubernetes.io/master-
  #+END_SRC

#+NAME: master node taints after untaint for master role
#+BEGIN_SRC shell :wrap "SRC json" :prologue (symbol-value nil) :epilogue (symbol-value nil)
  kubectl get node \
    $(kubectl get node \
      -l node-role.kubernetes.io/master \
      --namespace=kube-system \
      -o jsonpath='{.items[*].metadata.name}')\
    -o json \
  | jq -M .spec.taints
  #+END_SRC

  #+RESULTS: master node taints after untaint for master role
  #+BEGIN_SRC json
  [
    {
      "effect": "NoSchedule",
      "key": "node.kubernetes.io/not-ready"
    }
  ]
  #+END_SRC

  #+NAME: Status Ready Condition of Master Node
  #+BEGIN_SRC shell :wrap "SRC json" :prologue (symbol-value nil) :epilogue (symbol-value nil)
  kubectl get node \
    $(kubectl get node -l node-role.kubernetes.io/master --namespace=kube-system -o jsonpath='{.items[*].metadata.name}')\
     -o json \
  | jq -M '.status.conditions[] | select(.type=="Ready")'
  #+END_SRC

We likely haven't setup a CNI / network layer yet, no our node doesn't have a status.condition["Ready"} of True.

  #+RESULTS: Status Ready Condition of Master Node
  #+BEGIN_SRC json
  {
    "lastHeartbeatTime": "2019-02-22T01:19:56Z",
    "lastTransitionTime": "2019-02-22T01:10:52Z",
    "message": "runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized",
    "reason": "KubeletNotReady",
    "status": "False",
    "type": "Ready"
  }
  #+END_SRC

* networking
  
https://docs.projectcalico.org/v3.5/usage/calicoctl/install

TODO add other options linked in our dm channel (flannel, weaver)
  We were able to look at all our nodes but =coredns= was still pending, and not ready.  As long as =coredns= is down, we cannot schedule or have nodes talk to one another.  In other words, nothing will work.

~flannel~
  Flannel is a CNI (container network interface) that essentially helps get our network up.  So let's install it. 
  
#+NAME: flannel setup
#+BEGIN_SRC tmate
FLANNEL_RELEASE=v0.11.0
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/$FLANNEL_RELEASE/Documentation/kube-flannel.yml
#+END_SRC

  #+NAME: Status Ready Condition of Master Node after networking
  #+BEGIN_SRC shell :wrap "SRC json" :prologue (symbol-value nil) :epilogue (symbol-value nil)
  kubectl get node \
    $(kubectl get node -l node-role.kubernetes.io/master --namespace=kube-system -o jsonpath='{.items[*].metadata.name}')\
     -o json \
  | jq -M '.status.conditions[] | select(.type=="Ready")'
  #+END_SRC

  #+RESULTS: Status Ready Condition of Master Node after networking
  #+BEGIN_SRC json
  {
    "lastHeartbeatTime": "2019-02-22T01:20:16Z",
    "lastTransitionTime": "2019-02-22T01:20:16Z",
    "message": "kubelet is posting ready status. AppArmor enabled",
    "reason": "KubeletReady",
    "status": "True",
    "type": "Ready"
  }
  #+END_SRC

#+NAME: node should be ready
#+BEGIN_SRC shell
  kubectl get nodes
#+END_SRC	

#+RESULTS: node should be ready
#+BEGIN_EXAMPLE :noeval t
NAME         STATUS   ROLES    AGE     VERSION
ci.ii.coop   Ready    master   3m52s   v1.13.3
#+END_EXAMPLE

* helm

#+NAME: install helm
#+BEGIN_SRC tmate
curl -L \
  https://storage.googleapis.com/kubernetes-helm/helm-v2.12.3-linux-amd64.tar.gz \
  | tar xvz -f - --strip-components 1 -C /usr/local/bin linux-amd64/helm linux-amd64/tiller
#+END_SRC

#+NAME: create tiller service account
#+BEGIN_SRC tmate
  kubectl --namespace kube-system create serviceaccount tiller
  kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
#+END_SRC

#+NAME: helm init w/ tiller
#+BEGIN_SRC tmate
  helm init --service-account tiller
#+END_SRC

* disks
  
~hostpath-provisioner~

Uses local directories, created dynamically, to serve up PVs to PVCs
https://github.com/rimusz/hostpath-provisioner#dynamic-provisioning-of-kubernetes-hostpath-volumes
https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner/tree/master/examples/hostpath-provisioner

- torchbox (claims it's intended for production use)
https://github.com/torchbox/k8s-hostpath-provisioner

#+NAME: format and mount a drive under /volumes
#+BEGIN_SRC tmate :eval query
echo "Are you sure? if not C-c!!! Next step formats a drive!"
sleep 5
mkdir /volumes
mkfs.ext4 /dev/nvme0n1
echo /dev/nvme0n1 /volumes ext4 errors=remount-ro 0 1 >> /etc/fstab
mount /volumes
#+END_SRC

Ensure you tangle / write ~hostpath-provisioner.yaml~ file to the host then run the following command:

#+NAME: setup hostpath-provisioner
#+BEGIN_SRC tmate
kubectl apply -f ~/hostpath-provisioner.yaml
#+END_SRC

#+NAME: hostpath-provisioner.yaml
#+BEGIN_SRC yaml :tangle (concat "/ssh:" ssh-user-host ":hostpath-provisioner.yaml")
  # we added a default storage class and a testpvc
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    name: default
    annotations:
      storageclass.kubernetes.io/is-default-class: "true"
  provisioner: torchbox.com/hostpath
  parameters:
    pvDir: /volumes
  ---
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: testpvc
  spec:
    accessModes:
    - ReadWriteMany
    resources:
      requests:
        storage: 5Gi
  # vim:set sw=2 ts=2 et:
  #
  # Copyright (c) 2017 Torchbox Ltd.
  #
  # Permission is granted to anyone to use this software for any purpose,
  # including commercial applications, and to alter it and redistribute it
  # freely. This software is provided 'as-is', without any express or implied
  # warranty.
  ---
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    namespace: kube-system
    name: hostpath-provisioner

  ---

  apiVersion: rbac.authorization.k8s.io/v1beta1
  kind: ClusterRoleBinding
  metadata:
    name: hostpath-provisioner
  subjects:
  - kind: ServiceAccount
    name: hostpath-provisioner
    namespace: kube-system
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: system:persistent-volume-provisioner

  ---

  # The default system:persistent-volume-provisioner role in Kubernetes 1.8 is
  # insufficient:
  #
  # I1007 18:09:10.073558       1 controller.go:874] cannot start watcher for PVC default/testpvc: events is forbidden: User "system:serviceaccount:kube-system:hostpath-provisioner" cannot list events in the namespace "default": access denied

  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    name: hostpath-provisioner-extra
  rules:
  - apiGroups:
    - ""
    resources:
    - events
    verbs:
    - create
    - patch
    - update
    - list
    - get
    - watch

  ---

  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: hostpath-provisioner-extra
  subjects:
  - kind: ServiceAccount
    namespace: kube-system
    name: hostpath-provisioner
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: hostpath-provisioner-extra

  ---

  apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    name: hostpath-provisioner
    namespace: kube-system
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: hostpath-provisioner
    strategy:
      type: Recreate
    template:
      metadata:
        labels:
          app: hostpath-provisioner

      spec:
        serviceAccountName: hostpath-provisioner

        volumes:
        - name: volumes
          hostPath:
            path: /volumes

        containers:
        - name: hostpath-provisioner
          image: torchbox/k8s-hostpath-provisioner:latest

          volumeMounts:
          - name: volumes
            mountPath: /volumes

          resources:
            limits:
              cpu: 100m
              memory: 64Mi
            requests:
              cpu: 100m
              memory: 64Mi

#+END_SRC

#+name: inspect hostpath-provisioner logs
#+begin_src tmate

kubectl get pvc testpvc
kubectl logs -f `kubectl get pod -l app=hostpath-provisioner --all-namespaces -o jsonpath='{..metadata.name}'` --namespace=kube-system
#+END_SRC

* webui

https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/

#+NAME: install dashboard
#+BEGIN_SRC tmate
kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/recommended/kubernetes-dashboard.yaml
kubectl apply -f admin-serviceaccount.yaml
#+END_SRC

Forward your localhost:8001 to the remote localhost:8001 (start the proxy once, other folks/hosts will connect to the one proxy)

#+BEGIN_SRC :eval never
ssh -L 8001:localhost:8001 root@139.178.88.146 kubectl proxy
#+END_SRC

#+NAME: admin-serviceaccount.yaml
#+BEGIN_SRC yaml :tangle (concat "/ssh:" ssh-user-host ":admin-serviceaccount.yaml")
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: admin-user
    namespace: kube-system
  ---
  apiVersion: rbac.authorization.k8s.io/v1beta1
  kind: ClusterRoleBinding
  metadata:
    name: admin-user
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: cluster-admin
  subjects:
  - kind: ServiceAccount
    name: admin-user
    namespace: kube-system
#+END_SRC

#+NAME: admin_token
#+BEGIN_SRC shell
kubectl get -n kube-system -o json secret \
  `kubectl get secret -n kube-system | grep admin-user | awk '{print $1}'` \
  | jq -r .data.token \
  | base64 --decode
#+END_SRC

#+RESULTS: admin_token
#+BEGIN_EXAMPLE :noeval t
#+END_EXAMPLE

http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/
* tui

#+NAME: install kubexp
#+BEGIN_SRC tmate
KUBEXP_RELEASE="0.8.1"
wget https://github.com/alitari/kubexp/releases/download/${KUBEXP_RELEASE}/kubexp
chmod +x kubexp
#+END_SRC

#+NAME: configure kubexp
#+BEGIN_SRC tmate
kubectl apply -f https://github.com/alitari/kubexp/raw/master/rbac-default-clusteradmin.yaml
#+END_SRC
* Alternatives
** PVCs
*** rook =~ ceph (but managed natively by k8s)
  
#+NAME: Add rook helm repo
#+BEGIN_SRC tmate
  helm repo add rook-stable https://charts.rook.io/stable
#+END_SRC

#+NAME: Install rook-ceph-system
#+BEGIN_SRC tmate
  helm install \
       --namespace rook-ceph-system \
       rook-stable/rook-ceph 
#+END_SRC

** CNI's
*** calico
 #+NAME: install calicoctl
 #+BEGIN_SRC tmate
 wget curl -O calicoctl -L  https://github.com/projectcalico/calicoctl/releases/download/v3.5.1/calicoctl ;  chmod +x calicoctl  ; ./calicoctl version
 #+END_SRC

 #+BEGIN_SRC tmate
   kubectl apply -f \
https://docs.projectcalico.org/v3.5/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calicoctl.yaml
 #+END_SRC

 #+BEGIN_SRC tmate
 kubectl exec -ti -n kube-system calicoctl -- /calicoctl get profiles -o wide
 #+END_SRC

* Glossary
  - Control Plane Version :: 
  - helm ::
  - ingress :: 
  - Kubeless :: 
  - Kubelet ::
  - Kubeadm ::
  - Kubectl ::
  - RBAC ::
  - rook ::
  - rook-ceph ::
  - service-account :: 
  - tiller ::

* Footer
** hiccups
  For a new box, it won't have tmate yet.  So we need to ssh in first, install tmate.
  Tmate requires ssh keys to work properly.  We needed to run =ssh-keygen= on the remote box for it to work properly.
  if the tmate is not working, check if tmate is not forwarding due to sockets.  When that is the case, you need to rm the socket from your remote box and from your local box.  It is likely in =/tmp/$username.packet.ii.socket=

#+NAME: start documentation session
#+BEGIN_SRC shell :noeval yes
ssh -tAX root@139.178.88.146 \
tmate -S /tmp/$USER.emacs.iisocket new-session -A -s ii -n emacs \
\"tmate wait tmate-ready \&\& sleep 2 \&\& \
  echo \\\`tmate display -p \'#{tmate_ssh}\'\\\` \\\# left \
  \| xclip -i -sel p -f \| xclip -i -sel c \&\& \
  emacs -nw .\"
#+END_SRC

#+NAME: start repl session
#+BEGIN_SRC shell :noeval yes
ssh -tAX kind@arm.cncf.ci \
tmate -S /tmp/kind.kind-ci-box.iisocket new-session -A -s kind -n main \
\"tmate wait tmate-ready \&\& sleep 2 \&\& \
  echo \\\`tmate display -p \'#{tmate_ssh}\'\\\` \\\# right \
  \| xclip -i -sel p -f \| xclip -i -sel c \&\& \
  bash --login\"
#+END_SRC
* Footnotes

# xclip on then off, due to this being a remote box
# eval: (xclip-mode 1) 
# Local Variables:
# eval: (set (make-local-variable 'ssh-user-host) "root@139.178.88.146")
# eval: (set (make-local-variable 'org-file-dir) (file-name-directory buffer-file-name))
# eval: (set (make-local-variable 'user-buffer) (concat user-login-name "." (file-name-base buffer-file-name)))
# eval: (set (make-local-variable 'tmpdir) (make-temp-file (concat "/dev/shm/" user-buffer "-") t))
# eval: (set (make-local-variable 'socket) (concat "/tmp/" user-buffer ".iisocket"))
# eval: (set (make-local-variable 'select-enable-clipboard) t)
# eval: (set (make-local-variable 'select-enable-primary) t)
# eval: (set (make-local-variable 'start-tmate-command) (concat "tmate -S " socket " new-session -A -s " user-login-name " -n main \\\"tmate wait tmate-ready \\&\\& sleep 2 \\&\\& tmate display -p \'\\\#{tmate_ssh}\\ \\\\#\\ " user-buffer "\\ \\\\#\\ \\\#{tmate_web}\' \\| xclip -i -sel p -f \\| xclip -i -sel c \\&\\& bash --login\\\""))
# eval: (xclip-mode 1) 
# eval: (gui-select-text (concat "rm " socket "; ssh -tAX " ssh-user-host " -L " socket ":" socket " " start-tmate-command))
# eval: (xclip-mode 0) 
# org-babel-tmate-session-prefix: ""
# org-babel-tmate-default-window-name: "main"
# org-use-property-inheritance: t
# End:
