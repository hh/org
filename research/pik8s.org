#+PROPERTY: header-args:shell :dir /ssh:ubuntu@192.168.1.101:
#+PROPERTY: header-args:shell+ :results code
#+PROPERTY: header-args:shell+ :prologue "(\n" 
#+PROPERTY: header-args:shell+ :epilogue ") 2>&1\n:\n"
#+PROPERTY: header-args:shell+ :wrap EXAMPLE
* Start Over
** allover
  #+name: start over
  #+begin_src shell
  <<kubeadm reset>>
  <<lvm reset>>
  <<rook reset>>
  sudo rm -rf /etc/cni/net.d
  #+end_src

  #+RESULTS: start over
  #+begin_EXAMPLE
  [reset] Reading configuration from the cluster...
  [reset] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
  [preflight] Running pre-flight checks
  [reset] Removing info for node "ubuntu" from the ConfigMap "kubeadm-config" in the "kube-system" Namespace
  W0102 08:12:00.167924   28338 removeetcdmember.go:61] [reset] failed to remove etcd member: error syncing endpoints with etc: etcdclient: no available endpoints
  .Please manually remove this etcd member using etcdctl
  [reset] Stopping the kubelet service
  [reset] Unmounting mounted directories in "/var/lib/kubelet"
  [reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
  [reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]
  [reset] Deleting contents of stateful directories: [/var/lib/etcd /var/lib/kubelet /var/lib/dockershim /var/run/kubernetes /var/lib/cni]

  The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

  The reset process does not reset or clean up iptables rules or IPVS tables.
  If you wish to reset iptables, you must do so manually by using the "iptables" command.

  If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
  to reset your system's IPVS tables.

  The reset process does not clean your kubeconfig files and you must remove them manually.
  Please, check the contents of the $HOME/.kube/config file.
    No command with matching syntax recognised.  Run 'vgremove --help' for more information.
    Correct command syntax is:
    vgremove VG|Tag|Select ...

    Volume group "sdb" not found
    Cannot process volume group sdb
    Device /dev/sdb not found.
  #+end_EXAMPLE

** kubeadm reset
  #+name: kubeadm reset
  #+begin_src shell :async t
    sudo kubeadm reset -f 
  #+end_src

  #+RESULTS: kubeadm reset
  #+begin_EXAMPLE
  [preflight] Running pre-flight checks
  W0101 07:54:26.429924    1862 removeetcdmember.go:79] [reset] No kubeadm config, using etcd pod spec to get data directory
  [reset] No etcd config found. Assuming external etcd
  [reset] Please, manually reset etcd to prevent further issues
  [reset] Stopping the kubelet service
  [reset] Unmounting mounted directories in "/var/lib/kubelet"
  [reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
  [reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]
  [reset] Deleting contents of stateful directories: [/var/lib/kubelet /var/lib/dockershim /var/run/kubernetes /var/lib/cni]

  The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

  The reset process does not reset or clean up iptables rules or IPVS tables.
  If you wish to reset iptables, you must do so manually by using the "iptables" command.

  If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
  to reset your system's IPVS tables.

  The reset process does not clean your kubeconfig files and you must remove them manually.
  Please, check the contents of the $HOME/.kube/config file.
  #+end_EXAMPLE

** lvm reset
  #+NAME: lvm reset
  #+begin_src shell
    VG=$(sudo lvs | tail -1 | awk '{print $2}')
    sudo lvm vgremove $VG --force --force
    sudo lvm lvremove /dev/sdb --force --force
    sudo pvremove /dev/sdb
  #+end_src

** rook reset
  #+NAME: rook reset
  #+begin_src shell
    sudo rm -rf /var/lib/rook/
  #+end_src
** uninstall rook
   #+begin_src shell
     helm uninstall rook-ceph --namespace rook-ceph
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   release "rook-ceph" uninstalled
   #+end_EXAMPLE

** uninstall traefik
   #+begin_src shell
     helm uninstall ii-traefik --namespace traefik
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   manifest-2

   release "ii-traefik" uninstalled
   #+end_EXAMPLE

* Configuration
** kubeadm-config.yaml
 #+NAME: kubeadm-config.yaml
 #+BEGIN_SRC yaml :tangle /ssh:ubuntu@192.168.1.101:kubeadm-config.yaml :noweb yes
   apiVersion: kubeadm.k8s.io/v1beta1
   kind: InitConfiguration
   localAPIEndpoint:
     advertiseAddress: "192.168.1.101"
   nodeRegistration:
     taints: [] # defaults to NoSchedule on role=master
   ---
   apiVersion: kubeadm.k8s.io/v1beta1
   kind: ClusterConfiguration
   kubernetesVersion: v1.17.0
   controlPlaneEndpoint: ""
   networking:
     podSubnet: "10.244.0.0/16"
     serviceSubnet: "10.96.0.0/12"
   apiServer:
     extraArgs:
       service-node-port-range: "1-60000" # allow more ports via API
   ---
   apiVersion: kubeproxy.config.k8s.io/v1alpha1
   kind: KubeProxyConfiguration
   nodePortAddresses:
     - "192.168.1.0/24" # default is null
   portRange: "1-60000" # Proxy also needs port range to ensure we can use 22,80,443,and friends
 #+END_SRC
** rook-config.yaml
   #+name: rook-config.yaml
   #+begin_src yaml :tangle /ssh:ubuntu@192.168.1.101:rook.yaml :noweb yes
     image:
       prefix: rook
       repository: rook/ceph
       tag: master
       pullPolicy: IfNotPresent

     resources:
       limits:
         cpu: 200m
         memory: 512Mi
       requests:
         cpu: 200m
         memory: 512Mi

     rbacEnable: true
     pspEnable: true
   #+end_src
** rook-cluster.yaml
   #+begin_src yaml :tangle /ssh:ubuntu@192.168.1.101:rook-cluster.yaml :noweb yes
     apiVersion: ceph.rook.io/v1
     kind: CephCluster
     metadata:
       name: rook-ceph
       namespace: rook-ceph
     spec:
       cephVersion:
         image: ceph/ceph:v14.2.5
         allowUnsupported: false
       dataDirHostPath: /var/lib/rook
       mon:
         count: 1
         allowMultiplePerNode: false
       dashboard:
         enabled: true
         ssl: false
       monitoring:
         enabled: false  # requires Prometheus to be pre-installed
         rulesNamespace: rook-ceph
       network:
         hostNetwork: false
       storage:
         useAllNodes: true
         useAllDevices: false
         deviceFilter: "^sd"
   #+end_src
** ceph-block-pool.yaml
   #+begin_src yaml :tangle /ssh:ubuntu@192.168.1.101:ceph-block-pool.yaml :noweb yes
     apiVersion: ceph.rook.io/v1
     kind: CephBlockPool
     metadata:
       name: ii-block-pool
       namespace: rook-ceph
     spec:
       replicated:
         size: 1
   #+end_src
** storage-class.yaml
   #+begin_src yaml :tangle /ssh:ubuntu@192.168.1.101:storage-class.yaml :noweb yes
     apiVersion: storage.k8s.io/v1
     kind: StorageClass
     metadata:
       name: standard
     #provisioner: rook-ceph.cephfs.csi.ceph.com
     provisioner: rook-ceph.rbd.csi.ceph.com
     parameters:
       # clusterID is the namespace where operator is deployed.
       clusterID: rook-ceph

       # CephFS filesystem name into which the volume shall be created
       # fsName: iifs

       # Ceph pool into which the volume shall be created
       # Required for provisionVolume: "true"
       pool: ii-block-pool
       # RBD image format. Defaults to "2".
       imageFormat: "2"
       # RBD image features. Available for imageFormat: "2". CSI RBD currently supports only `layering` feature.
       imageFeatures: layering

       # Root path of an existing CephFS volume
       # Required for provisionVolume: "false"
       # rootPath: /absolute/path

       # The secrets contain Ceph admin credentials. These are generated automatically by the operator
       # in the same namespace as the cluster.
       csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
       csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
       csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
       csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
       csi.storage.k8s.io/fstype: ext4
       # (optional) The driver can use either ceph-fuse (fuse) or ceph kernel client (kernel)
       # If omitted, default volume mounter will be used - this is determined by probing for ceph-fuse
       # or by setting the default mounter explicitly via --volumemounter command-line argument.
       # mounter: kernel
     reclaimPolicy: Retain
     # reclaimPolicy: Delete
     mountOptions:
       # uncomment the following line for debugging
       #- debug
     # uncomment the following to use rbd-nbd as mounter on supported nodes
     #mounter: rbd-nbd
   #+end_src
** rook-tools.yaml
   #+begin_src yaml :tangle /ssh:ubuntu@192.168.1.101:rook-tools.yaml :noweb yes
     apiVersion: apps/v1
     kind: Deployment
     metadata:
       name: rook-ceph-tools
       namespace: rook-ceph
       labels:
         app: rook-ceph-tools
     spec:
       replicas: 1
       selector:
         matchLabels:
           app: rook-ceph-tools
       template:
         metadata:
           labels:
             app: rook-ceph-tools
         spec:
           dnsPolicy: ClusterFirstWithHostNet
           containers:
           - name: rook-ceph-tools
             image: rook/ceph:v1.2.0
             command: ["/tini"]
             args: ["-g", "--", "/usr/local/bin/toolbox.sh"]
             imagePullPolicy: IfNotPresent
             env:
               - name: ROOK_ADMIN_SECRET
                 valueFrom:
                   secretKeyRef:
                     name: rook-ceph-mon
                     key: admin-secret
             securityContext:
               privileged: true
             volumeMounts:
               - mountPath: /dev
                 name: dev
               - mountPath: /sys/bus
                 name: sysbus
               - mountPath: /lib/modules
                 name: libmodules
               - name: mon-endpoint-volume
                 mountPath: /etc/rook
           # if hostNetwork: false, the "rbd map" command hangs, see https://github.com/rook/rook/issues/2021
           hostNetwork: true
           volumes:
             - name: dev
               hostPath:
                 path: /dev
             - name: sysbus
               hostPath:
                 path: /sys/bus
             - name: libmodules
               hostPath:
                 path: /lib/modules
             - name: mon-endpoint-volume
               configMap:
                 name: rook-ceph-mon-endpoints
                 items:
                 - key: data
                   path: mon-endpoints
   #+end_src
** traefik-1.7-config.yaml
If you use this annotation on the PVC, it will skip deleting the resource on uninstall.

#+name: keepers
#+begin_src yaml
helm.sh/resource-policy: "keep"
#+end_src

Password injection has been a bit of a pain, for some reason it shows up twice.
I'd like to figure out why it ejects a newline:

"$apr$PASSWORD
"
And why that newline results in repeating the yaml lines when used as a noweb executable argument.

#+NAME: traefik-admin-password
#+BEGIN_SRC shell :results silent :dir "." :results value :epilogue "" :prologue ""
# . .traefik.env
# echo -n $TRAEFIK_ADMIN_PASS | htpasswd -i -n '' | sed s/^:// | head -1
# htpasswd -n -b '' iiadmin | sed s/^:// | head -1
htpasswd -n -b root iiroot | sed s/^root:// | head -1
#+END_SRC
#+begin_src emacs-lisp :results value
#+end_src
#+NAME: traefik.yaml helm values
#+BEGIN_SRC yaml :tangle /ssh:ubuntu@192.168.1.101:traefik-1.7-config.yaml :noweb yes
  deployment:
    hostPort:
      httpEnabled: true
      httpsEnabled: true
      dashboardEnabled: true
      httpPort: 80
      httpsPort: 443
      dashboardPort: 8080
    # labels to add to the deployment
    labels:
      dep-label: ii
    annotations:
      dep-anno: ii
    # labels to add to the pod container metadata
    podLabels:
      pod-label: ii
    podAnnotations:
      pod-anno: ii
  service:
    ## Further config for service of type NodePort
    ## Default config with empty string "" will assign a dynamic
    ## nodePort to http and https ports
    #  nodePorts:
    #    http: "80"
    #    https: "443"
    # serviceType: NodePort
    annotations:
      service-anno: ii
    labels:
      service-label: ii
  #loadBalancerIP: 192.168.1.101
  # kubernetes.io/ingress.class=traefik
  # ingressClass = "traefik-internal"
  # https://docs.traefik.io/configuration/backends/kubernetes/#ingressendpoint
  # testuser: $apr1$JXRA7j2s$LpVns9vsme8FHN0r.aSt11
  dashboard:
    enabled: true
    domain: traefik.ii.nz
    auth:
      basic:
        admin: $apr1$We5npcg/$Z1rVvxv82ZFQ97aEwyj0k0
        testuser: $apr1$JXRA7j2s$LpVns9vsme8FHN0r.aSt11
        root: $apr1$ILfACRlz$L2X6Sfxnrkg90OIblA3t5.
  ssl:
    enabled: true
    enforced: true
    permanentRedirect: true
  # service:
  #   annotations:
  #   labels:
  rbac:
    enabled: true
  accessLogs:
    enabled: true
    format: json
    fields:
      defaultMode: keep
  # kubernetes:
  #   ingressEndpoint:
  #     ip: 192.168.1.101
  #   namespaces: [] # all namespaces with empty array
    # namespaces:
      # - apisnoop
      # - default
      # - kube-system
  acme:
    enabled: true
    email: hh@ii.coop
    staging: false
    # challengeType: tls-sni-01
    # challengeType: http-01
    # Unable to obtain ACME certificate for domains \"hh-hasura.apisnoop.io\"
    # detected thanks to rule \"Host:hh-hasura.apisnoop.io\" : 
    # unable to generate a certificate for the domains [hh-hasura.apisnoop.io]:
    #  acme: Error -> One or more domains had a problem:\n[hh-hasura.apisnoop.io]
    #  acme: error: 403 :: urn:ietf:params:acme:err or:unauthorized ::
    #  Invalid response from https://hh-hasura.apisnoop.io/.well-known/acme-challenge/2znqGrOWczcTMbLmN5NVm2OwcpQGT_ViPhEoJOpKQb8
    #  [35.189.56.228]: 404, ur l: \n
    challengeType: tls-alpn-01
    # challengeType: dns-01 # Needed for wildcards
    resolvers:
      - 1.1.1.1:53
      - 8.8.8.8:53
    persistence:
      # We don't want helm to delete our pvc
      # https://github.com/helm/helm/issues/6261#issuecomment-523472128
      annotations:
        helm.sh/resource-policy: "keep"
      enable: true
      storageClass: standard
      accessMode: ReadWriteOnce
      size: 1Gi
      # only use if claim already exists
      # existingClaim: ii-traefik-acme
    # domains:
    #   enabled: false
    #   domainsList:
    #     - main: "*.apisnoop.io"
    #     - sans:
    #       - "traefik.apisnoop.io"
    #       - "hh-apisnoop.apisnoop.io"
    #       - "zz-apisnoop.apisnoop.io"
    # dnsProvider:
    #   # name: dnsimple
    #   dnsimple:
    #     DNSIMPLE_OAUTH_TOKEN: "<dnsimple-auth-token()>"
    #     DNSIMPLE_BASE_URL: "https://api.dnsimple.com/v2/"
#+END_SRC

** rook-ingress.yaml
#+NAME: traefik.yaml helm values
#+BEGIN_SRC yaml :tangle /ssh:ubuntu@192.168.1.101:rook-ingress.yaml :noweb yes
  apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    name: rook-ceph-mrg-dashboard
  spec:
    rules:
    - host: rook.ii.nz
      http:
        paths:
        - backend:
            serviceName: rook-ceph-mgr-dashboard
            servicePort: dashboard
#+END_SRC

** nginx-cephfs-pvc.yaml
 #+NAME: nginx-cephfs-pc.yaml
 #+BEGIN_SRC yaml :tangle /ssh:ubuntu@192.168.1.101:nginx-cephfs-pvc.yaml :noweb yes
   apiVersion: v1
   kind: PersistentVolumeClaim
   metadata:
     name: html-content
   spec:
     accessModes:
     - ReadWriteMany
     resources:
       requests:
         storage: 1Gi
     storageClassName: cephfs
 #+END_SRC
** nginx-deployment.yaml
 #+NAME: nginx-deployment.yaml
 #+BEGIN_SRC yaml :tangle /ssh:ubuntu@192.168.1.101:nginx-deployment.yaml :noweb yes
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: nginx
     labels:
       app: nginx
   spec:
     replicas: 1
     selector:
       matchLabels:
         app: nginx
     template:
       metadata:
         labels:
           app: nginx
       spec:
         volumes:
         - name: html-content
           persistentVolumeClaim:
             claimName: html-content
             readOnly: false
         containers:
         - name: nginx
           image: nginx
           ports:
           - containerPort: 80
           volumeMounts:
           - name: html-content
             mountPath: /var/lib/registry
 #+END_SRC
** nginx-service.yaml
 #+NAME: kubeadm-config.yaml
 #+BEGIN_SRC yaml :tangle /ssh:ubuntu@192.168.1.101:nginx-service.yaml :noweb yes
   apiVersion: v1
   kind: Service
   metadata:
     name: nginx-service
   spec:
     selector:
       app: nginx
     type: NodePort
     ports:
     - protocol: TCP
       port: 80
       targetPort: 80
 #+END_SRC
** nginx-ingress.yaml
#+NAME: nginx-ingress.yaml helm values
#+BEGIN_SRC yaml :tangle /ssh:ubuntu@192.168.1.101:nginx-ingress.yaml :noweb yes
  apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    name: ii-web
  spec:
    rules:
    - host: web.pi.ii.nz
      http:
        paths:
        - backend:
            serviceName: nginx-service
            servicePort: 80
#+END_SRC

** cephfs.yaml
#+NAME: cephfs.yaml
#+BEGIN_SRC yaml :tangle /ssh:ubuntu@192.168.1.101:cephfs.yaml :noweb yes
apiVersion: ceph.rook.io/v1
kind: CephFilesystem
metadata:
  name: iifs
  namespace: rook-ceph
spec:
  metadataPool:
    replicated:
      size: 1
  dataPools:
    - failureDomain: osd
      replicated:
        size: 1
  preservePoolsOnDelete: true
  metadataServer:
    activeCount: 1
    activeStandby: true
#+END_SRC

** cephfs-storage-class.yaml
#+NAME: cephfs-storage-class.yaml
#+BEGIN_SRC yaml :tangle /ssh:ubuntu@192.168.1.101:cephfs-storage-class.yaml :noweb yes
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    name: cephfs
  provisioner: rook-ceph.cephfs.csi.ceph.com
  parameters:
    # clusterID is the namespace where operator is deployed.
    clusterID: rook-ceph

    # CephFS filesystem name into which the volume shall be created
    fsName: iifs

    # Ceph pool into which the volume shall be created
    # Required for provisionVolume: "true"
    pool: ii-block-pool

    # Root path of an existing CephFS volume
    # Required for provisionVolume: "false"
    # rootPath: /absolute/path

    # The secrets contain Ceph admin credentials. These are generated automatically by the operator
    # in the same namespace as the cluster.
    csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
    csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
    csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
    csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

    # (optional) The driver can use either ceph-fuse (fuse) or ceph kernel client (kernel)
    # If omitted, default volume mounter will be used - this is determined by probing for ceph-fuse
    # or by setting the default mounter explicitly via --volumemounter command-line argument.
    # mounter: kernel
  reclaimPolicy: Delete
  mountOptions:
    # uncomment the following line for debugging
    #- debug
#+END_SRC

* Steps
  If you run them all, at about ~10 minutes you should nsee a ceph-osd-prepare-ubuntu job/pod.
This creates the ceph lvm out of /dev/sda
Assuming it doesn't have any other partittions on it.
** cluster up
   #+begin_src shell
   <<kubeadm init>>
   <<cp kubeconfig>>
   <<scp kubeconfig()>>
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   #+end_EXAMPLE

** install rook + traefik
These need to be installed first, as they provide CRDs for the remaining rook/pvc objects.
   #+begin_src shell
   <<install rook operator>>
   <<install traefik>>
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   namespace/rook-ceph created
   NAME: rook-ceph
   LAST DEPLOYED: Thu Jan  2 08:15:13 2020
   NAMESPACE: rook-ceph
   STATUS: deployed
   REVISION: 1
   TEST SUITE: None
   NOTES:
   The Rook Operator has been installed. Check its status by running:
     kubectl --namespace rook-ceph get pods -l "app=rook-ceph-operator"

   Visit https://rook.io/docs/rook/master for instructions on how to create and configure Rook clusters

   Note: You cannot just create a CephCluster resource, you need to also create a namespace and
   install suitable RBAC roles and role bindings for the cluster. The Rook Operator will not do
   this for you. Sample CephCluster manifest templates that include RBAC resources are available:

   - https://rook.github.io/docs/rook/master/ceph-quickstart.html
   - https://github.com/rook/rook/blob/master/cluster/examples/kubernetes/ceph/cluster.yaml

   Important Notes:
   - The links above are for the unreleased master version, if you deploy a different release you must find matching manifests.
   - You must customise the 'CephCluster' resource at the bottom of the sample manifests to met your situation.
   - Each CephCluster must be deployed to its own namespace, the samples use `rook-ceph` for the cluster.
   - The sample manifests assume you also installed the rook-ceph operator in the `rook-ceph` namespace.
   - The helm chart includes all the RBAC required to create a CephCluster CRD in the same namespace.
   - Any disk devices you add to the cluster in the 'CephCluster' must be empty (no filesystem and no partitions).
   - In the 'CephCluster' you must refer to disk devices by their '/dev/something' name, e.g. 'sdb' or 'xvde'.
   namespace/traefik created
   NAME: ii-traefik
   LAST DEPLOYED: Thu Jan  2 08:15:22 2020
   NAMESPACE: traefik
   STATUS: deployed
   REVISION: 1
   TEST SUITE: None
   NOTES:
   1. Get Traefik's load balancer IP/hostname:

        NOTE: It may take a few minutes for this to become available.

        You can watch the status by running:

            $ kubectl get svc ii-traefik --namespace traefik -w

        Once 'EXTERNAL-IP' is no longer '<pending>':

            $ kubectl describe svc ii-traefik --namespace traefik | grep Ingress | awk '{print $3}'

   2. Configure DNS records corresponding to Kubernetes ingress resources to point to the load balancer IP/hostname found in step 1
   #+end_EXAMPLE

** deploy and install
   #+begin_src shell
   <<apply cni-weaveworks>>
   <<apply cephcluster CRD>>
   <<apply ceph-block-pool>>
   <<apply storage-class>>
   <<apply rook-tools>>
   <<apply rook-ingress>>
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   serviceaccount/weave-net created
   clusterrole.rbac.authorization.k8s.io/weave-net created
   clusterrolebinding.rbac.authorization.k8s.io/weave-net created
   role.rbac.authorization.k8s.io/weave-net created
   rolebinding.rbac.authorization.k8s.io/weave-net created
   daemonset.apps/weave-net created
   cephcluster.ceph.rook.io/rook-ceph created
   storageclass.storage.k8s.io/standard created
   deployment.apps/rook-ceph-tools created
   #+end_EXAMPLE

** kubeadm init
  #+name: kubeadm init
  #+begin_src shell :async t
    sudo kubeadm init --config kubeadm-config.yaml
  #+end_src

  #+RESULTS: kubeadm init
  #+begin_EXAMPLE
  W0101 08:01:49.858946    3638 common.go:77] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta1". Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
  W0101 08:01:49.860637    3638 common.go:77] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta1". Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
  W0101 08:01:49.864775    3638 validation.go:28] Cannot validate kube-proxy config - no validator is available
  W0101 08:01:49.864823    3638 validation.go:28] Cannot validate kubelet config - no validator is available
  [init] Using Kubernetes version: v1.17.0
  [preflight] Running pre-flight checks
    [WARNING Service-Docker]: docker service is not enabled, please run 'systemctl enable docker.service'
    [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
  [preflight] Pulling images required for setting up a Kubernetes cluster
  [preflight] This might take a minute or two, depending on the speed of your internet connection
  [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
  [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
  [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
  [kubelet-start] Starting the kubelet
  [certs] Using certificateDir folder "/etc/kubernetes/pki"
  [certs] Generating "ca" certificate and key
  [certs] Generating "apiserver" certificate and key
  [certs] apiserver serving cert is signed for DNS names [ubuntu kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.1.101]
  [certs] Generating "apiserver-kubelet-client" certificate and key
  [certs] Generating "front-proxy-ca" certificate and key
  [certs] Generating "front-proxy-client" certificate and key
  [certs] Generating "etcd/ca" certificate and key
  [certs] Generating "etcd/server" certificate and key
  [certs] etcd/server serving cert is signed for DNS names [ubuntu localhost] and IPs [192.168.1.101 127.0.0.1 ::1]
  [certs] Generating "etcd/peer" certificate and key
  [certs] etcd/peer serving cert is signed for DNS names [ubuntu localhost] and IPs [192.168.1.101 127.0.0.1 ::1]
  [certs] Generating "etcd/healthcheck-client" certificate and key
  [certs] Generating "apiserver-etcd-client" certificate and key
  [certs] Generating "sa" key and public key
  [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
  [kubeconfig] Writing "admin.conf" kubeconfig file
  [kubeconfig] Writing "kubelet.conf" kubeconfig file
  [kubeconfig] Writing "controller-manager.conf" kubeconfig file
  [kubeconfig] Writing "scheduler.conf" kubeconfig file
  [control-plane] Using manifest folder "/etc/kubernetes/manifests"
  [control-plane] Creating static Pod manifest for "kube-apiserver"
  [control-plane] Creating static Pod manifest for "kube-controller-manager"
  W0101 08:02:04.870516    3638 manifests.go:214] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
  [control-plane] Creating static Pod manifest for "kube-scheduler"
  W0101 08:02:04.879957    3638 manifests.go:214] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
  [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
  [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
  [kubelet-check] Initial timeout of 40s passed.
  [apiclient] All control plane components are healthy after 46.511568 seconds
  [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
  [kubelet] Creating a ConfigMap "kubelet-config-1.17" in namespace kube-system with the configuration for the kubelets in the cluster
  [upload-certs] Skipping phase. Please see --upload-certs
  [mark-control-plane] Marking the node ubuntu as control-plane by adding the label "node-role.kubernetes.io/master=''"
  [bootstrap-token] Using token: p7v81s.8coeumseuna5t7o9
  [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
  [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
  [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
  [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
  [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
  [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
  [addons] Applied essential addon: CoreDNS
  [addons] Applied essential addon: kube-proxy

  Your Kubernetes control-plane has initialized successfully!

  To start using your cluster, you need to run the following as a regular user:

    mkdir -p $HOME/.kube
    sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo chown $(id -u):$(id -g) $HOME/.kube/config

  You should now deploy a pod network to the cluster.
  Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
    https://kubernetes.io/docs/concepts/cluster-administration/addons/

  Then you can join any number of worker nodes by running the following on each as root:

  kubeadm join 192.168.1.101:6443 --token rfmdm3.7qgj0l72m3c7ol9d \
      --discovery-token-ca-cert-hash sha256:aa68bdc1de848cf6efed7b690052f621336bb2743f490abc93efa778c5a05440 
  #+end_EXAMPLE

** copy new kubeconfig into place
file:~/.kube/config
  #+NAME: cp kubeconfig
  #+begin_src shell :results silent
    mkdir -p $HOME/.kube
    sudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config
    sudo chown $(id -u):$(id -g) $HOME/.kube/config
  #+end_src
** scp kubeconfig
  #+NAME: scp kubeconfig
  #+begin_src shell :results silent :dir ~/
    scp ubuntu@192.168.1.101:.kube/config $HOME/.kube/config
  #+end_src
** apply cni-weaveworks
https://www.weave.works/docs/net/latest/kubernetes/kube-addon/
  #+name: apply cni-weaveworks
  #+begin_src shell
    kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
  #+end_src

  #+RESULTS: apply cni-weaveworks
  #+begin_EXAMPLE
  serviceaccount/weave-net created
  clusterrole.rbac.authorization.k8s.io/weave-net created
  clusterrolebinding.rbac.authorization.k8s.io/weave-net created
  role.rbac.authorization.k8s.io/weave-net created
  rolebinding.rbac.authorization.k8s.io/weave-net created
  daemonset.apps/weave-net created
  #+end_EXAMPLE

** install rook operator
   #+name: install rook operator
   #+begin_src shell
     kubectl create ns rook-ceph
     helm install rook-ceph --namespace rook-ceph rook-release/rook-ceph -f rook.yaml
   #+end_src

** apply cephcluster CRD
   This takes a while,  the crashcollector needs a secret that doesn't seem to be created until after 3/4 minutes.
   #+name: apply cephcluster CRD
   #+begin_src shell
     kubectl apply -f rook-cluster.yaml
     # kubectl delete -f rook-cluster.yaml
   #+end_src

** apply ceph-block-pool

   ceph-osd-prepare container starts about now
   #+name: apply ceph-block-pool
   #+begin_src shell
      kubectl apply -f ceph-block-pool.yaml
   #+end_src

   #+RESULTS: apply ceph-block-pool
   #+begin_EXAMPLE
   cephblockpool.ceph.rook.io/ii-block-pool created
   #+end_EXAMPLE

** apply storage-class
   #+name: apply storage-class
   #+begin_src shell
      # kubectl delete -f storage-class.yaml
      kubectl apply -f storage-class.yaml
   #+end_src

   #+RESULTS: apply storage-class
   #+begin_EXAMPLE
   storageclass.storage.k8s.io/standard configured
   #+end_EXAMPLE

   #+RESULTS:
   #+begin_EXAMPLE
   storageclass.storage.k8s.io/standard created
   #+end_EXAMPLE

** apply rook-tools
   #+name: apply rook-tools
   #+begin_src shell
     kubectl apply -f rook-tools.yaml
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   deployment.apps/rook-ceph-tools created
   #+end_EXAMPLE
** apply rook-ingress
   #+name: apply rook-ingress
   #+begin_src shell
      kubectl apply -f rook-ingress.yaml -n rook-ceph
   #+end_src

   #+RESULTS: apply rook-ingress
   #+begin_EXAMPLE
   ingress.extensions/rook-ceph-mrg-dashboard created
   #+end_EXAMPLE

** apply cephfs
   #+name: apply cephfs
   #+begin_src shell
      kubectl apply -f cephfs.yaml -n rook-ceph
   #+end_src

   #+RESULTS: apply cephfs
   #+begin_EXAMPLE
   cephfilesystem.ceph.rook.io/iifs created
   #+end_EXAMPLE

** apply cephfs-storage-class
   #+name: apply cephfs-storage-class
   #+begin_src shell
      kubectl apply -f cephfs-storage-class.yaml -n rook-ceph
   #+end_src

   #+RESULTS: apply cephfs-storage-class
   #+begin_EXAMPLE
   storageclass.storage.k8s.io/cephfs created
   #+end_EXAMPLE

** install traefik
   #+name: install traefik
   #+begin_src shell 
     kubectl create namespace traefik
     helm install \
          ii-traefik \
          --namespace traefik \
          --values $HOME/traefik-1.7-config.yaml \
          stable/traefik 
          # --values $HOME/traefik-config.yaml \
          # $HOME/traefik-helm-chart
   #+end_src

** deploy nginx
   This takes a while,  the crashcollector needs a secret that doesn't seem to be created until after 3/4 minutes.
   #+name: deploy nginx
   #+begin_src shell
     kubectl apply -f nginx-cephfs-pvc.yaml
     kubectl apply -f nginx-deployment.yaml
     kubectl apply -f nginx-service.yaml
     kubectl apply -f nginx-ingress.yaml
   #+end_src

   #+RESULTS: deploy nginx
   #+begin_EXAMPLE
   persistentvolumeclaim/html-content created
   deployment.apps/nginx configured
   service/nginx-service unchanged
   ingress.extensions/ii-web unchanged
   #+end_EXAMPLE
* Installing htpasswd
  #+begin_src shell
    sudo apt-get install -y apache2-utils
  #+end_src

* Explore
** get a list of crds created by rook-ceph
   #+begin_src shell
     kubectl get crd
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   NAME                                             CREATED AT
   cephblockpools.ceph.rook.io                      2020-01-01T08:03:33Z
   cephclients.ceph.rook.io                         2020-01-01T08:03:33Z
   cephclusters.ceph.rook.io                        2020-01-01T08:03:33Z
   cephfilesystems.ceph.rook.io                     2020-01-01T08:03:33Z
   cephnfses.ceph.rook.io                           2020-01-01T08:03:33Z
   cephobjectstores.ceph.rook.io                    2020-01-01T08:03:33Z
   cephobjectstoreusers.ceph.rook.io                2020-01-01T08:03:33Z
   objectbucketclaims.objectbucket.io               2020-01-01T08:03:33Z
   objectbuckets.objectbucket.io                    2020-01-01T08:03:33Z
   volumes.rook.io                                  2020-01-01T08:03:33Z
   volumesnapshotclasses.snapshot.storage.k8s.io    2020-01-02T08:00:04Z
   volumesnapshotcontents.snapshot.storage.k8s.io   2020-01-02T08:00:04Z
   volumesnapshots.snapshot.storage.k8s.io          2020-01-02T08:00:04Z
   #+end_EXAMPLE
** describe pod/rook-ceph-operator
   #+begin_src shell
     ROOT_OP_POD=$(kubectl --namespace rook-ceph get pods -l "app=rook-ceph-operator" -o name)
     kubectl describe --namespace rook-ceph $ROOT_OP_POD
   #+end_src
** get pod/rook-ceph-operator

   #+begin_src shell
     ROOT_OP_POD=$(kubectl --namespace rook-ceph get pods -l "app=rook-ceph-operator" -o name)
     kubectl get --namespace rook-ceph $ROOT_OP_POD
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   NAME                                  READY   STATUS    RESTARTS   AGE
   rook-ceph-operator-5cf57b4fd7-v44rf   1/1     Running   0          7m56s
   #+end_EXAMPLE
** get cephclusters

   #+begin_src shell
     kubectl get cephclusters.ceph.rook.io --namespace=rook-ceph
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   NAME        DATADIRHOSTPATH   MONCOUNT   AGE     STATE     HEALTH
   rook-ceph   /var/lib/rook     1          3d10h   Created   HEALTH_WARN
   #+end_EXAMPLE
** get rook-ceph services
   #+begin_src shell
     kubectl get service --namespace=rook-ceph
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   NAME                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
   csi-cephfsplugin-metrics   ClusterIP   10.96.128.28    <none>        8080/TCP,8081/TCP   97m
   csi-rbdplugin-metrics      ClusterIP   10.96.135.214   <none>        8080/TCP,8081/TCP   97m
   rook-ceph-mgr              ClusterIP   10.96.48.184    <none>        9283/TCP            87m
   rook-ceph-mgr-dashboard    ClusterIP   10.96.229.60    <none>        7000/TCP            93m
   rook-ceph-mon-a            ClusterIP   10.96.136.146   <none>        6789/TCP,3300/TCP   96m
   #+end_EXAMPLE
** get rook-ceph pods
   #+begin_src shell
      kubectl get pods --namespace=rook-ceph
   #+end_src
   
   #+RESULTS:
   #+begin_EXAMPLE
   NAME                                               READY   STATUS      RESTARTS   AGE
   csi-cephfsplugin-58xhs                             3/3     Running     0          98m
   csi-cephfsplugin-provisioner-56c8b7ddf4-4gvvt      4/4     Running     0          98m
   csi-cephfsplugin-provisioner-56c8b7ddf4-xrmc2      4/4     Running     0          98m
   csi-rbdplugin-kj2tq                                3/3     Running     0          98m
   csi-rbdplugin-provisioner-6ff4dd4b94-b56jd         5/5     Running     1          98m
   csi-rbdplugin-provisioner-6ff4dd4b94-h7vss         5/5     Running     1          98m
   rook-ceph-crashcollector-ubuntu-5df5c69d4b-r7b5m   1/1     Running     0          93m
   rook-ceph-mgr-a-9b8cc4c58-r5wvb                    1/1     Running     1          93m
   rook-ceph-mon-a-64c6dc5dc9-knc9l                   1/1     Running     0          97m
   rook-ceph-operator-5cf57b4fd7-dq586                1/1     Running     0          98m
   rook-ceph-osd-prepare-ubuntu-ljfvl                 0/1     Completed   0          87m
   rook-discover-n9cvl                                1/1     Running     0          98m
   #+end_EXAMPLE
** free memory
   #+begin_src shell
     free -m
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
                 total        used        free      shared  buff/cache   available
   Mem:           3791        1765         397           8        1628        2143
   Swap:             0           0           0
   #+end_EXAMPLE
** get ceph dashboard password
 #+name: dashboard password
 #+begin_src shell :results silent
   kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath="{['data']['password']}" | base64 --decode && echo
 #+end_src

 #+name: port forward to access dashboard
 #+begin_src shell
 kubectl port-forward -n rook-ceph service/rook-ceph-mgr-dashboard 7000
 #+end_src
** describe pod/traefik
   #+begin_src shell
     ROOT_OP_POD=$(kubectl --namespace rook-ceph get pods -l "app=rook-ceph-operator" -o name)
     kubectl describe --namespace rook-ceph $ROOT_OP_POD
   #+end_src
** copy html to web.pi.ii.nz
   #+begin_src shell :dir "."
     NGINX_POD=$(kubectl get pods -l "app=nginx" -o name | sed s:pod/::)
     kubectl cp pik8s.html $NGINX_POD:/usr/share/nginx/html/index.html
   #+end_src
   
* Understanding why the PVC isn't create
** get traefik pvc
   #+begin_src shell :wrap "src json"
     kubectl get pvc -n traefik ii-traefik-acme -o json
   #+end_src

   #+RESULTS:
   #+begin_src json
   {
       "apiVersion": "v1",
       "kind": "PersistentVolumeClaim",
       "metadata": {
           "annotations": {
               "helm.sh/resource-policy": "keep",
               "volume.beta.kubernetes.io/storage-provisioner": "rook-ceph.rbd.csi.ceph.com"
           },
           "creationTimestamp": "2019-12-31T20:51:29Z",
           "finalizers": [
               "kubernetes.io/pvc-protection"
           ],
           "labels": {
               "app": "traefik",
               "chart": "traefik-1.85.0",
               "heritage": "Helm",
               "release": "ii-traefik"
           },
           "name": "ii-traefik-acme",
           "namespace": "traefik",
           "resourceVersion": "3413",
           "selfLink": "/api/v1/namespaces/traefik/persistentvolumeclaims/ii-traefik-acme",
           "uid": "79b47f05-e5b6-4645-bd09-228cccb2f61e"
       },
       "spec": {
           "accessModes": [
               "ReadWriteOnce"
           ],
           "resources": {
               "requests": {
                   "storage": "1Gi"
               }
           },
           "storageClassName": "standard",
           "volumeMode": "Filesystem"
       },
       "status": {
           "phase": "Pending"
       }
   }
   #+end_src

** describe traefik pvc
   #+begin_src shell :wrap "src json"
     kubectl describe -n traefik pvc/ii-traefik-acme
   #+end_src

   #+RESULTS:
   #+begin_src json
   Name:          ii-traefik-acme
   Namespace:     traefik
   StorageClass:  standard
   Status:        Pending
   Volume:        
   Labels:        app=traefik
                  chart=traefik-1.85.0
                  heritage=Helm
                  release=ii-traefik
   Annotations:   helm.sh/resource-policy: keep
                  volume.beta.kubernetes.io/storage-provisioner: rook-ceph.rbd.csi.ceph.com
   Finalizers:    [kubernetes.io/pvc-protection]
   Capacity:      
   Access Modes:  
   VolumeMode:    Filesystem
   Mounted By:    ii-traefik-59db7c8bdc-48nz5
   Events:
     Type    Reason                Age                     From                                                                                                        Message
     ----    ------                ----                    ----                                                                                                        -------
     Normal  ExternalProvisioning  4m54s (x622 over 160m)  persistentvolume-controller                                                                                 waiting for a volume to be created, either by external provisioner "rook-ceph.rbd.csi.ceph.com" or manually created by system administrator
     Normal  Provisioning          70s (x41 over 160m)     rook-ceph.rbd.csi.ceph.com_csi-rbdplugin-provisioner-6ff4dd4b94-fhrnr_7626e9a6-e923-4659-9885-fa7eb7755b8c  External provisioner is provisioning volume for claim "traefik/ii-traefik-acme"
   #+end_src

** get pods
   #+begin_src shell
     kubectl get pods --namespace rook-ceph
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   NAME                                               READY   STATUS    RESTARTS   AGE
   csi-cephfsplugin-provisioner-56c8b7ddf4-dfh7k      4/4     Running   0          166m
   csi-cephfsplugin-provisioner-56c8b7ddf4-z9279      4/4     Running   0          166m
   csi-cephfsplugin-tgsk5                             3/3     Running   0          166m
   csi-rbdplugin-provisioner-6ff4dd4b94-4tw42         5/5     Running   1          166m
   csi-rbdplugin-provisioner-6ff4dd4b94-fhrnr         5/5     Running   0          166m
   csi-rbdplugin-vmn8h                                3/3     Running   0          166m
   rook-ceph-crashcollector-ubuntu-5df5c69d4b-czw28   1/1     Running   0          163m
   rook-ceph-mgr-a-648b49bb98-q4dq4                   1/1     Running   0          163m
   rook-ceph-mon-a-78866995b5-gjhb8                   1/1     Running   0          165m
   rook-ceph-operator-5cf57b4fd7-lknjh                1/1     Running   0          168m
   rook-ceph-osd-prepare-ubuntu-lqsxx                 1/1     Running   0          160m
   rook-ceph-tools-75498b5cfc-p2ppl                   1/1     Running   0          165m
   rook-discover-26g8l                                1/1     Running   0          168m
   #+end_EXAMPLE

* rook-tools
  :PROPERTIES:
  :header-args:shell+: :dir ~/
  :header-args:shell+: :prologue "kubectl -n rook-ceph exec -i `kubectl -n rook-ceph get pod -l app=rook-ceph-tools -o jsonpath='{.items[0].metadata.name}' ` bash\n("
  :header-args:shell+: :epilogue ") 2>&1\n:\n"
  :END:
   #+begin_src shell :prologue "kubectl -n rook-ceph exec -i `kubectl -n rook-ceph get pod -l app=rook-ceph-tools -o jsonpath='{.items[0].metadata.name}' ` bash" :epilogue ""
** ceph commands
   #+name: ceph commands
   #+begin_src shell :var COMMAND="ceph status"
     kubectl -n rook-ceph exec -it \
     $(kubectl -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') \
     -- ceph -h | grep -i list
   #+end_src

   #+RESULTS: ceph commands
   #+begin_EXAMPLE
   Unable to use a TTY - input is not a terminal or the right kind of file
   daemonperf {type.id | path} list|ls [stat-pats] [priority]
                           List shows a table of all available stats
   auth ls                                 list authentication state
   balancer ls                             List all plans
   balancer pool ls                        List automatic balancing pools. Note 
                                            that empty list means all existing 
   config ls                               List available configuration options
   config-key ls                           list keys
   dashboard iscsi-gateway-list            List iSCSI gateways
   fs ls                                   list filesystems
   fs subvolume ls <vol_name> {<group_     List subvolumes
   fs subvolume snapshot ls <vol_name>     List subvolume snapshots
   fs subvolumegroup ls <vol_name>         List subvolumegroups
   fs subvolumegroup snapshot ls <vol_     List subvolumegroup snapshots
   fs volume ls                            List volumes
   mgr module ls                           list active mgr modules
   mgr services                            list service endpoints provided by mgr 
   mon feature ls {--with-value}           list available mon map features to be 
   node ls {all|osd|mon|mds|mgr}           list all nodes in cluster [type]
   orchestrator device ls {<host> [<host>. List devices on a node
   orchestrator host ls                    List hosts
   orchestrator service ls {<host>} {mon|  List services known to orchestrator
   osd blacklist add|rm <EntityAddr>       add (optionally until <expire> seconds 
                                            blacklist
   osd blacklist clear                     clear all blacklisted clients
   osd blacklist ls                        show blacklisted clients
   osd crush class ls                      list all crush device classes
   osd crush class ls-osd <class>          list all osds belonging to the specific 
   osd crush ls <node>                     list items beneath a node in the CRUSH 
   osd crush rule ls                       list crush rules
   osd crush rule ls-by-class <class>      list all crush rules that reference the 
   osd crush weight-set ls                 list crush weight sets
   osd erasure-code-profile ls             list all erasure code profiles
   osd pool ls {detail}                    list pools
   pg ls {<int>} {<states> [<states>...]}  list pg with specific pool, osd, state
   pg ls-by-osd <osdname (id|osd.id)>      list pg on osd [osd]
   pg ls-by-pool <poolstr> {<states>       list pg with pool = [poolname]
   pg ls-by-primary <osdname (id|osd.id)>  list pg with primary = [osd]
   rbd task list {<task_id>}               List pending or running asynchronous 
   restful list-keys                       List all API keys
   #+end_EXAMPLE


** rook-tool code block
   #+name: rook-tool
   #+begin_src shell :var COMMAND="ceph status"
     kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') \
       $COMMAND 2>&1
     #ceph df 
     #  rados df
   #+end_src

** ceph config ls
   #+begin_src shell
     ceph config ls
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   host
   fsid
   public_addr
   public_bind_addr
   cluster_addr
   public_network
   public_network_interface
   cluster_network
   cluster_network_interface
   monmap
   mon_host
   mon_dns_srv_name
   lockdep
   lockdep_force_backtrace
   run_dir
   admin_socket
   admin_socket_mode
   daemonize
   setuser
   setgroup
   setuser_match_path
   pid_file
   chdir
   fatal_signal_handlers
   crash_dir
   restapi_log_level
   restapi_base_url
   erasure_code_dir
   log_file
   log_max_new
   log_max_recent
   log_to_file
   log_to_stderr
   err_to_stderr
   log_stderr_prefix
   log_to_syslog
   err_to_syslog
   log_flush_on_exit
   log_stop_at_utilization
   log_to_graylog
   err_to_graylog
   log_graylog_host
   log_graylog_port
   log_coarse_timestamps
   clog_to_monitors
   clog_to_syslog
   clog_to_syslog_level
   clog_to_syslog_facility
   clog_to_graylog
   clog_to_graylog_host
   clog_to_graylog_port
   mon_cluster_log_to_stderr
   mon_cluster_log_to_syslog
   mon_cluster_log_to_syslog_level
   mon_cluster_log_to_syslog_facility
   mon_cluster_log_to_file
   mon_cluster_log_file
   mon_cluster_log_file_level
   mon_cluster_log_to_graylog
   mon_cluster_log_to_graylog_host
   mon_cluster_log_to_graylog_port
   enable_experimental_unrecoverable_data_corrupting_features
   plugin_dir
   xio_trace_mempool
   xio_trace_msgcnt
   xio_trace_xcon
   xio_queue_depth
   xio_mp_min
   xio_mp_max_64
   xio_mp_max_256
   xio_mp_max_1k
   xio_mp_max_page
   xio_mp_max_hint
   xio_portal_threads
   xio_max_conns_per_portal
   xio_transport_type
   xio_max_send_inline
   compressor_zlib_isal
   compressor_zlib_level
   qat_compressor_enabled
   plugin_crypto_accelerator
   mempool_debug
   key
   keyfile
   keyring
   heartbeat_interval
   heartbeat_file
   heartbeat_inject_failure
   perf
   ms_type
   ms_public_type
   ms_cluster_type
   ms_mon_cluster_mode
   ms_mon_service_mode
   ms_mon_client_mode
   ms_cluster_mode
   ms_service_mode
   ms_client_mode
   ms_learn_addr_from_peer
   ms_tcp_nodelay
   ms_tcp_rcvbuf
   ms_tcp_prefetch_max_size
   ms_initial_backoff
   ms_max_backoff
   ms_crc_data
   ms_crc_header
   ms_die_on_bad_msg
   ms_die_on_unhandled_msg
   ms_die_on_old_message
   ms_die_on_skipped_message
   ms_die_on_bug
   ms_dispatch_throttle_bytes
   ms_msgr2_sign_messages
   ms_msgr2_encrypt_messages
   ms_bind_ipv4
   ms_bind_ipv6
   ms_bind_prefer_ipv4
   ms_bind_msgr1
   ms_bind_msgr2
   ms_bind_port_min
   ms_bind_port_max
   ms_bind_retry_count
   ms_bind_retry_delay
   ms_bind_before_connect
   ms_tcp_listen_backlog
   ms_rwthread_stack_bytes
   ms_connection_ready_timeout
   ms_connection_idle_timeout
   ms_pq_max_tokens_per_priority
   ms_pq_min_cost
   ms_inject_socket_failures
   ms_inject_delay_type
   ms_inject_delay_msg_type
   ms_inject_delay_max
   ms_inject_delay_probability
   ms_inject_internal_delays
   ms_dump_on_send
   ms_dump_corrupt_message_level
   ms_async_op_threads
   ms_async_max_op_threads
   ms_async_rdma_device_name
   ms_async_rdma_enable_hugepage
   ms_async_rdma_buffer_size
   ms_async_rdma_send_buffers
   ms_async_rdma_receive_buffers
   ms_async_rdma_receive_queue_len
   ms_async_rdma_support_srq
   ms_async_rdma_port_num
   ms_async_rdma_polling_us
   ms_async_rdma_local_gid
   ms_async_rdma_roce_ver
   ms_async_rdma_sl
   ms_async_rdma_dscp
   ms_max_accept_failures
   ms_async_rdma_cm
   ms_async_rdma_type
   ms_dpdk_port_id
   ms_dpdk_coremask
   ms_dpdk_memory_channel
   ms_dpdk_hugepages
   ms_dpdk_pmd
   ms_dpdk_host_ipv4_addr
   ms_dpdk_gateway_ipv4_addr
   ms_dpdk_netmask_ipv4_addr
   ms_dpdk_lro
   ms_dpdk_hw_flow_control
   ms_dpdk_hw_queue_weight
   ms_dpdk_debug_allow_loopback
   ms_dpdk_rx_buffer_count_per_core
   inject_early_sigterm
   mon_enable_op_tracker
   mon_op_complaint_time
   mon_op_log_threshold
   mon_op_history_size
   mon_op_history_duration
   mon_op_history_slow_op_size
   mon_op_history_slow_op_threshold
   mon_data
   mon_initial_members
   mon_compact_on_start
   mon_compact_on_bootstrap
   mon_compact_on_trim
   mon_osdmap_full_prune_enabled
   mon_osdmap_full_prune_min
   mon_osdmap_full_prune_interval
   mon_osdmap_full_prune_txsize
   mon_osd_cache_size
   mon_osd_cache_size_min
   mon_memory_target
   mon_memory_autotune
   mon_cpu_threads
   mon_osd_mapping_pgs_per_chunk
   mon_clean_pg_upmaps_per_chunk
   mon_osd_max_creating_pgs
   mon_osd_max_initial_pgs
   mon_tick_interval
   mon_session_timeout
   mon_subscribe_interval
   mon_delta_reset_interval
   mon_osd_laggy_halflife
   mon_osd_laggy_weight
   mon_osd_laggy_max_interval
   mon_osd_adjust_heartbeat_grace
   mon_osd_adjust_down_out_interval
   mon_osd_auto_mark_in
   mon_osd_auto_mark_auto_out_in
   mon_osd_auto_mark_new_in
   mon_osd_destroyed_out_interval
   mon_osd_down_out_interval
   mon_osd_down_out_subtree_limit
   mon_osd_min_up_ratio
   mon_osd_min_in_ratio
   mon_osd_warn_op_age
   mon_osd_err_op_age_ratio
   mon_osd_prime_pg_temp
   mon_osd_prime_pg_temp_max_time
   mon_osd_prime_pg_temp_max_estimate
   mon_stat_smooth_intervals
   mon_election_timeout
   mon_lease
   mon_lease_renew_interval_factor
   mon_lease_ack_timeout_factor
   mon_accept_timeout_factor
   mon_clock_drift_allowed
   mon_clock_drift_warn_backoff
   mon_timecheck_interval
   mon_timecheck_skew_interval
   mon_pg_stuck_threshold
   mon_pg_warn_min_per_osd
   mon_max_pg_per_osd
   mon_target_pg_per_osd
   mon_pg_warn_max_object_skew
   mon_pg_warn_min_objects
   mon_pg_warn_min_pool_objects
   mon_pg_check_down_all_threshold
   mon_cache_target_full_warn_ratio
   mon_osd_full_ratio
   mon_osd_backfillfull_ratio
   mon_osd_nearfull_ratio
   mon_osd_initial_require_min_compat_client
   mon_allow_pool_delete
   mon_fake_pool_delete
   mon_globalid_prealloc
   mon_osd_report_timeout
   mon_warn_on_msgr2_not_enabled
   mon_warn_on_legacy_crush_tunables
   mon_crush_min_required_version
   mon_warn_on_crush_straw_calc_version_zero
   mon_warn_on_osd_down_out_interval_zero
   mon_warn_on_cache_pools_without_hit_sets
   mon_warn_on_pool_no_app
   mon_warn_on_misplaced
   mon_warn_on_too_few_osds
   mon_warn_on_slow_ping_time
   mon_warn_on_slow_ping_ratio
   mon_max_snap_prune_per_epoch
   mon_min_osdmap_epochs
   mon_max_log_epochs
   mon_max_mdsmap_epochs
   mon_max_mgrmap_epochs
   mon_max_osd
   mon_probe_timeout
   mon_client_bytes
   mon_daemon_bytes
   mon_mgr_proxy_client_bytes_ratio
   mon_log_max_summary
   mon_max_log_entries_per_event
   mon_reweight_min_pgs_per_osd
   mon_reweight_min_bytes_per_osd
   mon_reweight_max_osds
   mon_reweight_max_change
   mon_health_to_clog
   mon_health_to_clog_interval
   mon_health_to_clog_tick_interval
   mon_health_max_detail
   mon_health_log_update_period
   mon_data_avail_crit
   mon_data_avail_warn
   mon_data_size_warn
   mon_warn_pg_not_scrubbed_ratio
   mon_warn_pg_not_deep_scrubbed_ratio
   mon_scrub_interval
   mon_scrub_timeout
   mon_scrub_max_keys
   mon_scrub_inject_crc_mismatch
   mon_scrub_inject_missing_keys
   mon_config_key_max_entry_size
   mon_sync_timeout
   mon_sync_max_payload_size
   mon_sync_debug
   mon_inject_sync_get_chunk_delay
   mon_osd_min_down_reporters
   mon_osd_reporter_subtree_level
   mon_osd_snap_trim_queue_warn_on
   mon_osd_force_trim_to
   mon_mds_force_trim_to
   mon_mds_skip_sanity
   mon_debug_extra_checks
   mon_debug_block_osdmap_trim
   mon_debug_deprecated_as_obsolete
   mon_debug_dump_transactions
   mon_debug_dump_json
   mon_debug_dump_location
   mon_debug_no_require_mimic
   mon_debug_no_require_nautilus
   mon_debug_no_require_bluestore_for_ec_overwrites
   mon_debug_no_initial_persistent_features
   mon_inject_transaction_delay_max
   mon_inject_transaction_delay_probability
   mon_inject_pg_merge_bounce_probability
   mon_sync_provider_kill_at
   mon_sync_requester_kill_at
   mon_force_quorum_join
   mon_keyvaluedb
   mon_debug_unsafe_allow_tier_with_nonempty_snaps
   mon_osd_blacklist_default_expire
   mon_mds_blacklist_interval
   mon_osd_crush_smoke_test
   mon_smart_report_timeout
   paxos_stash_full_interval
   paxos_max_join_drift
   paxos_propose_interval
   paxos_min_wait
   paxos_min
   paxos_trim_min
   paxos_trim_max
   paxos_service_trim_min
   paxos_service_trim_max
   paxos_kill_at
   auth_cluster_required
   auth_service_required
   auth_client_required
   auth_supported
   max_rotating_auth_attempts
   rotating_keys_bootstrap_timeout
   rotating_keys_renewal_timeout
   cephx_require_signatures
   cephx_require_version
   cephx_cluster_require_signatures
   cephx_cluster_require_version
   cephx_service_require_signatures
   cephx_service_require_version
   cephx_sign_messages
   auth_mon_ticket_ttl
   auth_service_ticket_ttl
   auth_debug
   mon_client_hunt_parallel
   mon_client_hunt_interval
   mon_client_ping_interval
   mon_client_ping_timeout
   mon_client_hunt_interval_backoff
   mon_client_hunt_interval_min_multiple
   mon_client_hunt_interval_max_multiple
   mon_client_max_log_entries_per_message
   mon_client_directed_command_retry
   mon_max_pool_pg_num
   mon_pool_quota_warn_threshold
   mon_pool_quota_crit_threshold
   crush_location
   crush_location_hook
   crush_location_hook_timeout
   objecter_tick_interval
   objecter_timeout
   objecter_inflight_op_bytes
   objecter_inflight_ops
   objecter_completion_locks_per_session
   objecter_inject_no_watch_ping
   objecter_retry_writes_after_first_reply
   objecter_debug_inject_relock_delay
   filer_max_purge_ops
   filer_max_truncate_ops
   journaler_write_head_interval
   journaler_prefetch_periods
   journaler_prezero_periods
   osd_calc_pg_upmaps_aggressively
   osd_calc_pg_upmaps_max_stddev
   osd_calc_pg_upmaps_local_fallback_retries
   osd_numa_prefer_iface
   osd_numa_auto_affinity
   osd_numa_node
   osd_smart_report_timeout
   osd_check_max_object_name_len_on_startup
   osd_max_backfills
   osd_min_recovery_priority
   osd_backfill_retry_interval
   osd_recovery_retry_interval
   osd_agent_max_ops
   osd_agent_max_low_ops
   osd_agent_min_evict_effort
   osd_agent_quantize_effort
   osd_agent_delay_time
   osd_find_best_info_ignore_history_les
   osd_agent_hist_halflife
   osd_agent_slop
   osd_uuid
   osd_data
   osd_journal
   osd_journal_size
   osd_journal_flush_on_shutdown
   osd_os_flags
   osd_max_write_size
   osd_max_pgls
   osd_client_message_size_cap
   osd_client_message_cap
   osd_crush_update_weight_set
   osd_crush_chooseleaf_type
   osd_pool_use_gmt_hitset
   osd_crush_update_on_start
   osd_class_update_on_start
   osd_crush_initial_weight
   osd_pool_default_ec_fast_read
   osd_pool_default_crush_rule
   osd_pool_erasure_code_stripe_unit
   osd_pool_default_size
   osd_pool_default_min_size
   osd_pool_default_pg_num
   osd_pool_default_pgp_num
   osd_pool_default_type
   osd_pool_default_erasure_code_profile
   osd_erasure_code_plugins
   osd_allow_recovery_below_min_size
   osd_pool_default_flags
   osd_pool_default_flag_hashpspool
   osd_pool_default_flag_nodelete
   osd_pool_default_flag_nopgchange
   osd_pool_default_flag_nosizechange
   osd_pool_default_hit_set_bloom_fpp
   osd_pool_default_cache_target_dirty_ratio
   osd_pool_default_cache_target_dirty_high_ratio
   osd_pool_default_cache_target_full_ratio
   osd_pool_default_cache_min_flush_age
   osd_pool_default_cache_min_evict_age
   osd_pool_default_cache_max_evict_check_size
   osd_pool_default_pg_autoscale_mode
   osd_hit_set_min_size
   osd_hit_set_max_size
   osd_hit_set_namespace
   osd_tier_promote_max_objects_sec
   osd_tier_promote_max_bytes_sec
   osd_tier_default_cache_mode
   osd_tier_default_cache_hit_set_count
   osd_tier_default_cache_hit_set_period
   osd_tier_default_cache_hit_set_type
   osd_tier_default_cache_min_read_recency_for_promote
   osd_tier_default_cache_min_write_recency_for_promote
   osd_tier_default_cache_hit_set_grade_decay_rate
   osd_tier_default_cache_hit_set_search_last_n
   osd_objecter_finishers
   osd_map_dedup
   osd_map_cache_size
   osd_map_message_max
   osd_map_message_max_bytes
   osd_map_share_max_epochs
   osd_pg_epoch_max_lag_factor
   osd_inject_bad_map_crc_probability
   osd_inject_failure_on_pg_removal
   osd_max_markdown_period
   osd_max_markdown_count
   osd_op_pq_max_tokens_per_priority
   osd_op_pq_min_cost
   osd_recover_clone_overlap
   osd_op_num_threads_per_shard
   osd_op_num_threads_per_shard_hdd
   osd_op_num_threads_per_shard_ssd
   osd_op_num_shards
   osd_op_num_shards_hdd
   osd_op_num_shards_ssd
   osd_skip_data_digest
   osd_op_queue
   osd_op_queue_cut_off
   osd_op_queue_mclock_client_op_res
   osd_op_queue_mclock_client_op_wgt
   osd_op_queue_mclock_client_op_lim
   osd_op_queue_mclock_osd_rep_op_res
   osd_op_queue_mclock_osd_rep_op_wgt
   osd_op_queue_mclock_osd_rep_op_lim
   osd_op_queue_mclock_snap_res
   osd_op_queue_mclock_snap_wgt
   osd_op_queue_mclock_snap_lim
   osd_op_queue_mclock_recov_res
   osd_op_queue_mclock_recov_wgt
   osd_op_queue_mclock_recov_lim
   osd_op_queue_mclock_scrub_res
   osd_op_queue_mclock_scrub_wgt
   osd_op_queue_mclock_scrub_lim
   osd_op_queue_mclock_anticipation_timeout
   osd_op_queue_mclock_pg_delete_res
   osd_op_queue_mclock_pg_delete_wgt
   osd_op_queue_mclock_pg_delete_lim
   osd_op_queue_mclock_peering_event_res
   osd_op_queue_mclock_peering_event_wgt
   osd_op_queue_mclock_peering_event_lim
   osd_ignore_stale_divergent_priors
   osd_read_ec_check_for_errors
   osd_recover_clone_overlap_limit
   osd_debug_feed_pullee
   osd_backfill_scan_min
   osd_backfill_scan_max
   osd_op_thread_timeout
   osd_op_thread_suicide_timeout
   osd_recovery_sleep
   osd_recovery_sleep_hdd
   osd_recovery_sleep_ssd
   osd_recovery_sleep_hybrid
   osd_snap_trim_sleep
   osd_snap_trim_sleep_hdd
   osd_snap_trim_sleep_ssd
   osd_snap_trim_sleep_hybrid
   osd_scrub_invalid_stats
   osd_command_thread_timeout
   osd_command_thread_suicide_timeout
   osd_heartbeat_interval
   osd_heartbeat_grace
   osd_heartbeat_stale
   osd_heartbeat_min_peers
   osd_heartbeat_use_min_delay_socket
   osd_heartbeat_min_size
   osd_pg_max_concurrent_snap_trims
   osd_max_trimming_pgs
   osd_heartbeat_min_healthy_ratio
   osd_mon_heartbeat_interval
   osd_mon_heartbeat_stat_stale
   osd_mon_report_interval
   osd_mon_report_max_in_flight
   osd_beacon_report_interval
   osd_pg_stat_report_interval_max
   osd_mon_ack_timeout
   osd_stats_ack_timeout_factor
   osd_stats_ack_timeout_decay
   osd_max_snap_prune_intervals_per_epoch
   osd_default_data_pool_replay_window
   osd_auto_mark_unfound_lost
   osd_recovery_delay_start
   osd_recovery_max_active
   osd_recovery_max_single_start
   osd_recovery_max_chunk
   osd_recovery_max_omap_entries_per_chunk
   osd_copyfrom_max_chunk
   osd_push_per_object_cost
   osd_max_push_cost
   osd_max_push_objects
   osd_max_scrubs
   osd_scrub_during_recovery
   osd_repair_during_recovery
   osd_scrub_begin_hour
   osd_scrub_end_hour
   osd_scrub_begin_week_day
   osd_scrub_end_week_day
   osd_scrub_load_threshold
   osd_scrub_min_interval
   osd_scrub_max_interval
   osd_scrub_interval_randomize_ratio
   osd_scrub_backoff_ratio
   osd_scrub_chunk_min
   osd_scrub_chunk_max
   osd_scrub_sleep
   osd_scrub_auto_repair
   osd_scrub_auto_repair_num_errors
   osd_scrub_max_preemptions
   osd_deep_scrub_interval
   osd_deep_scrub_randomize_ratio
   osd_deep_scrub_stride
   osd_deep_scrub_keys
   osd_deep_scrub_update_digest_min_age
   osd_deep_scrub_large_omap_object_key_threshold
   osd_deep_scrub_large_omap_object_value_sum_threshold
   osd_class_dir
   osd_open_classes_on_start
   osd_class_load_list
   osd_class_default_list
   osd_check_for_log_corruption
   osd_use_stale_snap
   osd_rollback_to_cluster_snap
   osd_default_notify_timeout
   osd_kill_backfill_at
   osd_pg_epoch_persisted_max_stale
   osd_min_pg_log_entries
   osd_max_pg_log_entries
   osd_pg_log_dups_tracked
   osd_force_recovery_pg_log_entries_factor
   osd_pg_log_trim_min
   osd_force_auth_primary_missing_objects
   osd_async_recovery_min_cost
   osd_max_pg_per_osd_hard_ratio
   osd_pg_log_trim_max
   osd_op_complaint_time
   osd_command_max_records
   osd_max_pg_blocked_by
   osd_op_log_threshold
   osd_verify_sparse_read_holes
   osd_backoff_on_unfound
   osd_backoff_on_degraded
   osd_backoff_on_peering
   osd_debug_shutdown
   osd_debug_crash_on_ignored_backoff
   osd_debug_inject_dispatch_delay_probability
   osd_debug_inject_dispatch_delay_duration
   osd_debug_drop_ping_probability
   osd_debug_drop_ping_duration
   osd_debug_op_order
   osd_debug_verify_missing_on_start
   osd_debug_verify_snaps
   osd_debug_verify_stray_on_activate
   osd_debug_skip_full_check_in_backfill_reservation
   osd_debug_reject_backfill_probability
   osd_debug_inject_copyfrom_error
   osd_debug_misdirected_ops
   osd_debug_skip_full_check_in_recovery
   osd_debug_random_push_read_error
   osd_debug_verify_cached_snaps
   osd_debug_deep_scrub_sleep
   osd_debug_no_acting_change
   osd_debug_no_purge_strays
   osd_debug_pretend_recovery_active
   osd_enable_op_tracker
   osd_num_op_tracker_shard
   osd_op_history_size
   osd_op_history_duration
   osd_op_history_slow_op_size
   osd_op_history_slow_op_threshold
   osd_target_transaction_size
   osd_delete_sleep
   osd_delete_sleep_hdd
   osd_delete_sleep_ssd
   osd_delete_sleep_hybrid
   osd_failsafe_full_ratio
   osd_fast_fail_on_connection_refused
   osd_pg_object_context_cache_count
   osd_tracing
   osd_function_tracing
   osd_fast_info
   osd_debug_pg_log_writeout
   osd_loop_before_reset_tphandle
   threadpool_default_timeout
   threadpool_empty_queue_max_wait
   leveldb_log_to_ceph_log
   leveldb_write_buffer_size
   leveldb_cache_size
   leveldb_block_size
   leveldb_bloom_size
   leveldb_max_open_files
   leveldb_compression
   leveldb_paranoid
   leveldb_log
   leveldb_compact_on_mount
   kinetic_host
   kinetic_port
   kinetic_user_id
   kinetic_hmac_key
   kinetic_use_ssl
   rocksdb_log_to_ceph_log
   rocksdb_cache_size
   rocksdb_cache_row_ratio
   rocksdb_cache_shard_bits
   rocksdb_cache_type
   rocksdb_block_size
   rocksdb_perf
   rocksdb_collect_compaction_stats
   rocksdb_collect_extended_stats
   rocksdb_collect_memory_stats
   rocksdb_enable_rmrange
   rocksdb_max_items_rmrange
   rocksdb_bloom_bits_per_key
   rocksdb_cache_index_and_filter_blocks
   rocksdb_cache_index_and_filter_blocks_with_high_priority
   rocksdb_pin_l0_filter_and_index_blocks_in_cache
   rocksdb_index_type
   rocksdb_partition_filters
   rocksdb_metadata_block_size
   mon_rocksdb_options
   osd_client_op_priority
   osd_recovery_op_priority
   osd_peering_op_priority
   osd_snap_trim_priority
   osd_snap_trim_cost
   osd_pg_delete_priority
   osd_pg_delete_cost
   osd_scrub_priority
   osd_scrub_cost
   osd_requested_scrub_priority
   osd_recovery_priority
   osd_recovery_cost
   osd_recovery_op_warn_multiple
   osd_mon_shutdown_timeout
   osd_shutdown_pgref_assert
   osd_max_object_size
   osd_max_object_name_len
   osd_max_object_namespace_len
   osd_max_attr_name_len
   osd_max_attr_size
   osd_max_omap_entries_per_request
   osd_max_omap_bytes_per_request
   osd_objectstore
   osd_objectstore_tracing
   osd_objectstore_fuse
   osd_bench_small_size_max_iops
   osd_bench_large_size_max_throughput
   osd_bench_max_block_size
   osd_bench_duration
   osd_blkin_trace_all
   osdc_blkin_trace_all
   osd_discard_disconnected_ops
   osd_memory_target
   osd_memory_target_cgroup_limit_ratio
   osd_memory_base
   osd_memory_expected_fragmentation
   osd_memory_cache_min
   osd_memory_cache_resize_interval
   memstore_device_bytes
   memstore_page_set
   memstore_page_size
   objectstore_blackhole
   bdev_debug_inflight_ios
   bdev_inject_crash
   bdev_inject_crash_flush_delay
   bdev_aio
   bdev_aio_poll_ms
   bdev_aio_max_queue_depth
   bdev_aio_reap_max
   bdev_block_size
   bdev_debug_aio
   bdev_debug_aio_suicide_timeout
   bdev_debug_aio_log_age
   bdev_nvme_unbind_from_kernel
   bdev_nvme_retry_count
   bdev_enable_discard
   bdev_async_discard
   bluefs_alloc_size
   bluefs_shared_alloc_size
   bluefs_max_prefetch
   bluefs_min_log_runway
   bluefs_max_log_runway
   bluefs_log_compact_min_ratio
   bluefs_log_compact_min_size
   bluefs_min_flush_size
   bluefs_compact_log_sync
   bluefs_buffered_io
   bluefs_sync_write
   bluefs_allocator
   bluefs_preextend_wal_files
   bluestore_bluefs
   bluestore_bluefs_env_mirror
   bluestore_bluefs_min
   bluestore_bluefs_min_free
   bluestore_bluefs_min_ratio
   bluestore_bluefs_max_ratio
   bluestore_bluefs_gift_ratio
   bluestore_bluefs_reclaim_ratio
   bluestore_bluefs_balance_interval
   bluestore_bluefs_alloc_failure_dump_interval
   bluestore_bluefs_db_compatibility
   bluestore_spdk_mem
   bluestore_spdk_coremask
   bluestore_spdk_max_io_completion
   bluestore_spdk_io_sleep
   bluestore_block_path
   bluestore_block_size
   bluestore_block_create
   bluestore_block_db_path
   bluestore_block_db_size
   bluestore_block_db_create
   bluestore_block_wal_path
   bluestore_block_wal_size
   bluestore_block_wal_create
   bluestore_block_preallocate_file
   bluestore_ignore_data_csum
   bluestore_csum_type
   bluestore_retry_disk_reads
   bluestore_min_alloc_size
   bluestore_min_alloc_size_hdd
   bluestore_min_alloc_size_ssd
   bluestore_max_alloc_size
   bluestore_prefer_deferred_size
   bluestore_prefer_deferred_size_hdd
   bluestore_prefer_deferred_size_ssd
   bluestore_compression_mode
   bluestore_compression_algorithm
   bluestore_compression_min_blob_size
   bluestore_compression_min_blob_size_hdd
   bluestore_compression_min_blob_size_ssd
   bluestore_compression_max_blob_size
   bluestore_compression_max_blob_size_hdd
   bluestore_compression_max_blob_size_ssd
   bluestore_gc_enable_blob_threshold
   bluestore_gc_enable_total_threshold
   bluestore_max_blob_size
   bluestore_max_blob_size_hdd
   bluestore_max_blob_size_ssd
   bluestore_compression_required_ratio
   bluestore_extent_map_shard_max_size
   bluestore_extent_map_shard_target_size
   bluestore_extent_map_shard_min_size
   bluestore_extent_map_shard_target_size_slop
   bluestore_extent_map_inline_shard_prealloc_size
   bluestore_cache_trim_interval
   bluestore_cache_trim_max_skip_pinned
   bluestore_cache_type
   bluestore_2q_cache_kin_ratio
   bluestore_2q_cache_kout_ratio
   bluestore_cache_size
   bluestore_cache_size_hdd
   bluestore_cache_size_ssd
   bluestore_cache_meta_ratio
   bluestore_cache_kv_ratio
   bluestore_cache_autotune
   bluestore_cache_autotune_interval
   bluestore_kvbackend
   bluestore_allocator
   bluestore_freelist_blocks_per_key
   bluestore_bitmapallocator_blocks_per_zone
   bluestore_bitmapallocator_span_size
   bluestore_max_deferred_txc
   bluestore_rocksdb_options
   bluestore_rocksdb_cf
   bluestore_rocksdb_cfs
   bluestore_fsck_on_mount
   bluestore_fsck_on_mount_deep
   bluestore_fsck_quick_fix_on_mount
   bluestore_fsck_on_umount
   bluestore_fsck_on_umount_deep
   bluestore_fsck_on_mkfs
   bluestore_fsck_on_mkfs_deep
   bluestore_sync_submit_transaction
   bluestore_fsck_read_bytes_cap
   bluestore_fsck_quick_fix_threads
   bluestore_throttle_bytes
   bluestore_throttle_deferred_bytes
   bluestore_throttle_cost_per_io
   bluestore_throttle_cost_per_io_hdd
   bluestore_throttle_cost_per_io_ssd
   bluestore_deferred_batch_ops
   bluestore_deferred_batch_ops_hdd
   bluestore_deferred_batch_ops_ssd
   bluestore_nid_prealloc
   bluestore_blobid_prealloc
   bluestore_clone_cow
   bluestore_default_buffered_read
   bluestore_default_buffered_write
   bluestore_debug_misc
   bluestore_debug_no_reuse_blocks
   bluestore_debug_small_allocations
   bluestore_debug_freelist
   bluestore_debug_prefill
   bluestore_debug_prefragment_max
   bluestore_debug_inject_read_err
   bluestore_debug_randomize_serial_transaction
   bluestore_debug_omit_block_device_write
   bluestore_debug_fsck_abort
   bluestore_debug_omit_kv_commit
   bluestore_debug_permit_any_bdev_label
   bluestore_debug_random_read_err
   bluestore_debug_inject_bug21040
   bluestore_debug_inject_csum_err_probability
   bluestore_fsck_error_on_no_per_pool_stats
   bluestore_warn_on_bluefs_spillover
   bluestore_warn_on_legacy_statfs
   bluestore_log_op_age
   bluestore_log_omap_iterator_age
   bluestore_log_collection_list_age
   kstore_max_ops
   kstore_max_bytes
   kstore_backend
   kstore_rocksdb_options
   kstore_fsck_on_mount
   kstore_fsck_on_mount_deep
   kstore_nid_prealloc
   kstore_sync_transaction
   kstore_sync_submit_transaction
   kstore_onode_map_size
   kstore_default_stripe_size
   filestore_rocksdb_options
   filestore_omap_backend
   filestore_omap_backend_path
   filestore_wbthrottle_enable
   filestore_wbthrottle_btrfs_bytes_start_flusher
   filestore_wbthrottle_btrfs_bytes_hard_limit
   filestore_wbthrottle_btrfs_ios_start_flusher
   filestore_wbthrottle_btrfs_ios_hard_limit
   filestore_wbthrottle_btrfs_inodes_start_flusher
   filestore_wbthrottle_xfs_bytes_start_flusher
   filestore_wbthrottle_xfs_bytes_hard_limit
   filestore_wbthrottle_xfs_ios_start_flusher
   filestore_wbthrottle_xfs_ios_hard_limit
   filestore_wbthrottle_xfs_inodes_start_flusher
   filestore_wbthrottle_btrfs_inodes_hard_limit
   filestore_wbthrottle_xfs_inodes_hard_limit
   filestore_odsync_write
   filestore_index_retry_probability
   filestore_debug_inject_read_err
   filestore_debug_random_read_err
   filestore_debug_omap_check
   filestore_omap_header_cache_size
   filestore_max_inline_xattr_size
   filestore_max_inline_xattr_size_xfs
   filestore_max_inline_xattr_size_btrfs
   filestore_max_inline_xattr_size_other
   filestore_max_inline_xattrs
   filestore_max_inline_xattrs_xfs
   filestore_max_inline_xattrs_btrfs
   filestore_max_inline_xattrs_other
   filestore_max_xattr_value_size
   filestore_max_xattr_value_size_xfs
   filestore_max_xattr_value_size_btrfs
   filestore_max_xattr_value_size_other
   filestore_sloppy_crc
   filestore_sloppy_crc_block_size
   filestore_max_alloc_hint_size
   filestore_max_sync_interval
   filestore_min_sync_interval
   filestore_btrfs_snap
   filestore_btrfs_clone_range
   filestore_zfs_snap
   filestore_fsync_flushes_journal_data
   filestore_fiemap
   filestore_punch_hole
   filestore_seek_data_hole
   filestore_splice
   filestore_fadvise
   filestore_collect_device_partition_information
   filestore_xfs_extsize
   filestore_journal_parallel
   filestore_journal_writeahead
   filestore_journal_trailing
   filestore_queue_max_ops
   filestore_queue_max_bytes
   filestore_caller_concurrency
   filestore_expected_throughput_bytes
   filestore_expected_throughput_ops
   filestore_queue_max_delay_multiple
   filestore_queue_high_delay_multiple
   filestore_queue_max_delay_multiple_bytes
   filestore_queue_high_delay_multiple_bytes
   filestore_queue_max_delay_multiple_ops
   filestore_queue_high_delay_multiple_ops
   filestore_queue_low_threshhold
   filestore_queue_high_threshhold
   filestore_op_threads
   filestore_op_thread_timeout
   filestore_op_thread_suicide_timeout
   filestore_commit_timeout
   filestore_fiemap_threshold
   filestore_merge_threshold
   filestore_split_multiple
   filestore_split_rand_factor
   filestore_update_to
   filestore_blackhole
   filestore_fd_cache_size
   filestore_fd_cache_shards
   filestore_ondisk_finisher_threads
   filestore_apply_finisher_threads
   filestore_dump_file
   filestore_kill_at
   filestore_inject_stall
   filestore_fail_eio
   filestore_debug_verify_split
   journal_dio
   journal_aio
   journal_force_aio
   journal_block_size
   journal_block_align
   journal_write_header_frequency
   journal_max_write_bytes
   journal_max_write_entries
   journal_throttle_low_threshhold
   journal_throttle_high_threshhold
   journal_throttle_high_multiple
   journal_throttle_max_multiple
   journal_align_min_size
   journal_replay_from
   mgr_stats_threshold
   journal_zero_on_create
   journal_ignore_corruption
   journal_discard
   fio_dir
   rados_mon_op_timeout
   rados_osd_op_timeout
   rados_tracing
   nss_db_path
   mgr_module_path
   mgr_initial_modules
   mgr_data
   mgr_tick_period
   mgr_stats_period
   mgr_client_bytes
   mgr_client_messages
   mgr_osd_bytes
   mgr_osd_messages
   mgr_mds_bytes
   mgr_mds_messages
   mgr_mon_bytes
   mgr_mon_messages
   mgr_connect_retry_interval
   mgr_service_beacon_grace
   mgr_client_service_daemon_unregister_timeout
   mgr_debug_aggressive_pg_num_changes
   mon_mgr_digest_period
   mon_mgr_beacon_grace
   mon_mgr_inactive_grace
   mon_mgr_mkfs_grace
   throttler_perf_counter
   event_tracing
   debug_deliberately_leak_memory
   debug_asserts_on_shutdown
   debug_asok_assert_abort
   target_max_misplaced_ratio
   device_failure_prediction_mode
   gss_ktab_client_file
   gss_target_name
   debug_disable_randomized_ping
   debug_heartbeat_testing_span
   rgw_acl_grants_max_num
   rgw_cors_rules_max_num
   rgw_delete_multi_obj_max_num
   rgw_website_routing_rules_max_num
   rgw_rados_tracing
   rgw_op_tracing
   rgw_max_chunk_size
   rgw_put_obj_min_window_size
   rgw_put_obj_max_window_size
   rgw_max_put_size
   rgw_max_put_param_size
   rgw_max_attr_size
   rgw_max_attr_name_len
   rgw_max_attrs_num_in_req
   rgw_override_bucket_index_max_shards
   rgw_bucket_index_max_aio
   rgw_enable_quota_threads
   rgw_enable_gc_threads
   rgw_enable_lc_threads
   rgw_data
   rgw_enable_apis
   rgw_cache_enabled
   rgw_cache_lru_size
   rgw_socket_path
   rgw_host
   rgw_port
   rgw_dns_name
   rgw_dns_s3website_name
   rgw_service_provider_name
   rgw_content_length_compat
   rgw_relaxed_region_enforcement
   rgw_lifecycle_work_time
   rgw_lc_lock_max_time
   rgw_lc_thread_delay
   rgw_lc_max_objs
   rgw_lc_max_rules
   rgw_lc_debug_interval
   rgw_mp_lock_max_time
   rgw_script_uri
   rgw_request_uri
   rgw_ignore_get_invalid_range
   rgw_swift_url
   rgw_swift_url_prefix
   rgw_swift_auth_url
   rgw_swift_auth_entry
   rgw_swift_tenant_name
   rgw_swift_account_in_url
   rgw_swift_enforce_content_length
   rgw_keystone_url
   rgw_keystone_admin_token
   rgw_keystone_admin_token_path
   rgw_keystone_admin_user
   rgw_keystone_admin_password
   rgw_keystone_admin_password_path
   rgw_keystone_admin_tenant
   rgw_keystone_admin_project
   rgw_keystone_admin_domain
   rgw_keystone_barbican_user
   rgw_keystone_barbican_password
   rgw_keystone_barbican_tenant
   rgw_keystone_barbican_project
   rgw_keystone_barbican_domain
   rgw_keystone_api_version
   rgw_keystone_accepted_roles
   rgw_keystone_accepted_admin_roles
   rgw_keystone_token_cache_size
   rgw_keystone_revocation_interval
   rgw_keystone_verify_ssl
   rgw_keystone_implicit_tenants
   rgw_cross_domain_policy
   rgw_healthcheck_disabling_path
   rgw_s3_auth_use_rados
   rgw_s3_auth_use_keystone
   rgw_s3_auth_order
   rgw_barbican_url
   rgw_ldap_uri
   rgw_ldap_binddn
   rgw_ldap_searchdn
   rgw_ldap_dnattr
   rgw_ldap_secret
   rgw_s3_auth_use_ldap
   rgw_ldap_searchfilter
   rgw_opa_url
   rgw_opa_token
   rgw_opa_verify_ssl
   rgw_use_opa_authz
   rgw_admin_entry
   rgw_enforce_swift_acls
   rgw_swift_token_expiration
   rgw_print_continue
   rgw_print_prohibited_content_length
   rgw_remote_addr_param
   rgw_op_thread_timeout
   rgw_op_thread_suicide_timeout
   rgw_thread_pool_size
   rgw_num_control_oids
   rgw_num_rados_handles
   rgw_verify_ssl
   rgw_nfs_lru_lanes
   rgw_nfs_lru_lane_hiwat
   rgw_nfs_fhcache_partitions
   rgw_nfs_fhcache_size
   rgw_nfs_namespace_expire_secs
   rgw_nfs_max_gc
   rgw_nfs_write_completion_interval_s
   rgw_nfs_s3_fast_attrs
   rgw_rados_pool_autoscale_bias
   rgw_rados_pool_pg_num_min
   rgw_zone
   rgw_zone_root_pool
   rgw_default_zone_info_oid
   rgw_region
   rgw_region_root_pool
   rgw_default_region_info_oid
   rgw_zonegroup
   rgw_zonegroup_root_pool
   rgw_default_zonegroup_info_oid
   rgw_realm
   rgw_realm_root_pool
   rgw_default_realm_info_oid
   rgw_period_root_pool
   rgw_period_latest_epoch_info_oid
   rgw_log_nonexistent_bucket
   rgw_log_object_name
   rgw_log_object_name_utc
   rgw_usage_max_shards
   rgw_usage_max_user_shards
   rgw_enable_ops_log
   rgw_enable_usage_log
   rgw_ops_log_rados
   rgw_ops_log_socket_path
   rgw_ops_log_data_backlog
   rgw_fcgi_socket_backlog
   rgw_usage_log_flush_threshold
   rgw_usage_log_tick_interval
   rgw_init_timeout
   rgw_mime_types_file
   rgw_gc_max_objs
   rgw_gc_obj_min_wait
   rgw_gc_processor_max_time
   rgw_gc_processor_period
   rgw_gc_max_concurrent_io
   rgw_gc_max_trim_chunk
   rgw_s3_success_create_obj_status
   rgw_resolve_cname
   rgw_obj_stripe_size
   rgw_extended_http_attrs
   rgw_exit_timeout_secs
   rgw_get_obj_window_size
   rgw_get_obj_max_req_size
   rgw_relaxed_s3_bucket_names
   rgw_defer_to_bucket_acls
   rgw_list_buckets_max_chunk
   rgw_md_log_max_shards
   rgw_curl_wait_timeout_ms
   rgw_curl_low_speed_limit
   rgw_curl_low_speed_time
   rgw_copy_obj_progress
   rgw_copy_obj_progress_every_bytes
   rgw_obj_tombstone_cache_size
   rgw_data_log_window
   rgw_data_log_changes_size
   rgw_data_log_num_shards
   rgw_data_log_obj_prefix
   rgw_bucket_quota_ttl
   rgw_bucket_quota_soft_threshold
   rgw_bucket_quota_cache_size
   rgw_bucket_default_quota_max_objects
   rgw_bucket_default_quota_max_size
   rgw_expose_bucket
   rgw_frontends
   rgw_user_quota_bucket_sync_interval
   rgw_user_quota_sync_interval
   rgw_user_quota_sync_idle_users
   rgw_user_quota_sync_wait_time
   rgw_user_default_quota_max_objects
   rgw_user_default_quota_max_size
   rgw_multipart_min_part_size
   rgw_multipart_part_upload_limit
   rgw_max_slo_entries
   rgw_olh_pending_timeout_sec
   rgw_user_max_buckets
   rgw_objexp_gc_interval
   rgw_objexp_hints_num_shards
   rgw_objexp_chunk_size
   rgw_enable_static_website
   rgw_user_unique_email
   rgw_log_http_headers
   rgw_num_async_rados_threads
   rgw_md_notify_interval_msec
   rgw_run_sync_thread
   rgw_sync_lease_period
   rgw_sync_log_trim_interval
   rgw_sync_log_trim_max_buckets
   rgw_sync_log_trim_min_cold_buckets
   rgw_sync_log_trim_concurrent_buckets
   rgw_sync_data_inject_err_probability
   rgw_sync_meta_inject_err_probability
   rgw_sync_trace_history_size
   rgw_sync_trace_per_node_log_size
   rgw_sync_trace_servicemap_update_interval
   rgw_period_push_interval
   rgw_period_push_interval_max
   rgw_safe_max_objects_per_shard
   rgw_shard_warning_threshold
   rgw_swift_versioning_enabled
   rgw_swift_custom_header
   rgw_swift_need_stats
   rgw_reshard_num_logs
   rgw_reshard_bucket_lock_duration
   rgw_reshard_batch_size
   rgw_reshard_max_aio
   rgw_trust_forwarded_https
   rgw_crypt_require_ssl
   rgw_crypt_default_encryption_key
   rgw_crypt_s3_kms_encryption_keys
   rgw_crypt_suppress_logs
   rgw_list_bucket_min_readahead
   rgw_rest_getusage_op_compat
   rgw_torrent_flag
   rgw_torrent_tracker
   rgw_torrent_createby
   rgw_torrent_comment
   rgw_torrent_encoding
   rgw_data_notify_interval_msec
   rgw_torrent_origin
   rgw_torrent_sha_unit
   rgw_dynamic_resharding
   rgw_max_objs_per_shard
   rgw_reshard_thread_interval
   rgw_cache_expiry_interval
   rgw_inject_notify_timeout_probability
   rgw_max_notify_retries
   rgw_sts_entry
   rgw_sts_key
   rgw_s3_auth_use_sts
   rgw_sts_max_session_duration
   rgw_max_listing_results
   rgw_sts_token_introspection_url
   rgw_sts_client_id
   rgw_sts_client_secret
   rgw_max_concurrent_requests
   rgw_scheduler_type
   rgw_dmclock_admin_res
   rgw_dmclock_admin_wgt
   rgw_dmclock_admin_lim
   rgw_dmclock_auth_res
   rgw_dmclock_auth_wgt
   rgw_dmclock_auth_lim
   rgw_dmclock_data_res
   rgw_dmclock_data_wgt
   rgw_dmclock_data_lim
   rgw_dmclock_metadata_res
   rgw_dmclock_metadata_wgt
   rgw_dmclock_metadata_lim
   rbd_default_pool
   rbd_default_data_pool
   rbd_default_features
   rbd_op_threads
   rbd_op_thread_timeout
   rbd_non_blocking_aio
   rbd_cache
   rbd_cache_writethrough_until_flush
   rbd_cache_size
   rbd_cache_max_dirty
   rbd_cache_target_dirty
   rbd_cache_max_dirty_age
   rbd_cache_max_dirty_object
   rbd_cache_block_writes_upfront
   rbd_concurrent_management_ops
   rbd_balance_snap_reads
   rbd_localize_snap_reads
   rbd_balance_parent_reads
   rbd_localize_parent_reads
   rbd_sparse_read_threshold_bytes
   rbd_readahead_trigger_requests
   rbd_readahead_max_bytes
   rbd_readahead_disable_after_bytes
   rbd_clone_copy_on_read
   rbd_blacklist_on_break_lock
   rbd_blacklist_expire_seconds
   rbd_request_timed_out_seconds
   rbd_skip_partial_discard
   rbd_discard_granularity_bytes
   rbd_enable_alloc_hint
   rbd_tracing
   rbd_blkin_trace_all
   rbd_validate_pool
   rbd_validate_names
   rbd_auto_exclusive_lock_until_manual_request
   rbd_move_to_trash_on_remove
   rbd_move_to_trash_on_remove_expire_seconds
   rbd_mirroring_resync_after_disconnect
   rbd_mirroring_delete_delay
   rbd_mirroring_replay_delay
   rbd_default_format
   rbd_default_order
   rbd_default_stripe_count
   rbd_default_stripe_unit
   rbd_default_map_options
   rbd_default_clone_format
   rbd_journal_order
   rbd_journal_splay_width
   rbd_journal_commit_age
   rbd_journal_object_writethrough_until_flush
   rbd_journal_object_flush_interval
   rbd_journal_object_flush_bytes
   rbd_journal_object_flush_age
   rbd_journal_object_max_in_flight_appends
   rbd_journal_pool
   rbd_journal_max_payload_bytes
   rbd_journal_max_concurrent_object_sets
   rbd_qos_iops_limit
   rbd_qos_bps_limit
   rbd_qos_read_iops_limit
   rbd_qos_write_iops_limit
   rbd_qos_read_bps_limit
   rbd_qos_write_bps_limit
   rbd_qos_iops_burst
   rbd_qos_bps_burst
   rbd_qos_read_iops_burst
   rbd_qos_write_iops_burst
   rbd_qos_read_bps_burst
   rbd_qos_write_bps_burst
   rbd_qos_schedule_tick_min
   rbd_discard_on_zeroed_write_same
   rbd_mtime_update_interval
   rbd_atime_update_interval
   rbd_mirror_journal_commit_age
   rbd_mirror_journal_poll_age
   rbd_mirror_journal_max_fetch_bytes
   rbd_mirror_sync_point_update_age
   rbd_mirror_concurrent_image_syncs
   rbd_mirror_pool_replayers_refresh_interval
   rbd_mirror_concurrent_image_deletions
   rbd_mirror_delete_retry_interval
   rbd_mirror_image_state_check_interval
   rbd_mirror_leader_heartbeat_interval
   rbd_mirror_leader_max_missed_heartbeats
   rbd_mirror_leader_max_acquire_attempts_before_break
   rbd_mirror_image_policy_type
   rbd_mirror_image_policy_migration_throttle
   rbd_mirror_image_policy_update_throttle_interval
   rbd_mirror_image_policy_rebalance_timeout
   rbd_mirror_perf_stats_prio
   mds_data
   mds_max_xattr_pairs_size
   mds_cache_trim_interval
   mds_cache_size
   mds_cache_memory_limit
   mds_cache_reservation
   mds_health_cache_threshold
   mds_cache_mid
   mds_cache_trim_decay_rate
   mds_cache_trim_threshold
   mds_max_file_recover
   mds_dir_max_commit_size
   mds_dir_keys_per_op
   mds_decay_halflife
   mds_beacon_interval
   mds_beacon_grace
   mds_heartbeat_grace
   mds_enforce_unique_name
   mds_session_blacklist_on_timeout
   mds_session_blacklist_on_evict
   mds_sessionmap_keys_per_op
   mds_recall_max_caps
   mds_recall_max_decay_rate
   mds_recall_max_decay_threshold
   mds_recall_global_max_decay_threshold
   mds_recall_warning_threshold
   mds_recall_warning_decay_rate
   mds_freeze_tree_timeout
   mds_health_summarize_threshold
   mds_reconnect_timeout
   mds_tick_interval
   mds_dirstat_min_interval
   mds_scatter_nudge_interval
   mds_client_prealloc_inos
   mds_early_reply
   mds_default_dir_hash
   mds_log_pause
   mds_log_skip_corrupt_events
   mds_log_max_events
   mds_log_events_per_segment
   mds_log_segment_size
   mds_log_max_segments
   mds_bal_export_pin
   mds_bal_sample_interval
   mds_bal_replicate_threshold
   mds_bal_unreplicate_threshold
   mds_bal_split_size
   mds_bal_split_rd
   mds_bal_split_wr
   mds_bal_split_bits
   mds_bal_merge_size
   mds_bal_interval
   mds_bal_fragment_interval
   mds_bal_fragment_size_max
   mds_bal_fragment_fast_factor
   mds_bal_fragment_dirs
   mds_bal_idle_threshold
   mds_bal_max
   mds_bal_max_until
   mds_bal_mode
   mds_bal_min_rebalance
   mds_bal_min_start
   mds_bal_need_min
   mds_bal_need_max
   mds_bal_midchunk
   mds_bal_minchunk
   mds_bal_target_decay
   mds_replay_interval
   mds_shutdown_check
   mds_thrash_exports
   mds_thrash_fragments
   mds_dump_cache_on_map
   mds_dump_cache_after_rejoin
   mds_verify_scatter
   mds_debug_scatterstat
   mds_debug_frag
   mds_debug_auth_pins
   mds_debug_subtrees
   mds_kill_mdstable_at
   mds_max_export_size
   mds_kill_export_at
   mds_kill_import_at
   mds_kill_link_at
   mds_kill_rename_at
   mds_kill_openc_at
   mds_kill_journal_at
   mds_kill_journal_expire_at
   mds_kill_journal_replay_at
   mds_journal_format
   mds_kill_create_at
   mds_inject_traceless_reply_probability
   mds_wipe_sessions
   mds_wipe_ino_prealloc
   mds_skip_ino
   mds_enable_op_tracker
   mds_op_history_size
   mds_op_history_duration
   mds_op_complaint_time
   mds_op_log_threshold
   mds_snap_min_uid
   mds_snap_max_uid
   mds_snap_rstat
   mds_verify_backtrace
   mds_max_completed_flushes
   mds_max_completed_requests
   mds_action_on_write_error
   mds_mon_shutdown_timeout
   mds_max_purge_files
   mds_max_purge_ops
   mds_max_purge_ops_per_pg
   mds_purge_queue_busy_flush_period
   mds_root_ino_uid
   mds_root_ino_gid
   mds_max_scrub_ops_in_progress
   mds_damage_table_max_entries
   mds_client_writeable_range_max_inc_objs
   mds_min_caps_per_client
   mds_max_caps_per_client
   mds_hack_allow_loading_invalid_metadata
   mds_defer_session_stale
   mds_inject_migrator_session_race
   mds_request_load_average_decay_rate
   mds_cap_revoke_eviction_timeout
   mds_max_retries_on_remount_failure
   mds_dump_cache_threshold_formatter
   mds_dump_cache_threshold_file
   client_cache_size
   client_cache_mid
   client_use_random_mds
   client_mount_timeout
   client_tick_interval
   client_trace
   client_readahead_min
   client_readahead_max_bytes
   client_readahead_max_periods
   client_reconnect_stale
   client_snapdir
   client_mountpoint
   client_mount_uid
   client_mount_gid
   client_notify_timeout
   osd_client_watch_timeout
   client_caps_release_delay
   client_quota_df
   client_oc
   client_oc_size
   client_oc_max_dirty
   client_oc_target_dirty
   client_oc_max_dirty_age
   client_oc_max_objects
   client_debug_getattr_caps
   client_debug_force_sync_read
   client_debug_inject_tick_delay
   client_max_inline_size
   client_inject_release_failure
   client_inject_fixed_oldest_tid
   client_metadata
   client_acl_type
   client_permissions
   client_dirsize_rbytes
   client_force_lazyio
   fuse_use_invalidate_cb
   fuse_disable_pagecache
   fuse_allow_other
   fuse_default_permissions
   fuse_big_writes
   fuse_max_write
   fuse_atomic_o_trunc
   fuse_debug
   fuse_multithreaded
   fuse_require_active_mds
   fuse_syncfs_on_mksnap
   fuse_set_user_groups
   client_try_dentry_invalidate
   client_die_on_failed_remount
   client_die_on_failed_dentry_invalidate
   client_check_pool_perm
   client_use_faked_inos
   client_mds_namespace
   fake_statfs_for_testing
   debug_allow_any_pool_priority
   mgr/ansible/password
   mgr/ansible/server_url
   mgr/ansible/username
   mgr/ansible/verify_server
   mgr/balancer/active
   mgr/balancer/begin_time
   mgr/balancer/begin_weekday
   mgr/balancer/crush_compat_max_iterations
   mgr/balancer/crush_compat_metrics
   mgr/balancer/crush_compat_step
   mgr/balancer/end_time
   mgr/balancer/end_weekday
   mgr/balancer/min_score
   mgr/balancer/mode
   mgr/balancer/pool_ids
   mgr/balancer/sleep_interval
   mgr/balancer/upmap_max_deviation
   mgr/balancer/upmap_max_iterations
   mgr/crash/retain_interval
   mgr/crash/warn_recent_interval
   mgr/dashboard/ALERTMANAGER_API_HOST
   mgr/dashboard/AUDIT_API_ENABLED
   mgr/dashboard/AUDIT_API_LOG_PAYLOAD
   mgr/dashboard/ENABLE_BROWSABLE_API
   mgr/dashboard/FEATURE_TOGGLE_cephfs
   mgr/dashboard/FEATURE_TOGGLE_iscsi
   mgr/dashboard/FEATURE_TOGGLE_mirroring
   mgr/dashboard/FEATURE_TOGGLE_rbd
   mgr/dashboard/FEATURE_TOGGLE_rgw
   mgr/dashboard/GANESHA_CLUSTERS_RADOS_POOL_NAMESPACE
   mgr/dashboard/GRAFANA_API_PASSWORD
   mgr/dashboard/GRAFANA_API_URL
   mgr/dashboard/GRAFANA_API_USERNAME
   mgr/dashboard/GRAFANA_UPDATE_DASHBOARDS
   mgr/dashboard/ISCSI_API_SSL_VERIFICATION
   mgr/dashboard/PROMETHEUS_API_HOST
   mgr/dashboard/REST_REQUESTS_TIMEOUT
   mgr/dashboard/RGW_API_ACCESS_KEY
   mgr/dashboard/RGW_API_ADMIN_RESOURCE
   mgr/dashboard/RGW_API_HOST
   mgr/dashboard/RGW_API_PORT
   mgr/dashboard/RGW_API_SCHEME
   mgr/dashboard/RGW_API_SECRET_KEY
   mgr/dashboard/RGW_API_SSL_VERIFY
   mgr/dashboard/RGW_API_USER_ID
   mgr/dashboard/crt_file
   mgr/dashboard/jwt_token_ttl
   mgr/dashboard/key_file
   mgr/dashboard/password
   mgr/dashboard/server_addr
   mgr/dashboard/server_port
   mgr/dashboard/ssl
   mgr/dashboard/ssl_server_port
   mgr/dashboard/standby_behaviour
   mgr/dashboard/standby_error_status_code
   mgr/dashboard/url_prefix
   mgr/dashboard/username
   mgr/deepsea/salt_api_eauth
   mgr/deepsea/salt_api_password
   mgr/deepsea/salt_api_url
   mgr/deepsea/salt_api_username
   mgr/devicehealth/enable_monitoring
   mgr/devicehealth/mark_out_threshold
   mgr/devicehealth/pool_name
   mgr/devicehealth/retention_period
   mgr/devicehealth/scrape_frequency
   mgr/devicehealth/self_heal
   mgr/devicehealth/sleep_interval
   mgr/devicehealth/warn_threshold
   mgr/diskprediction_local/predict_interval
   mgr/diskprediction_local/sleep_interval
   mgr/influx/batch_size
   mgr/influx/database
   mgr/influx/hostname
   mgr/influx/interval
   mgr/influx/password
   mgr/influx/port
   mgr/influx/ssl
   mgr/influx/threads
   mgr/influx/username
   mgr/influx/verify_ssl
   mgr/localpool/failure_domain
   mgr/localpool/min_size
   mgr/localpool/num_rep
   mgr/localpool/pg_num
   mgr/localpool/prefix
   mgr/localpool/subtree
   mgr/orchestrator_cli/orchestrator
   mgr/pg_autoscaler/sleep_interval
   mgr/progress/max_completed_events
   mgr/progress/persist_interval
   mgr/prometheus/rbd_stats_pools
   mgr/prometheus/rbd_stats_pools_refresh_interval
   mgr/prometheus/scrape_interval
   mgr/prometheus/server_addr
   mgr/prometheus/server_port
   mgr/restful/key_file
   mgr/restful/server_addr
   mgr/restful/server_port
   mgr/selftest/roption1
   mgr/selftest/roption2
   mgr/selftest/rwoption1
   mgr/selftest/rwoption2
   mgr/selftest/rwoption3
   mgr/selftest/rwoption4
   mgr/selftest/rwoption5
   mgr/selftest/rwoption6
   mgr/selftest/testkey
   mgr/selftest/testlkey
   mgr/selftest/testnewline
   mgr/ssh/inventory_cache_timeout_min
   mgr/ssh/ssh_config_file
   mgr/telegraf/address
   mgr/telegraf/interval
   mgr/telemetry/channel_basic
   mgr/telemetry/channel_crash
   mgr/telemetry/channel_device
   mgr/telemetry/channel_ident
   mgr/telemetry/contact
   mgr/telemetry/description
   mgr/telemetry/device_url
   mgr/telemetry/enabled
   mgr/telemetry/interval
   mgr/telemetry/last_opt_revision
   mgr/telemetry/leaderboard
   mgr/telemetry/organization
   mgr/telemetry/proxy
   mgr/telemetry/url
   mgr/zabbix/identifier
   mgr/zabbix/interval
   mgr/zabbix/zabbix_host
   mgr/zabbix/zabbix_port
   mgr/zabbix/zabbix_sender
   #+end_EXAMPLE

** ceph config-key ls
   #+name: ceph config-key ls
   #+call: rook-tool("ceph config-key ls")

   #+RESULTS: ceph config-key ls
   #+begin_EXAMPLE
   Unable to use a TTY - input is not a terminal or the right kind of file
   [
       "config-history/1/",
       "config-history/2/",
       "config-history/2/+mon_allow_pool_delete",
       "config-history/3/",
       "config-history/3/+rbd_default_features",
       "config-history/4/",
       "config-history/4/+mgr/mgr/orchestrator_cli/orchestrator",
       "config-history/5/",
       "config-history/5/+mgr.a/mgr/dashboard/ssl",
       "config-history/6/",
       "config-history/6/+mgr.a/mgr/dashboard/server_port",
       "config/mgr.a/mgr/dashboard/server_port",
       "config/mgr.a/mgr/dashboard/ssl",
       "config/mgr/mgr/orchestrator_cli/orchestrator",
       "config/mon_allow_pool_delete",
       "config/rbd_default_features",
       "mgr/dashboard/accessdb_v1",
       "mgr/dashboard/jwt_secret"
   ]
   #+end_EXAMPLE

** ceph fs ls
   #+name: ceph fs ls
   #+call: rook-tool("ceph fs ls")

   #+RESULTS: ceph fs ls
   #+begin_EXAMPLE
   Unable to use a TTY - input is not a terminal or the right kind of file
   name: iifs, metadata pool: iifs-metadata, data pools: [iifs-data0 ]
   #+end_EXAMPLE

** ceph fs volume ls
   Trying to understand the cephfs status.
It appears to be hung, because ceph fs ls doesn't return.

https://docs.ceph.com/docs/master/cephfs/


   #+name: ceph fs volume ls
   #+call: rook-tool("ceph fs volume ls")

** rados commands
   #+name: rados commands
   #+begin_src shell
     kubectl -n rook-ceph exec -it \
     $(kubectl -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') \
     -- rados -h | grep -i list
   #+end_src

   #+RESULTS: rados commands
   #+begin_EXAMPLE
   Unable to use a TTY - input is not a terminal or the right kind of file
      lspools                          list pools
      ls                               list objects in pool
      lssnap                           list snaps
      listxattr <obj-name>
      listsnaps <obj-name>             list the snapshots of this object
      listomapkeys <obj-name>          list the keys in the object map
      listomapvals <obj-name>          list the keys and vals in the object map 
      listwatchers <obj-name>          list the watchers of this object
      lock list <obj-name>
          List all advisory locks on an object
      list-inconsistent-pg <pool>      list inconsistent PGs in given pool
      list-inconsistent-obj <pgid>     list inconsistent objects in given PG
      list-inconsistent-snapset <pgid> list inconsistent snapsets in the given PG
           Use with ls to list objects in all namespaces
           Use with ls to list objects in default namespace
   #+end_EXAMPLE

** rados lspools
   Trying to understand the cephfs status.
It appears to be hung, because ceph fs ls doesn't return.

https://docs.ceph.com/docs/master/cephfs/

   #+name: rados lspools
   #+call: rook-tool("rados lspools")

   #+RESULTS: rados lspools
   #+begin_EXAMPLE
   Unable to use a TTY - input is not a terminal or the right kind of file
   ii-block-pool
   iifs-metadata
   iifs-data0
   #+end_EXAMPLE

** rados ls
   Trying to understand the cephfs status.
It appears to be hung, because ceph fs ls doesn't return.

https://docs.ceph.com/docs/master/cephfs/

   #+name: rados ls
   #+begin_src shell :prologue "kubectl -n rook-ceph exec -i `kubectl -n rook-ceph get pod -l app=rook-ceph-tools -o jsonpath='{.items[0].metadata.name}' ` bash" :epilogue ""
      rados ls -p iifs-metadata
   #+end_src

   #+RESULTS: rados ls
   #+begin_EXAMPLE
   601.00000000
   602.00000000
   600.00000000
   603.00000000
   1.00000000.inode
   200.00000000
   200.00000001
   606.00000000
   607.00000000
   mds0_openfiles.0
   608.00000000
   604.00000000
   500.00000000
   mds_snaptable
   605.00000000
   mds0_inotable
   100.00000000
   mds0_sessionmap
   609.00000000
   400.00000000
   100.00000000.inode
   1.00000000
   #+end_EXAMPLE

   #+call: rook-tool("-- rados ls -p iifs-data0")



** ceph status
   #+name: ceph status
   #+call: rook-tool("ceph status")
   #+RESULTS: ceph status
   #+begin_EXAMPLE
   Unable to use a TTY - input is not a terminal or the right kind of file
     cluster:
       id:     000bd0c3-d911-4dc7-8d72-f3024e714115
       health: HEALTH_WARN
               OSD count 1 < osd_pool_default_size 3
               too few PGs per OSD (24 < min 30)

     services:
       mon: 1 daemons, quorum a (age 3d)
       mgr: a(active, since 3d)
       mds: iifs:1 {0=iifs-a=up:active} 1 up:standby-replay
       osd: 1 osds: 1 up (since 3d), 1 in (since 3d)

     data:
       pools:   3 pools, 24 pgs
       objects: 40 objects, 11 MiB
       usage:   1.0 GiB used, 29 GiB / 30 GiB avail
       pgs:     24 active+clean

   #+end_EXAMPLE
** ceph df
   #+call: rook-tool[:wrap "src TEXT" :dir "/tmp"]("ceph df")

   #+RESULTS:
   #+begin_src TEXT
   Unable to use a TTY - input is not a terminal or the right kind of file
   RAW STORAGE:
       CLASS     SIZE       AVAIL      USED        RAW USED     %RAW USED 
       hdd       30 GiB     29 GiB     7.1 MiB      1.0 GiB          3.36 
       TOTAL     30 GiB     29 GiB     7.1 MiB      1.0 GiB          3.36 

   POOLS:
       POOL              ID     STORED      OBJECTS     USED        %USED     MAX AVAIL 
       ii-block-pool      1     872 KiB          15     1.5 MiB         0        27 GiB 
       iifs-metadata      2     2.2 KiB          25     512 KiB         0        27 GiB 
       iifs-data0         3         0 B           0         0 B         0        27 GiB 
   #+end_src

** ceph pg dump
   #+call: rook-tool("ceph pg dump")
   #+RESULTS:
   #+begin_EXAMPLE
   Unable to use a TTY - input is not a terminal or the right kind of file
   dumped all
   version 35893
   stamp 2019-12-30 03:52:01.102189
   last_osdmap_epoch 0
   last_pg_scan 0
   PG_STAT OBJECTS MISSING_ON_PRIMARY DEGRADED MISPLACED UNFOUND BYTES OMAP_BYTES* OMAP_KEYS* LOG DISK_LOG STATE        STATE_STAMP                VERSION REPORTED UP  UP_PRIMARY ACTING ACTING_PRIMARY LAST_SCRUB SCRUB_STAMP                LAST_DEEP_SCRUB DEEP_SCRUB_STAMP           SNAPTRIMQ_LEN 
   1.7           0                  0        0         0       0     0           0          0   0        0 active+clean 2019-12-29 08:01:34.039628     0'0    11:17 [0]          0    [0]              0        0'0 2019-12-29 08:01:21.070991             0'0 2019-12-29 08:01:21.070991             0 
   1.6           0                  0        0         0       0     0           0          0   0        0 active+clean 2019-12-29 08:01:34.035377     0'0    11:17 [0]          0    [0]              0        0'0 2019-12-29 08:01:21.070991             0'0 2019-12-29 08:01:21.070991             0 
   1.5           0                  0        0         0       0     0           0          0   0        0 active+clean 2019-12-29 08:01:34.040298     0'0    11:17 [0]          0    [0]              0        0'0 2019-12-29 08:01:21.070991             0'0 2019-12-29 08:01:21.070991             0 
   1.4           0                  0        0         0       0     0           0          0   0        0 active+clean 2019-12-29 08:01:34.035829     0'0    11:17 [0]          0    [0]              0        0'0 2019-12-29 08:01:21.070991             0'0 2019-12-29 08:01:21.070991             0 
   1.0           0                  0        0         0       0     0           0          0   0        0 active+clean 2019-12-29 08:01:34.040512     0'0    11:17 [0]          0    [0]              0        0'0 2019-12-29 08:01:21.070991             0'0 2019-12-29 08:01:21.070991             0 
   1.1           0                  0        0         0       0     0           0          0   0        0 active+clean 2019-12-29 08:01:34.035688     0'0    11:17 [0]          0    [0]              0        0'0 2019-12-29 08:01:21.070991             0'0 2019-12-29 08:01:21.070991             0 
   1.2           0                  0        0         0       0     0           0          0   0        0 active+clean 2019-12-29 08:01:34.039864     0'0    11:17 [0]          0    [0]              0        0'0 2019-12-29 08:01:21.070991             0'0 2019-12-29 08:01:21.070991             0 
   1.3           0                  0        0         0       0     0           0          0   0        0 active+clean 2019-12-29 08:01:34.039064     0'0    11:17 [0]          0    [0]              0        0'0 2019-12-29 08:01:21.070991             0'0 2019-12-29 08:01:21.070991             0 

   1 0 0 0 0 0 0 0 0 0 0 

   sum 0 0 0 0 0 0 0 0 0 0 
   OSD_STAT USED    AVAIL   USED_RAW TOTAL   HB_PEERS PG_SUM PRIMARY_PG_SUM 
   0        1.7 MiB 893 GiB  1.0 GiB 894 GiB       []      8              8 
   sum      1.7 MiB 893 GiB  1.0 GiB 894 GiB                                

   ,* NOTE: Omap statistics are gathered during deep scrub and may be inaccurate soon afterwards depending on utilisation. See http://docs.ceph.com/docs/master/dev/placement-group/#omap-statistics for further details.
   #+end_EXAMPLE
** ceph osd tree
   #+call: rook-tool("ceph osd tree")

   #+RESULTS:
   #+begin_EXAMPLE
   Unable to use a TTY - input is not a terminal or the right kind of file
   ID CLASS WEIGHT  TYPE NAME       STATUS REWEIGHT PRI-AFF 
   -1       0.02930 root default                            
   -3       0.02930     host ubuntu                         
    0   hdd 0.02930         osd.0       up  1.00000 1.00000 
   #+end_EXAMPLE
** ceph osd stat
  #+call: rook-tool("ceph osd stat")
** ceph mds stat
  #+call: rook-tool("ceph mds stat")
** ceph mon stat
  #+call: rook-tool("ceph mon stat")
** ceph osd lspools
  #+call: rook-tool("ceph osd lspools")

  #+RESULTS:
  #+begin_EXAMPLE
  Unable to use a TTY - input is not a terminal or the right kind of file
  1 ii-block-pool
  2 iifs-metadata
  3 iifs-data0
  #+end_EXAMPLE

** ceph rados df
  #+call: rook-tool("rados df")

  #+RESULTS:
  #+begin_EXAMPLE
  Unable to use a TTY - input is not a terminal or the right kind of file
  POOL_NAME        USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED RD_OPS     RD WR_OPS      WR USED COMPR UNDER COMPR 
  ii-block-pool 1.5 MiB      15      0     15                  0       0        0  67033 64 MiB    100 1.2 MiB        0 B         0 B 
  iifs-data0        0 B       0      0      0                  0       0        0      0    0 B      0     0 B        0 B         0 B 
  iifs-metadata 512 KiB      25      0     25                  0       0        0      1  1 KiB     49  17 KiB        0 B         0 B 

  total_objects    40
  total_used       1.0 GiB
  total_avail      29 GiB
  total_space      30 GiB
  #+end_EXAMPLE
  #+call: rook-tool("ceph osd get ii-block-pool")
** ceph auth list
Might want to ensure you don't save the results of this one
  #+call: rook-tool[:results silent]("ceph auth list")


  #+call: rook-tool("rdb ls")

  #+RESULTS:
  #+begin_EXAMPLE
  #+end_EXAMPLE

* Get keys onto pi
  #+begin_src shell :dir ~/
     scp ~/.ssh/id_rsa-4096-20090605-ccc.pub ubuntu@192.168.1.101:.ssh/authorized_keys
  #+end_src

* Update iptables etc to -legacy
  #+begin_src shell :dir /ssh:ubuntu@192.168.1.101:/
    sudo update-alternatives --set iptables /usr/sbin/iptables-legacy
    sudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy
    sudo update-alternatives --set arptables /usr/sbin/arptables-legacy
    sudo update-alternatives --set ebtables /usr/sbin/ebtables-legacy
  #+end_src

* ensure cgroups for raspi

We had an error regarding cgroups when trying to run kubeadm init.
Stephen noted this was the fix he's used on his pi's.

   #+begin_src shell
     echo "cgroup_enable=memory cgroup_memory=1" | sudo tee -a /boot/firmware/nobtcmd.txt
   #+end_src

   #+begin_src shell :results silent
      sudo reboot
   #+end_src

* install docker
** install
  #+begin_src shell :results silent
    sudo apt-get install -y docker.io
  #+end_src
** add ubuntu to docker group
  #+begin_src shell :results silent
    sudo adduser ubuntu docker
  #+end_src
** check
  #+begin_src shell
    id
  #+end_src

  #+RESULTS:
  #+begin_EXAMPLE
  uid=1000(ubuntu) gid=1000(ubuntu) groups=1000(ubuntu),4(adm),20(dialout),24(cdrom),25(floppy),27(sudo),29(audio),30(dip),44(video),46(plugdev),114(netdev),117(lxd),118(docker)
  #+end_EXAMPLE

** docker ps check

   #+begin_src shell
      docker ps
   #+end_src

   #+RESULTS:
   : CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
* Install kube-*

** setup repos
   #+begin_src shell
     sudo apt-get update && sudo apt-get install -y apt-transport-https curl
     curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
     cat <<-EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
     deb https://apt.kubernetes.io/ kubernetes-xenial main
     EOF
     sudo apt-get update
   #+end_src
** install and don't upgrade packages
   #+begin_src shell :results silent
     sudo apt-get install -y kubeadm kubectl kubelet 
     sudo apt-mark hold kubelet kubeadm kubectl
   #+end_src
   
** Verify
   #+begin_src shell
     kubectl version
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   Client Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.0", GitCommit:"70132b0f130acc0bed193d9ba59dd186f0e634cf", GitTreeState:"clean", BuildDate:"2019-12-07T21:20:10Z", GoVersion:"go1.13.4", Compiler:"gc", Platform:"linux/arm64"}
   Server Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.0", GitCommit:"70132b0f130acc0bed193d9ba59dd186f0e634cf", GitTreeState:"clean", BuildDate:"2019-12-07T21:12:17Z", GoVersion:"go1.13.4", Compiler:"gc", Platform:"linux/arm64"}
   #+end_EXAMPLE

Ensure that docker info shows no errors relating to cgroups.

   #+begin_src shell :results code
     (
       docker info
     ) 2>&1
     :
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   Client:
    Debug Mode: false

   Server:
    Containers: 20
     Running: 17
     Paused: 0
     Stopped: 3
    Images: 9
    Server Version: 19.03.2
    Storage Driver: overlay2
     Backing Filesystem: extfs
     Supports d_type: true
     Native Overlay Diff: true
    Logging Driver: json-file
    Cgroup Driver: cgroupfs
    Plugins:
     Volume: local
     Network: bridge host ipvlan macvlan null overlay
     Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog
    Swarm: inactive
    Runtimes: runc
    Default Runtime: runc
    Init Binary: docker-init
    containerd version: 
    runc version: 
    init version: 
    Security Options:
     apparmor
     seccomp
      Profile: default
    Kernel Version: 5.3.0-1014-raspi2
    Operating System: Ubuntu 19.10
    OSType: linux
    Architecture: aarch64
    CPUs: 4
    Total Memory: 3.703GiB
    Name: ubuntu
    ID: 2W3G:EMYS:O363:SAS2:PLLY:ZLZL:WCGT:ZDM3:EBOR:NILT:Y2Y3:XPED
    Docker Root Dir: /var/lib/docker
    Debug Mode: false
    Registry: https://index.docker.io/v1/
    Labels:
    Experimental: false
    Insecure Registries:
     127.0.0.0/8
    Live Restore Enabled: false

   WARNING: No swap limit support
   #+end_EXAMPLE
* Install kubernetes
  #+begin_src shell
    ip a show dev eth0
  #+end_src

  #+RESULTS:
  #+begin_EXAMPLE
  2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
      link/ether dc:a6:32:48:88:5f brd ff:ff:ff:ff:ff:ff
      inet 192.168.1.101/24 brd 192.168.1.255 scope global dynamic eth0
         valid_lft 15988sec preferred_lft 15988sec
      inet6 fe80::dea6:32ff:fe48:885f/64 scope link 
         valid_lft forever preferred_lft forever
  #+end_EXAMPLE
** migrate old config
  #+begin_src shell :async t
    sudo kubeadm config migrate --old-config kubeadm-config.yaml --new-config kubeadm-config-new.yaml
  #+end_src

  #+RESULTS:
  #+begin_EXAMPLE
  W1228 01:52:42.588628   10899 validation.go:28] Cannot validate kube-proxy config - no validator is available
  W1228 01:52:42.588818   10899 validation.go:28] Cannot validate kubelet config - no validator is available
  #+end_EXAMPLE


** show that the config comes from the pi
#+begin_src shell :dir ~/
kubectl config view 
#+end_src

#+RESULTS:
#+begin_EXAMPLE
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://192.168.1.101:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
#+end_EXAMPLE

** note that coredns WILL NOT START until networking is happy
  #+begin_src shell
    kubectl get pods --all-namespaces
  #+end_src

  #+RESULTS:
  #+begin_EXAMPLE
  NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE
  kube-system   coredns-6955765f44-29kb9         1/1     Running   0          40s
  kube-system   coredns-6955765f44-hl925         1/1     Running   0          40s
  kube-system   etcd-ubuntu                      1/1     Running   0          31s
  kube-system   kube-apiserver-ubuntu            1/1     Running   0          31s
  kube-system   kube-controller-manager-ubuntu   1/1     Running   0          31s
  kube-system   kube-proxy-lf66k                 1/1     Running   0          40s
  kube-system   kube-scheduler-ubuntu            1/1     Running   0          31s
  kube-system   weave-net-nlskh                  2/2     Running   0          20s
  #+end_EXAMPLE
** Core DNS Starts!
  #+begin_src shell :wrap "src json"
    COREDNS_NODE=$(kubectl get pod --namespace=kube-system -l k8s-app=kube-dns -o name | head -1)
    kubectl get $COREDNS_NODE  --namespace=kube-system 
  #+end_src

  #+RESULTS:
  #+begin_src json
  NAME                       READY   STATUS    RESTARTS   AGE
  coredns-6955765f44-lfbm7   1/1     Running   0          98s
  #+end_src

  #+begin_src shell :wrap "src json"
    COREDNS_NODE=$(kubectl get pod --namespace=kube-system -l k8s-app=kube-dns -o name | head -1)
    kubectl get $COREDNS_NODE  --namespace=kube-system 
  #+end_src

  #+RESULTS:
  #+begin_src json
  NAME                       READY   STATUS    RESTARTS   AGE
  coredns-6955765f44-h7bjj   1/1     Running   0          5m1s
  #+end_src
  #+begin_src shell
    free -m
  #+end_src

  #+RESULTS:
  #+begin_EXAMPLE
                total        used        free      shared  buff/cache   available
  Mem:           3791         953         451           4        2386        2863
  Swap:             0           0           0
  #+end_EXAMPLE
* locally run kubectl 
  :PROPERTIES:
  :header-args:shell+: :dir ~/
  :END:
** kubectl deploy some stuff
  #+begin_src shell
    kubectl version
  #+end_src

  #+RESULTS:
  #+begin_EXAMPLE
  Client Version: version.Info{Major:"1", Minor:"16", GitVersion:"v1.16.3", GitCommit:"b3cbbae08ec52a7fc73d334838e18d17e8512749", GitTreeState:"clean", BuildDate:"2019-11-13T11:23:11Z", GoVersion:"go1.12.12", Compiler:"gc", Platform:"linux/amd64"}
  Server Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.0", GitCommit:"70132b0f130acc0bed193d9ba59dd186f0e634cf", GitTreeState:"clean", BuildDate:"2019-12-07T21:12:17Z", GoVersion:"go1.13.4", Compiler:"gc", Platform:"linux/arm64"}
  #+end_EXAMPLE

* TODO kubectl apply -f http://iimacs.org
* setup pi.ii.nz
** get ip
  #+name: pi_ip
  #+begin_src shell :cache yes
    curl icanhazip.com
  #+end_src

  #+RESULTS[9df271cb6b4030541da56f2edf034902fe5ab69d]: pi_ip
  #+begin_EXAMPLE
  103.26.16.167
  #+end_EXAMPLE

** setup/check dns (dnsimple.com for now)
  :PROPERTIES:
  :header-args:shell+: :dir ~/
  :END:
   #+begin_src shell
      host pi.ii.nz
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   pi.ii.nz has address 103.26.16.167
   #+end_EXAMPLE
   #+begin_src shell
      host traefik.ii.nz
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   traefik.ii.nz is an alias for pi.ii.nz.
   pi.ii.nz has address 103.26.16.167
   #+end_EXAMPLE

* traefik
** install helm
   #+begin_src shell
     curl -s -L \
         https://get.helm.sh/helm-v3.0.2-linux-arm64.tar.gz \
         | sudo tar xvz -f - --strip-components 1 \
               -C /usr/local/bin linux-arm64/helm
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   linux-arm64/helm
   #+end_EXAMPLE

** check helm
   #+begin_src shell
     helm version 
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   version.BuildInfo{Version:"v3.0.2", GitCommit:"19e47ee3283ae98139d98460de796c1be1e3975f", GitTreeState:"clean", GoVersion:"go1.13.5"}
   #+end_EXAMPLE

** TODO Setup org-babel block for htpasswd cli later.
   For now http://www.htaccesstools.com/htpasswd-generator/
** update helm repo to include default k8s stable
   #+begin_src shell 
     helm repo add stable https://kubernetes-charts.storage.googleapis.com/
     helm repo update
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   "stable" has been added to your repositories
   Hang tight while we grab the latest from your chart repositories...
   ...Successfully got an update from the "stable" chart repository
   Update Complete. ⎈ Happy Helming!⎈ 
   #+end_EXAMPLE

** configure and install 
   #+begin_src shell 
     helm uninstall traefiik
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   release "traefiik" uninstalled
   #+end_EXAMPLE


   #+begin_src shell
   kubectl get svc --namespace traefik
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   NAME                   TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
   ii-traefik             LoadBalancer   10.96.86.160   <pending>     80:57952/TCP,443:11583/TCP   82m
   ii-traefik-dashboard   ClusterIP      10.96.170.0    <none>        80/TCP                       82m
   #+end_EXAMPLE
   #+begin_src shell
     sudo ss -ltnp
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   State     Recv-Q    Send-Q       Local Address:Port        Peer Address:Port                                                                                    
   LISTEN    0         128                0.0.0.0:22               0.0.0.0:*        users:(("sshd",pid=1295,fd=3))                                                 
   LISTEN    0         128          192.168.1.101:9080             0.0.0.0:*        users:(("cephcsi",pid=16941,fd=3))                                             
   LISTEN    0         128          192.168.1.101:9081             0.0.0.0:*        users:(("cephcsi",pid=16917,fd=5))                                             
   LISTEN    0         128          192.168.1.101:11583            0.0.0.0:*        users:(("kube-proxy",pid=10743,fd=11))                                         
   LISTEN    0         128          192.168.1.101:57952            0.0.0.0:*        users:(("kube-proxy",pid=10743,fd=8))                                          
   LISTEN    0         128              127.0.0.1:6784             0.0.0.0:*        users:(("weaver",pid=11546,fd=19))                                             
   LISTEN    0         128          192.168.1.101:9090             0.0.0.0:*        users:(("cephcsi",pid=16459,fd=6))                                             
   LISTEN    0         128          192.168.1.101:9091             0.0.0.0:*        users:(("cephcsi",pid=16442,fd=6))                                             
   LISTEN    0         128              127.0.0.1:34631            0.0.0.0:*        users:(("containerd",pid=1279,fd=8))                                           
   LISTEN    0         128              127.0.0.1:10248            0.0.0.0:*        users:(("kubelet",pid=10227,fd=28))                                            
   LISTEN    0         128              127.0.0.1:10249            0.0.0.0:*        users:(("kube-proxy",pid=10743,fd=15))                                         
   LISTEN    0         128          192.168.1.101:2379             0.0.0.0:*        users:(("etcd",pid=9682,fd=6))                                                 
   LISTEN    0         128              127.0.0.1:2379             0.0.0.0:*        users:(("etcd",pid=9682,fd=5))                                                 
   LISTEN    0         128          192.168.1.101:2380             0.0.0.0:*        users:(("etcd",pid=9682,fd=3))                                                 
   LISTEN    0         128              127.0.0.1:2381             0.0.0.0:*        users:(("etcd",pid=9682,fd=11))                                                
   LISTEN    0         128              127.0.0.1:35663            0.0.0.0:*        users:(("kubelet",pid=10227,fd=14))                                            
   LISTEN    0         128              127.0.0.1:10257            0.0.0.0:*        users:(("kube-controller",pid=9700,fd=6))                                      
   LISTEN    0         128              127.0.0.1:10259            0.0.0.0:*        users:(("kube-scheduler",pid=9691,fd=6))                                       
   LISTEN    0         128          127.0.0.53%lo:53               0.0.0.0:*        users:(("systemd-resolve",pid=1179,fd=13))                                     
   LISTEN    0         128                   [::]:22                  [::]:*        users:(("sshd",pid=1295,fd=4))                                                 
   LISTEN    0         128                      *:6781                   *:*        users:(("weave-npc",pid=11536,fd=10))                                          
   LISTEN    0         128                      *:6782                   *:*        users:(("weaver",pid=11546,fd=18))                                             
   LISTEN    0         128                      *:6783                   *:*        users:(("weaver",pid=11546,fd=17))                                             
   LISTEN    0         128                      *:10250                  *:*        users:(("kubelet",pid=10227,fd=37))                                            
   LISTEN    0         128                      *:10251                  *:*        users:(("kube-scheduler",pid=9691,fd=5))                                       
   LISTEN    0         128                      *:6443                   *:*        users:(("kube-apiserver",pid=9696,fd=5))                                       
   LISTEN    0         128                      *:10252                  *:*        users:(("kube-controller",pid=9700,fd=5))                                      
   LISTEN    0         128                      *:10256                  *:*        users:(("kube-proxy",pid=10743,fd=14))                                         
   #+end_EXAMPLE

** helm upgrade in place
#+NAME: helm upgrade in place
#+begin_SRC shell
  helm upgrade \
       ii-traefik \
       --namespace traefik \
       --values traefik-1.7-config.yaml \
       stable/traefik 
#+end_SRC

#+RESULTS: helm upgrade in place
#+begin_EXAMPLE
Release "ii-traefik" has been upgraded. Happy Helming!
NAME: ii-traefik
LAST DEPLOYED: Tue Dec 31 00:57:38 2019
NAMESPACE: traefik
STATUS: deployed
REVISION: 2
TEST SUITE: None
NOTES:
1. Get Traefik's load balancer IP/hostname:

     NOTE: It may take a few minutes for this to become available.

     You can watch the status by running:

         $ kubectl get svc ii-traefik --namespace traefik -w

     Once 'EXTERNAL-IP' is no longer '<pending>':

         $ kubectl describe svc ii-traefik --namespace traefik | grep Ingress | awk '{print $3}'

2. Configure DNS records corresponding to Kubernetes ingress resources to point to the load balancer IP/hostname found in step 1
#+end_EXAMPLE

** traefik logs

#+BEGIN_SRC tmate :session foo:traefik_logs
  TRAEFIK_POD=$(
    kubectl get pod --selector=app=traefik --namespace=${TRAEFIK_NAMESPACE} -o name \
    | sed s:pod/::)
  kubectl logs $TRAEFIK_POD --namespace=${TRAEFIK_NAMESPACE} -f | jq .
#+END_SRC

** wait for ip to set dns for
*** wait (-w) for traefik service to get an IP via tmate
  #+NAME: watch traefik get an IP
  #+BEGIN_SRC tmate :session foo:watch
    kubectl get svc --namespace=${TRAEFIK_NAMESPACE} ${TRAEFIK_DEPLOYMENT} -w
  #+END_SRC

*** traefik service
  #+NAME: get traefik service
  #+BEGIN_SRC shell
    kubectl get svc --namespace=${TRAEFIK_NAMESPACE} ${TRAEFIK_DEPLOYMENT}
  #+END_SRC

  #+RESULTS: get traefik service
  #+begin_EXAMPLE
  NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
  kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   103m
  #+end_EXAMPLE

*** traefik inbound ip

  #+NAME: traefik inbound IP
  #+BEGIN_SRC shell
  kubectl describe svc --namespace=${TRAEFIK_NAMESPACE} ${TRAEFIK_DEPLOYMENT} | grep Ingress | awk '{print $3}'
  #+END_SRC

  #+RESULTS: traefik inbound IP
  #+begin_EXAMPLE
  35.189.56.228
  #+end_EXAMPLE

** look at traefik
*** deployment
#+NAME: ii-traefik deployment
#+BEGIN_SRC shell :wrap "SRC yaml"
kubectl get deployment --namespace ${TRAEFIK_NAMESPACE} ${TRAEFIK_DEPLOYMENT} -o yaml
#+END_SRC

#+RESULTS: ii-traefik deployment
#+begin_SRC yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "4"
  creationTimestamp: "2019-08-30T05:07:16Z"
  generation: 4
  labels:
    app: traefik
    chart: traefik-1.77.1
    heritage: Tiller
    release: ii-traefik
  name: ii-traefik
  namespace: kube-system
  resourceVersion: "647910"
  selfLink: /apis/extensions/v1beta1/namespaces/kube-system/deployments/ii-traefik
  uid: 08d82ebc-cae4-11e9-9d36-42010a9800d6
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: traefik
      release: ii-traefik
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: 1ea5e59bdf9f15878cc4f13a3849d2f25ca9d4d48e8ad2fc9e7fb71e23584be5
      creationTimestamp: null
      labels:
        app: traefik
        chart: traefik-1.77.1
        heritage: Tiller
        release: ii-traefik
    spec:
      containers:
      - args:
        - --configfile=/config/traefik.toml
        env:
        - name: DNSIMPLE_BASE_URL
          valueFrom:
            secretKeyRef:
              key: DNSIMPLE_BASE_URL
              name: ii-traefik-dnsprovider-config
        - name: DNSIMPLE_OAUTH_TOKEN
          valueFrom:
            secretKeyRef:
              key: DNSIMPLE_OAUTH_TOKEN
              name: ii-traefik-dnsprovider-config
        image: traefik:1.7.14
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /ping
            port: 80
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 2
        name: ii-traefik
        ports:
        - containerPort: 80
          name: http
          protocol: TCP
        - containerPort: 8880
          name: httpn
          protocol: TCP
        - containerPort: 443
          name: https
          protocol: TCP
        - containerPort: 8080
          name: dash
          protocol: TCP
        readinessProbe:
          failureThreshold: 1
          httpGet:
            path: /ping
            port: 80
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 2
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /config
          name: config
        - mountPath: /ssl
          name: ssl
        - mountPath: /acme
          name: acme
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: ii-traefik
      serviceAccountName: ii-traefik
      terminationGracePeriodSeconds: 60
      volumes:
      - configMap:
          defaultMode: 420
          name: ii-traefik
        name: config
      - name: ssl
        secret:
          defaultMode: 420
          secretName: ii-traefik-default-cert
      - name: acme
        persistentVolumeClaim:
          claimName: ii-traefik-acme
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: "2019-08-30T05:07:48Z"
    lastUpdateTime: "2019-08-30T05:07:48Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2019-08-30T05:07:16Z"
    lastUpdateTime: "2019-08-30T05:21:11Z"
    message: ReplicaSet "ii-traefik-fdcf76955" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 4
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1
#+end_SRC

*** services
**** traefik service list
#+NAME: ii-traefik service list
#+BEGIN_SRC shell
kubectl get services --namespace ${TRAEFIK_NAMESPACE} | grep traefik
#+END_SRC

#+RESULTS: ii-traefik service list
#+begin_EXAMPLE
ii-traefik             LoadBalancer   10.0.4.69     35.189.56.228   80:31199/TCP,443:31755/TCP   6d22h
ii-traefik-dashboard   ClusterIP      10.0.1.227    <none>          80/TCP                       6d22h
#+end_EXAMPLE

**** traefik service
#+NAME: ii-traefik service
#+BEGIN_SRC shell :wrap "SRC yaml"
kubectl get services --namespace ${TRAEFIK_NAMESPACE} ${TRAEFIK_DEPLOYMENT} -o yaml
#+END_SRC

#+RESULTS: ii-traefik service
#+begin_SRC yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2019-08-30T05:07:16Z"
  labels:
    app: traefik
    chart: traefik-1.77.1
    heritage: Tiller
    release: ii-traefik
  name: ii-traefik
  namespace: kube-system
  resourceVersion: "645195"
  selfLink: /api/v1/namespaces/kube-system/services/ii-traefik
  uid: 08d6858a-cae4-11e9-9d36-42010a9800d6
spec:
  clusterIP: 10.0.4.69
  externalTrafficPolicy: Cluster
  ports:
  - name: http
    nodePort: 31199
    port: 80
    protocol: TCP
    targetPort: http
  - name: https
    nodePort: 31755
    port: 443
    protocol: TCP
    targetPort: https
  selector:
    app: traefik
    release: ii-traefik
  sessionAffinity: None
  type: LoadBalancer
status:
  loadBalancer:
    ingress:
    - ip: 35.189.56.228
#+end_SRC

**** traefik-dashboard service
#+NAME: ii-traefik-dashbord service
#+BEGIN_SRC shell :wrap "SRC yaml"
kubectl get services --namespace ${TRAEFIK_NAMESPACE} ${TRAEFIK_DEPLOYMENT}-dashboard -o yaml
#+END_SRC

#+RESULTS: ii-traefik-dashbord service
#+begin_SRC yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2019-08-30T05:07:16Z"
  labels:
    app: traefik
    chart: traefik-1.77.1
    heritage: Tiller
    release: ii-traefik
  name: ii-traefik-dashboard
  namespace: kube-system
  resourceVersion: "644960"
  selfLink: /api/v1/namespaces/kube-system/services/ii-traefik-dashboard
  uid: 08d34a95-cae4-11e9-9d36-42010a9800d6
spec:
  clusterIP: 10.0.1.227
  ports:
  - name: dashboard-http
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: traefik
    release: ii-traefik
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
#+end_SRC



*** ingress
**** traefik ingress list
#+NAME: traefik ingress list
#+BEGIN_SRC shell
kubectl get ingress --namespace ${TRAEFIK_NAMESPACE} | grep traefik
#+END_SRC

#+RESULTS: traefik ingress list
#+begin_EXAMPLE
ii-traefik-dashboard   traefik.apisnoop.io             80      6d22h
#+end_EXAMPLE

**** traefik-dashboard ingress
#+NAME: traefik-dashboard ingress
#+BEGIN_SRC shell :wrap "SRC yaml"
kubectl get ingress --namespace ${TRAEFIK_NAMESPACE} ${TRAEFIK_DEPLOYMENT}-dashboard -o yaml
#+END_SRC

#+RESULTS: traefik-dashboard ingress
#+begin_SRC yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  creationTimestamp: "2019-08-30T05:07:16Z"
  generation: 1
  labels:
    app: traefik
    chart: traefik-1.77.1
    heritage: Tiller
    release: ii-traefik
  name: ii-traefik-dashboard
  namespace: kube-system
  resourceVersion: "810181"
  selfLink: /apis/extensions/v1beta1/namespaces/kube-system/ingresses/ii-traefik-dashboard
  uid: 08d9af53-cae4-11e9-9d36-42010a9800d6
spec:
  rules:
  - host: traefik.apisnoop.io
    http:
      paths:
      - backend:
          serviceName: ii-traefik-dashboard
          servicePort: dashboard-http
status:
  loadBalancer: {}
#+end_SRC

#+BEGIN_SRC shell
kubectl api-resources -o wide
#+END_SRC

#+RESULTS:
#+begin_EXAMPLE
NAME                              SHORTNAMES        APIGROUP                       NAMESPACED   KIND                             VERBS
bindings                                                                           true         Binding                          [create]
componentstatuses                 cs                                               false        ComponentStatus                  [get list]
configmaps                        cm                                               true         ConfigMap                        [create delete deletecollection get list patch update watch]
endpoints                         ep                                               true         Endpoints                        [create delete deletecollection get list patch update watch]
events                            ev                                               true         Event                            [create delete deletecollection get list patch update watch]
limitranges                       limits                                           true         LimitRange                       [create delete deletecollection get list patch update watch]
namespaces                        ns                                               false        Namespace                        [create delete get list patch update watch]
nodes                             no                                               false        Node                             [create delete deletecollection get list patch update watch]
persistentvolumeclaims            pvc                                              true         PersistentVolumeClaim            [create delete deletecollection get list patch update watch]
persistentvolumes                 pv                                               false        PersistentVolume                 [create delete deletecollection get list patch update watch]
pods                              po                                               true         Pod                              [create delete deletecollection get list patch update watch]
podtemplates                                                                       true         PodTemplate                      [create delete deletecollection get list patch update watch]
replicationcontrollers            rc                                               true         ReplicationController            [create delete deletecollection get list patch update watch]
resourcequotas                    quota                                            true         ResourceQuota                    [create delete deletecollection get list patch update watch]
secrets                                                                            true         Secret                           [create delete deletecollection get list patch update watch]
serviceaccounts                   sa                                               true         ServiceAccount                   [create delete deletecollection get list patch update watch]
services                          svc                                              true         Service                          [create delete get list patch update watch]
mutatingwebhookconfigurations                       admissionregistration.k8s.io   false        MutatingWebhookConfiguration     [create delete deletecollection get list patch update watch]
validatingwebhookconfigurations                     admissionregistration.k8s.io   false        ValidatingWebhookConfiguration   [create delete deletecollection get list patch update watch]
customresourcedefinitions         crd,crds          apiextensions.k8s.io           false        CustomResourceDefinition         [create delete deletecollection get list patch update watch]
apiservices                                         apiregistration.k8s.io         false        APIService                       [create delete deletecollection get list patch update watch]
controllerrevisions                                 apps                           true         ControllerRevision               [create delete deletecollection get list patch update watch]
daemonsets                        ds                apps                           true         DaemonSet                        [create delete deletecollection get list patch update watch]
deployments                       deploy            apps                           true         Deployment                       [create delete deletecollection get list patch update watch]
replicasets                       rs                apps                           true         ReplicaSet                       [create delete deletecollection get list patch update watch]
statefulsets                      sts               apps                           true         StatefulSet                      [create delete deletecollection get list patch update watch]
tokenreviews                                        authentication.k8s.io          false        TokenReview                      [create]
localsubjectaccessreviews                           authorization.k8s.io           true         LocalSubjectAccessReview         [create]
selfsubjectaccessreviews                            authorization.k8s.io           false        SelfSubjectAccessReview          [create]
selfsubjectrulesreviews                             authorization.k8s.io           false        SelfSubjectRulesReview           [create]
subjectaccessreviews                                authorization.k8s.io           false        SubjectAccessReview              [create]
horizontalpodautoscalers          hpa               autoscaling                    true         HorizontalPodAutoscaler          [create delete deletecollection get list patch update watch]
cronjobs                          cj                batch                          true         CronJob                          [create delete deletecollection get list patch update watch]
jobs                                                batch                          true         Job                              [create delete deletecollection get list patch update watch]
cephblockpools                                      ceph.rook.io                   true         CephBlockPool                    [delete deletecollection get list patch create update watch]
cephclients                                         ceph.rook.io                   true         CephClient                       [delete deletecollection get list patch create update watch]
cephclusters                                        ceph.rook.io                   true         CephCluster                      [delete deletecollection get list patch create update watch]
cephfilesystems                                     ceph.rook.io                   true         CephFilesystem                   [delete deletecollection get list patch create update watch]
cephnfses                         nfs               ceph.rook.io                   true         CephNFS                          [delete deletecollection get list patch create update watch]
cephobjectstores                                    ceph.rook.io                   true         CephObjectStore                  [delete deletecollection get list patch create update watch]
cephobjectstoreusers              rcou,objectuser   ceph.rook.io                   true         CephObjectStoreUser              [delete deletecollection get list patch create update watch]
certificatesigningrequests        csr               certificates.k8s.io            false        CertificateSigningRequest        [create delete deletecollection get list patch update watch]
leases                                              coordination.k8s.io            true         Lease                            [create delete deletecollection get list patch update watch]
endpointslices                                      discovery.k8s.io               true         EndpointSlice                    [create delete deletecollection get list patch update watch]
events                            ev                events.k8s.io                  true         Event                            [create delete deletecollection get list patch update watch]
ingresses                         ing               extensions                     true         Ingress                          [create delete deletecollection get list patch update watch]
ingresses                         ing               networking.k8s.io              true         Ingress                          [create delete deletecollection get list patch update watch]
networkpolicies                   netpol            networking.k8s.io              true         NetworkPolicy                    [create delete deletecollection get list patch update watch]
runtimeclasses                                      node.k8s.io                    false        RuntimeClass                     [create delete deletecollection get list patch update watch]
objectbucketclaims                obc,obcs          objectbucket.io                true         ObjectBucketClaim                [delete deletecollection get list patch create update watch]
objectbuckets                     ob,obs            objectbucket.io                false        ObjectBucket                     [delete deletecollection get list patch create update watch]
poddisruptionbudgets              pdb               policy                         true         PodDisruptionBudget              [create delete deletecollection get list patch update watch]
podsecuritypolicies               psp               policy                         false        PodSecurityPolicy                [create delete deletecollection get list patch update watch]
clusterrolebindings                                 rbac.authorization.k8s.io      false        ClusterRoleBinding               [create delete deletecollection get list patch update watch]
clusterroles                                        rbac.authorization.k8s.io      false        ClusterRole                      [create delete deletecollection get list patch update watch]
rolebindings                                        rbac.authorization.k8s.io      true         RoleBinding                      [create delete deletecollection get list patch update watch]
roles                                               rbac.authorization.k8s.io      true         Role                             [create delete deletecollection get list patch update watch]
volumes                           rv                rook.io                        true         Volume                           [delete deletecollection get list patch create update watch]
priorityclasses                   pc                scheduling.k8s.io              false        PriorityClass                    [create delete deletecollection get list patch update watch]
volumesnapshotclasses                               snapshot.storage.k8s.io        false        VolumeSnapshotClass              [delete deletecollection get list patch create update watch]
volumesnapshotcontents                              snapshot.storage.k8s.io        false        VolumeSnapshotContent            [delete deletecollection get list patch create update watch]
volumesnapshots                                     snapshot.storage.k8s.io        true         VolumeSnapshot                   [delete deletecollection get list patch create update watch]
csidrivers                                          storage.k8s.io                 false        CSIDriver                        [create delete deletecollection get list patch update watch]
csinodes                                            storage.k8s.io                 false        CSINode                          [create delete deletecollection get list patch update watch]
storageclasses                    sc                storage.k8s.io                 false        StorageClass                     [create delete deletecollection get list patch update watch]
volumeattachments                                   storage.k8s.io                 false        VolumeAttachment                 [create delete deletecollection get list patch update watch]
#+end_EXAMPLE
** explores
#+BEGIN_SRC shell
kubectl get ingress --all-namespaces
#+END_SRC

#+RESULTS:
#+begin_EXAMPLE
No resources found
#+end_EXAMPLE

** traefik-2.1-config.yaml
#+NAME: traefik.yaml helm values
#+BEGIN_SRC yaml :tangle /ssh:ubuntu@192.168.1.101:traefik-config.yaml :noweb yes
  # Default values for Traefik
  image:
    name: traefik
    tag: 2.1.1

  #
  # Configure the deployment
  #
  deployment:
    # Number of pods of the deployment
    replicas: 1

  rollingUpdate:
    maxUnavailable: 1
    maxSurge: 1

  #
  # Configure Traefik entry points
  # Additional arguments to be passed at Traefik's binary
  ## Use curly braces to pass values: `helm install --set="{--providers.kubernetesingress,--global.checknewversion=true}" ."
  additionalArguments:
    - "--providers.kubernetesingress"
    - "--global.checknewversion=true"

  ports:
    # The name of this one can't be changed as it is used for the readiness and
    # liveness probes, but you can adjust its config to your liking
    traefik:
      port: 9000
      # Defines whether the port is exposed if service.type is LoadBalancer or
      # NodePort.
      #
      # You SHOULD NOT expose the traefik port on production deployments.
      # If you want to access it from outside of your cluster,
      # use `kubectl proxy` or create a secure ingress
      expose: false
      # The exposed port for this service
      exposedPort: 9000
    web:
      port: 8000
      expose: true
      exposedPort: 80
    websecure:
      port: 8443
      expose: true
      exposedPort: 443

  # Options for the main traefik service, where the entrypoints traffic comes
  # from.
  service:
    # type: LoadBalancer
    # type: NodePort
    # Additional annotations (e.g. for cloud provider specific config)
    annotations: {}
    # Additional entries here will be added to the service spec. Cannot contains
    # type, selector or ports entries.
    spec: {}
      # externalTrafficPolicy: Cluster
      # loadBalancerIp: "1.2.3.4"
      # clusterIP: "2.3.4.5"

  dashboard:
    # Enable the dashboard on Traefik
    enable: true

    # Expose the dashboard and api through an ingress route at /dashboard
    # and /api This is not secure and SHOULD NOT be enabled on production
    # deployments
    ingressRoute: true

  logs:
    loglevel: WARN
  #
  resources: {}
    # requests:
    #   cpu: "100m"
    #   memory: "50Mi"
    # limits:
    #   cpu: "300m"
    #   memory: "150Mi"
  nodeSelector: {}
  tolerations: []
#+END_SRC

* k
  #+begin_src shell
    kubeadm config print init-defaults --component-configs KubeletConfiguration,KubeProxyConfiguration
  #+end_src

  #+RESULTS:
  #+begin_EXAMPLE
  W1220 01:00:47.420920   22898 validation.go:28] Cannot validate kubelet config - no validator is available
  W1220 01:00:47.421090   22898 validation.go:28] Cannot validate kube-proxy config - no validator is available
  W1220 01:00:47.424708   22898 validation.go:28] Cannot validate kube-proxy config - no validator is available
  W1220 01:00:47.424762   22898 validation.go:28] Cannot validate kubelet config - no validator is available
  W1220 01:00:47.427539   22898 validation.go:28] Cannot validate kube-proxy config - no validator is available
  W1220 01:00:47.427595   22898 validation.go:28] Cannot validate kubelet config - no validator is available
  apiVersion: kubeadm.k8s.io/v1beta2
  bootstrapTokens:
  - groups:
    - system:bootstrappers:kubeadm:default-node-token
    token: abcdef.0123456789abcdef
    ttl: 24h0m0s
    usages:
    - signing
    - authentication
  kind: InitConfiguration
  localAPIEndpoint:
    advertiseAddress: 1.2.3.4
    bindPort: 6443
  nodeRegistration:
    criSocket: /var/run/dockershim.sock
    name: ubuntu
    taints:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
  ---
  apiServer:
    timeoutForControlPlane: 4m0s
  apiVersion: kubeadm.k8s.io/v1beta2
  certificatesDir: /etc/kubernetes/pki
  clusterName: kubernetes
  controllerManager: {}
  dns:
    type: CoreDNS
  etcd:
    local:
      dataDir: /var/lib/etcd
  imageRepository: k8s.gcr.io
  kind: ClusterConfiguration
  kubernetesVersion: v1.17.0
  networking:
    dnsDomain: cluster.local
    serviceSubnet: 10.96.0.0/12
  scheduler: {}
  ---
  apiVersion: kubelet.config.k8s.io/v1beta1
  authentication:
    anonymous:
      enabled: false
    webhook:
      cacheTTL: 0s
      enabled: true
    x509:
      clientCAFile: /etc/kubernetes/pki/ca.crt
  authorization:
    mode: Webhook
    webhook:
      cacheAuthorizedTTL: 0s
      cacheUnauthorizedTTL: 0s
  clusterDNS:
  - 10.96.0.10
  clusterDomain: cluster.local
  cpuManagerReconcilePeriod: 0s
  evictionPressureTransitionPeriod: 0s
  fileCheckFrequency: 0s
  healthzBindAddress: 127.0.0.1
  healthzPort: 10248
  httpCheckFrequency: 0s
  imageMinimumGCAge: 0s
  kind: KubeletConfiguration
  nodeStatusReportFrequency: 0s
  nodeStatusUpdateFrequency: 0s
  rotateCertificates: true
  runtimeRequestTimeout: 0s
  staticPodPath: /etc/kubernetes/manifests
  streamingConnectionIdleTimeout: 0s
  syncFrequency: 0s
  volumeStatsAggPeriod: 0s
  ---
  apiVersion: kubeproxy.config.k8s.io/v1alpha1
  bindAddress: 0.0.0.0
  clientConnection:
    acceptContentTypes: ""
    burst: 0
    contentType: ""
    kubeconfig: /var/lib/kube-proxy/kubeconfig.conf
    qps: 0
  clusterCIDR: ""
  configSyncPeriod: 0s
  conntrack:
    maxPerCore: null
    min: null
    tcpCloseWaitTimeout: null
    tcpEstablishedTimeout: null
  enableProfiling: false
  healthzBindAddress: ""
  hostnameOverride: ""
  iptables:
    masqueradeAll: false
    masqueradeBit: null
    minSyncPeriod: 0s
    syncPeriod: 0s
  ipvs:
    excludeCIDRs: null
    minSyncPeriod: 0s
    scheduler: ""
    strictARP: false
    syncPeriod: 0s
  kind: KubeProxyConfiguration
  metricsBindAddress: ""
  mode: ""
  nodePortAddresses: null
  oomScoreAdj: null
  portRange: ""
  udpIdleTimeout: 0s
  winkernel:
    enableDSR: false
    networkName: ""
    sourceVip: ""
  #+end_EXAMPLE
  #+begin_src shell
    #cat /etc/kubernetes/bootstrap-kubelet.conf
    ls -la /etc/kubernetes/
    #ls -la /etc/default/kubelet
    #KUBELET_EXTRA_ARGS
  #+end_src

  #+RESULTS:
  #+begin_EXAMPLE
  total 44
  drwxr-xr-x  4 root root 4096 Dec 20 01:58 .
  drwxr-xr-x 98 root root 4096 Dec 19 18:33 ..
  -rw-------  1 root root 5453 Dec 20 01:58 admin.conf
  -rw-------  1 root root 5485 Dec 20 01:58 controller-manager.conf
  -rw-------  1 root root 1861 Dec 20 01:58 kubelet.conf
  drwxr-xr-x  2 root root 4096 Dec 20 01:58 manifests
  drwxr-xr-x  3 root root 4096 Dec 20 01:58 pki
  -rw-------  1 root root 5433 Dec 20 01:58 scheduler.conf
  #+end_EXAMPLE
* Allow all ports
** apiserver cli arguments
  #+begin_src shell
     ps -axwu | grep kube-apiserver | sed 's/ /\n/g' \
      | grep \\-\\- | sort
  #+end_src

  #+RESULTS:
  #+begin_EXAMPLE
  --advertise-address=192.168.1.101
  --allow-privileged=true
  --authorization-mode=Node,RBAC
  --client-ca-file=/etc/kubernetes/pki/ca.crt
  --enable-admission-plugins=NodeRestriction
  --enable-bootstrap-token-auth=true
  --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
  --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
  --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
  --etcd-servers=https://127.0.0.1:2379
  --insecure-port=0
  --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
  --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
  --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
  --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
  --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
  --requestheader-allowed-names=front-proxy-client
  --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
  --requestheader-extra-headers-prefix=X-Remote-Extra-
  --requestheader-group-headers=X-Remote-Group
  --requestheader-username-headers=X-Remote-User
  --secure-port=6443
  --service-account-key-file=/etc/kubernetes/pki/sa.pub
  --service-cluster-ip-range=10.96.0.0/12
  --service-node-port-range=22-30000
  --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
  --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
  #+end_EXAMPLE
** kubeproxy cli arguments
  #+begin_src shell
     ps -axwu | grep kube-proxy | sed 's/ /\n/g' \
      | grep \\-\\- | sort
  #+end_src

  #+RESULTS:
  #+begin_EXAMPLE
  --config=/var/lib/kube-proxy/config.conf
  --hostname-override=ubuntu
  #+end_EXAMPLE

  #+begin_src shell
     ls -la /var/lib/ | grep ku
  #+end_src

  #+RESULTS:
  #+begin_EXAMPLE
  drwxr-xr-x  8 root      root      4096 Dec 20 01:58 kubelet
  #+end_EXAMPLE
[[/ssh:ubuntu@192.168.1.101|sudo:root@192.168.1.101:/etc/kubernetes/]]
* Requirements
** ip address
** port / hostport
** pvc / default storage class
*** nfs would work later
*** sig-storage-static-provisioner
We might want to have a local disk attached.
So we put on a 1TB ssd.
#+begin_src shell
lsblk | grep sda
#+end_src

#+RESULTS:
#+begin_EXAMPLE
sda           8:0    0 894.3G  0 disk 
#+end_EXAMPLE
** rook
Do we need to upgrade helm's approach?
Is rbac enabled?

#+begin_src shell
kubectl cluster-info dump | grep authorization-mode
#+end_src

#+RESULTS:
#+begin_EXAMPLE
                            "--authorization-mode=Node,RBAC",
#+end_EXAMPLE

#+begin_src shell
# Create a ServiceAccount for Tiller in the `kube-system` namespace
kubectl --namespace kube-system create sa tiller

# Create a ClusterRoleBinding for Tiller
kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller

# Patch Tiller's Deployment to use the new ServiceAccount
kubectl --namespace kube-system patch deploy/tiller-deploy -p '{"spec": {"template": {"spec": {"serviceAccountName": "tiller"}}}}'
#+end_src

#+RESULTS:
#+begin_EXAMPLE
serviceaccount/tiller created
clusterrolebinding.rbac.authorization.k8s.io/tiller created
Error from server (NotFound): deployments.apps "tiller-deploy" not found
#+end_EXAMPLE
** update helm repo to include default k8s stable
   #+begin_src shell
     helm repo add rook-release https://charts.rook.io/release
     helm search repo rook
     # helm search rook-ceph
     # helm install --namespace rook-ceph rook-release/rook-ceph
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   "rook-release" has been added to your repositories
   NAME                  	CHART VERSION	APP VERSION	DESCRIPTION                                       
   rook-release/rook-ceph	v1.2.0       	           	File, Block, and Object Storage Services for yo...
   stable/rookout        	0.1.0        	1.0        	A Helm chart for Rookout agent on Kubernetes      
   #+end_EXAMPLE
** Ensure lvm works
   #+begin_src shell
     sudo pvs -a
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
     PV             VG Fmt Attr PSize PFree
     /dev/loop0            ---     0     0 
     /dev/loop1            ---     0     0 
     /dev/loop2            ---     0     0 
     /dev/mmcblk0p1        ---     0     0 
     /dev/mmcblk0p2        ---     0     0 
     /dev/sda              ---     0     0 
   #+end_EXAMPLE
* Install Rook
   #+begin_src shell
     ls -la /dev/disk/by-path/platform-*pci*usb*
     # ls -la /dev/disk/by-path/platform-.*pci.*
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   lrwxrwxrwx 1 root root 9 Dec 28 02:54 /dev/disk/by-path/platform-fd500000.pcie-pci-0000:01:00.0-usb-0:2:1.0-scsi-0:0:0:0 -> ../../sda
   #+end_EXAMPLE
** for later
   #+begin_src shell
     ROOT_OP_POD=$(kubectl --namespace rook-ceph describe d
     kubectl logs --namespace rook-ceph $ROOT_OP_POD
     # ROOT_OP_POD=$(kubectl --namespace rook-ceph get pods -l "app=rook-ceph-operator" -o name)
     # kubectl logs --namespace rook-ceph $ROOT_OP_POD
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   #+end_EXAMPLE
   #+begin_src shell
     docker images | grep ceph\\\|csi
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   csi-node-driver-registrar                  latest              bf8c90f910d9        18 hours ago        16.6MB
   quay.io/k8scsi/csi-node-driver-registrar   v1.1.0              bf8c90f910d9        18 hours ago        16.6MB
   csi-attacher                               latest              89499377228b        18 hours ago        44.7MB
   quay.io/k8scsi/csi-attacher                v1.2.0              89499377228b        18 hours ago        44.7MB
   csi-snapshotter                            latest              cd74005517c1        18 hours ago        46MB
   quay.io/k8scsi/csi-snapshotter             v1.2.2              cd74005517c1        18 hours ago        46MB
   quay.io/k8scsi/csi-provisioner             v1.4.0              2dc30504f03e        19 hours ago        52.6MB
   csi-provisioner                            latest              2dc30504f03e        19 hours ago        52.6MB
   quay.io/cephcsi/cephcsi                    v1.2.2              e73792b88385        19 hours ago        940MB
   rook/ceph                                  master              0de3709a4ba8        2 days ago          929MB
   rook/ceph                                  v1.2.0              2e69cb44dd57        10 days ago         929MB
   ceph/ceph                                  v14.2               7fb4cbf85c65        2 weeks ago         855MB
   ceph/ceph                                  v14.2.5             7fb4cbf85c65        2 weeks ago         855MB
   quay.io/cephcsi/cephcsi                    <none>              d46311d35105        5 weeks ago         984MB
   quay.io/k8scsi/csi-snapshotter             <none>              538dbe77c2f9        2 months ago        47.6MB
   quay.io/k8scsi/csi-provisioner             <none>              2130c4e026a5        2 months ago        54.5MB
   quay.io/k8scsi/csi-attacher                <none>              eef7a9550ede        6 months ago        46.2MB
   quay.io/k8scsi/csi-node-driver-registrar   <none>              a93898755322        8 months ago        15.8MB
   #+end_EXAMPLE
Need to enable docker experimental CLI options.
#+begin_src shell
mkdir -p ~/.docker
echo '{"experimental":"enabled"}' > ~/.docker/config.json
rm ~/.docker/config.json
rm -rf ~/.docker
#+end_src

#+RESULTS:
#+begin_EXAMPLE
#+end_EXAMPLE
   #+begin_src shell :var DOCKER_CLI_EXPERIMENTAL="enabled"
     echo $DOCKER_CLI_EXPERIMENTAL
     docker manifest inspect quay.io/cephcsi/cephcsi:v1.2.2
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   enabled
   docker manifest inspect is only supported on a Docker cli with experimental cli features enabled
   #+end_EXAMPLE

   #+begin_src shell
     docker manifest inspect rook/ceph:master
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   {
      "schemaVersion": 2,
      "mediaType": "application/vnd.docker.distribution.manifest.list.v2+json",
      "manifests": [
         {
            "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
            "size": 1787,
            "digest": "sha256:f8268ed131d0ad151d749bcfa9692b7341c410625568445b8107a67019c2172a",
            "platform": {
               "architecture": "amd64",
               "os": "linux"
            }
         },
         {
            "mediaType": "application/vnd.docker.distribution.manifest.v2+json",
            "size": 1788,
            "digest": "sha256:e586993b4db487dd022eb85ea5b1f81afdcf9324bd272e9ce1648b6846bf11e7",
            "platform": {
               "architecture": "arm64",
               "os": "linux"
            }
         }
      ]
   }
   #+end_EXAMPLE

   #+begin_src shell
     docker manifest inspect quay.io/k8scsi/csi-snapshotter
   #+end_src

   #+RESULTS:
   #+begin_EXAMPLE
   no such manifest: quay.io/k8scsi/csi-snapshotter:latest
   #+end_EXAMPLE
* rebuilding csi
  #+begin_src shell
  mkdir -p ~/go/src/github.com/ceph
  cd ~/go/src/github.com/ceph
  git clone --recursive --branch v1.2.2 --depth 1 https://github.com/ceph/ceph-csi
  #+end_src
  #+begin_src shell
    cd ~/go/src/github.com/ceph/ceph-csi
    make image-cephcsi
  #+end_src

  #+RESULTS:
  #+begin_EXAMPLE
  cephcsi image settings: quay.io/cephcsi/cephcsi version v1.2.2
  if [ ! -d ./vendor ]; then dep ensure -vendor-only; fi
  CGO_ENABLED=0 GOOS=linux go build -a -ldflags ' -X github.com/ceph/ceph-csi/pkg/util.GitCommit=f8c854dc7d6ffff02cb2eed6002534dc0473f111 -X github.com/ceph/ceph-csi/pkg/util.DriverVersion=v1.2.2 -extldflags "-static"' -o  _output/cephcsi ./cmd/
  cp _output/cephcsi deploy/cephcsi/image/cephcsi
  docker build -t quay.io/cephcsi/cephcsi:v1.2.2 deploy/cephcsi/image
  Sending build context to Docker daemon  557.1kBSending build context to Docker daemon  3.342MBSending build context to Docker daemon  6.128MBSending build context to Docker daemon  8.913MBSending build context to Docker daemon  11.14MBSending build context to Docker daemon  13.93MBSending build context to Docker daemon  16.71MBSending build context to Docker daemon   19.5MBSending build context to Docker daemon  22.28MBSending build context to Docker daemon  25.07MBSending build context to Docker daemon  27.85MBSending build context to Docker daemon  30.64MBSending build context to Docker daemon  32.31MBSending build context to Docker daemon  34.54MBSending build context to Docker daemon  37.32MBSending build context to Docker daemon  40.11MBSending build context to Docker daemon  42.73MB
  Step 1/7 : FROM ceph/ceph:v14.2
  v14.2: Pulling from ceph/ceph
  Digest: sha256:8c86fc6acf47edb6c3e38777b72c3fea2bad5be18c7e88553673205b378d0121
  Status: Downloaded newer image for ceph/ceph:v14.2
   ---> 7fb4cbf85c65
  Step 2/7 : LABEL maintainers="Ceph-CSI Authors"
   ---> Running in 1469b57d9381
  Removing intermediate container 1469b57d9381
   ---> de4f1e0ee45e
  Step 3/7 : LABEL description="Ceph-CSI Plugin"
   ---> Running in e6a81785954e
  Removing intermediate container e6a81785954e
   ---> 38c1b8574903
  Step 4/7 : ENV CSIBIN=/usr/local/bin/cephcsi
   ---> Running in d13c37ddc1a4
  Removing intermediate container d13c37ddc1a4
   ---> f2991dd06573
  Step 5/7 : COPY cephcsi $CSIBIN
   ---> 459e4a563a1d
  Step 6/7 : RUN chmod +x $CSIBIN
   ---> Running in 1ccfd3b5884d
  Removing intermediate container 1ccfd3b5884d
   ---> 24d99a35aef1
  Step 7/7 : ENTRYPOINT ["/usr/local/bin/cephcsi"]
   ---> Running in 1cb36208fc39
  Removing intermediate container 1cb36208fc39
   ---> e73792b88385
  Successfully built e73792b88385
  Successfully tagged quay.io/cephcsi/cephcsi:v1.2.2
  #+end_EXAMPLE

  #+begin_src shell
    sudo apt-get install -y make golang-go
  #+end_src


  #+begin_src shell
  mkdir -p ~/go/src/github.com/kubernetes-csi
  cd ~/go/src/github.com/kubernetes-csi
  git clone --recursive --branch v1.4.0 --depth 1 https://github.com/kubernetes-csi/external-provisioner
  #+end_src
  
  #+begin_src shell
    cd ~/go/src/github.com/kubernetes-csi/external-provisioner
    make container
    docker tag csi-provisioner:latest quay.io/k8scsi/csi-provisioner:v1.4.0
  #+end_src
  #+begin_src shell
  cd ~/go/src/github.com/kubernetes-csi
  git clone --recursive --branch v1.2.2 --depth 1 https://github.com/kubernetes-csi/external-snapshotter
  #+end_src
  #+begin_src shell
    cd ~/go/src/github.com/kubernetes-csi/external-snapshotter
    make container
    docker tag csi-snapshotter:latest quay.io/k8scsi/csi-snapshotter:v1.2.2
  #+end_src
  #+begin_src shell
  cd ~/go/src/github.com/kubernetes-csi
  git clone --recursive --branch v1.2.0 --depth 1 https://github.com/kubernetes-csi/external-attacher
  #+end_src
  #+begin_src shell
    cd ~/go/src/github.com/kubernetes-csi/external-attacher
    make container
    docker tag csi-attacher:latest quay.io/k8scsi/csi-attacher:v1.2.0
  #+end_src

  #+begin_src shell
  cd ~/go/src/github.com/kubernetes-csi
  git clone --recursive --branch v1.1.0 --depth 1 https://github.com/kubernetes-csi/node-driver-registrar
  #+end_src
  #+begin_src shell
    cd ~/go/src/github.com/kubernetes-csi/node-driver-registrar
    make container
    docker tag csi-node-driver-registrar:latest quay.io/k8scsi/csi-node-driver-registrar:v1.1.0
  #+end_src
* rebuilding k14s / kwt
  #+begin_src shell
  mkdir -p ~/go/src/github.com/k14s
  cd ~/go/src/github.com/k14s
  git clone --recursive --branch v0.0.6 --depth 1 https://github.com/k14s/kwt
  #+end_src

  #+begin_src shell
    cd ~/go/src/github.com/k14s/kwt/images/sshd
    docker build -t cppforfile/sshd .
  #+end_src

  #+begin_src shell
    sudo apt-get -y install iptables
  #+end_src
* building tilt
Tilt is required for building tmate
  #+begin_src shell
  mkdir -p ~/go/src/github.com/windmilleng
  cd ~/go/src/github.com/windmilleng
  git clone --recursive --branch v0.10.25 --depth 1 https://github.com/windmilleng/tilt
  #+end_src
  #+begin_src shell
    cd ~/go/src/github.com/windmilleng/tilt
    make
  #+end_src

* rebuilding tmate
  #+begin_src shell
  
  cd ~/go/src/github.com/k14s
  git clone --recursive --branch v0.0.6 --depth 1 https://github.com/k14s/kwt
  #+end_src
* web
  #+begin_src emacs-lisp
(symbol-value 'file-local-variables-alist)
(alist-get 'ii file-local-variables-alist)
  
  #+end_src
# Local Variables:
# ii: set
# End:
