#+TITLE: Tracking flakes for Deployment Lifecycle Test


* Summary

Locating the root cause of an e2e test flake is hard.
The following process looks to provide a clear process by using the [[https://github.com/brianpursley/k8s-e2e-log-combiner][k8s-e2e-log-combiner]] container, which pulls the e2e test logs together into a single sequential file.
Various code blocks will help filter the combined log file into a various files for closer inspection.

* Deployment Lifecycle Test: Overview

- Test Grid: https://testgrid.k8s.io/sig-release-master-blocking#gce-cos-master-default&include-filter-by-regex=should.run.*lifecycle.*Deployment&width=5
- e2e test: https://github.com/kubernetes/kubernetes/blob/2f263b24a7aab78c794fb90339c176678de6bf8e/test/e2e/apps/deployment.go#L176
- [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]

* Deployment Lifecycle Test: Success

Before exploring test flakes it's nice to have a clear list and order of the what componments the cluster exercised during the test run.
=release-master-blocking/kind-master-parallel= has some test flakes that will be investigated later.
First, select a recent passing prow job.

** Combine Prow Logs together

#+BEGIN_SRC shell :results silent :async t
docker run brianpursley/k8s-e2e-log-combiner https://prow.k8s.io/view/gs/kubernetes-jenkins/logs/ci-kubernetes-kind-e2e-parallel/1367182553303224320 > ci-kubernetes-kind-e2e-parallel-1367182553303224320-pass.log
#+END_SRC

** Filter logs for 'Conformance'

To help track/note events later we will included a set of line numbers in the resulting log file.

#+BEGIN_SRC shell :results silent :async t
nl ci-kubernetes-kind-e2e-parallel-1367182553303224320-pass.log | \
   grep -a 'Conformance' > ci-kubernetes-kind-e2e-parallel-1367182553303224320-pass-conformance.log
#+END_SRC

** Locate the start/end of the test run

#+BEGIN_SRC shell :results verbatim :exports both
cat ci-kubernetes-kind-e2e-parallel-1367182553303224320-pass-conformance.log | grep -a Deployment | grep -a lifecycle | tail -3
#+END_SRC

#+RESULTS:
#+begin_example
200721	19:02:01.402000000 [/build-log.txt]                                               [It] should run the lifecycle of a Deployment [Conformance]
207667	19:02:19.423000000 [/build-log.txt]                                                 should run the lifecycle of a Deployment [Conformance]
207670	19:02:19.423000000 [/build-log.txt]                                               {"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":-1,"completed":8,"skipped":70,"failed":0}
#+end_example

The above results provide 200721 (start) and 207670 (end) of the test run.
Rounding those numbers to the nearest 5k provides a small buffer around the test to help understand the state of the cluster before/after the test run.

** Trim log file to Deployment Lifecycle Test run

#+BEGIN_SRC shell :results silent :async t
nl ci-kubernetes-kind-e2e-parallel-1367182553303224320-pass.log | \
    head -210000 | tail -n +200000 > ci-kubernetes-kind-e2e-parallel-1367182553303224320-pass-deployment-lifecycle.log
#+END_SRC

A short review of the logs will locate the namespace used by the deployment lifecycle test, =deployment-197=

** Short summary of deployment lifecycle test

This filter provides a short log of the steps taken in testing the Deployment lifecycle.

#+BEGIN_SRC shell :results silent :async t
cat ci-kubernetes-kind-e2e-parallel-1367182553303224320-pass-deployment-lifecycle.log | \
    grep -a "deployment-197" > ci-kubernetes-kind-e2e-parallel-1367182553303224320-pass-deployment-197.log
#+END_SRC

* Deployment Lifecycle Test: Fail #1

Select a recent failing prow job from =release-master-blocking/kind-master-parallel=

** Combine Prow Logs together

#+BEGIN_SRC shell :results silent :async t
PROW_URL="https://prow.k8s.io/view/gs/kubernetes-jenkins/logs"
PROW_JOB="ci-kubernetes-kind-e2e-parallel/1366153928210649088"
PROW_JOB_BASE_NAME="ci-kubernetes-kind-e2e-parallel-1366153928210649088"
LOG_DIR="/tmp/logs"

docker run brianpursley/k8s-e2e-log-combiner ${PROW_URL}/${PROW_JOB} > ${LOG_DIR}/${PROW_JOB_BASE_NAME}-fail1.log
#+END_SRC

** Filter log for 'Conformance'

To help track/note events later we will included a set of line numbers in the resulting log file.

#+BEGIN_SRC shell :results silent :async t
LOG_DIR="/tmp/logs"
PROW_JOB_BASE_NAME="ci-kubernetes-kind-e2e-parallel-1366153928210649088"

nl ${LOG_DIR}/${PROW_JOB_BASE_NAME}-fail1.log | \
   grep -a 'Conformance' > ${LOG_DIR}/${PROW_JOB_BASE_NAME}-fail1-conformance.log
#+END_SRC

** Locate the start/end of the test run

*** Test Start

#+BEGIN_SRC shell :results verbatim :exports both
LOG_DIR="/tmp/logs"
PROW_JOB_BASE_NAME="ci-kubernetes-kind-e2e-parallel-1366153928210649088"

cat ${LOG_DIR}/${PROW_JOB_BASE_NAME}-fail1-conformance.log | grep -a "\[It\] should run the lifecycle of a Deployment" | tail -4
#+END_SRC

#+RESULTS:
#+begin_example
112105	22:51:10.708000000 [/build-log.txt]                                               [It] should run the lifecycle of a Deployment [Conformance]
482857	23:07:37.019000000 [/build-log.txt]                                               [91m[1m[Fail] [0m[90m[sig-apps] Deployment [0m[91m[1m[It] should run the lifecycle of a Deployment [Conformance] [0m
#+end_example


The above result provides the start of the test run at 112105, rounding to 110,000

*** Test End

#+BEGIN_SRC shell :results verbatim :exports both
LOG_DIR="/tmp/logs"
PROW_JOB_BASE_NAME="ci-kubernetes-kind-e2e-parallel-1366153928210649088"

cat ${LOG_DIR}/${PROW_JOB_BASE_NAME}-fail1-conformance.log | grep -a "Deployment should" | grep -a lifecycle | grep -v "PASSED" | tail -10
#+END_SRC

#+RESULTS:
#+begin_example
126652	22:52:10.748000000 [/build-log.txt]                                               {"msg":"FAILED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":-1,"completed":5,"skipped":50,"failed":1,"failures":["[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]"]}
#+end_example

The above result provides the end point in the test run at 126652, rounding to 130,000

** Trim log file to Deployment Lifecycle Test run

#+BEGIN_SRC shell :results silent :async t
LOG_DIR="/tmp/logs"
PROW_JOB_BASE_NAME="ci-kubernetes-kind-e2e-parallel-1366153928210649088"
nl ${LOG_DIR}/${PROW_JOB_BASE_NAME}-fail1.log | \
    head -130000 | tail -n +110000 > ${LOG_DIR}/${PROW_JOB_BASE_NAME}-fail1-deployment-lifecycle.log
#+END_SRC

A short review of the logs will locate the namespace used by the deployment lifecycle test, =deployment-2394=

** Short summary of deployment lifecycle test

This filter provides a short log of the steps taken in testing the Deployment lifecycle.

#+BEGIN_SRC shell :results silent :async t
LOG_DIR="/tmp/logs"
PROW_JOB_BASE_NAME="ci-kubernetes-kind-e2e-parallel-1366153928210649088"

cat ${LOG_DIR}/${PROW_JOB_BASE_NAME}-fail1-deployment-lifecycle.log | \
    grep -a "deployment-2394" > ${LOG_DIR}/${PROW_JOB_BASE_NAME}-fail1-deployment-2394.log
#+END_SRC

** Filtering logs futher
*** Focus on the node running the test

The test is using node =kind-worker= so lets's remove logs for =kind-worker2=

#+BEGIN_SRC shell :results silent :async t
LOG_DIR="/tmp/logs"
PROW_JOB_BASE_NAME="ci-kubernetes-kind-e2e-parallel-1366153928210649088"

cat ${LOG_DIR}/${PROW_JOB_BASE_NAME}-fail1-deployment-lifecycle.log | \
    grep -v "kind-worker2" > ${LOG_DIR}/${PROW_JOB_BASE_NAME}-fail1-deployment-lifecycle-no-kind-worker2.log
#+END_SRC

*** Locate pod details

#+BEGIN_SRC shell :results silent :async t
LOG_DIR="/tmp/logs"
PROW_JOB_BASE_NAME="ci-kubernetes-kind-e2e-parallel-1366153928210649088"

cat ${LOG_DIR}/${PROW_JOB_BASE_NAME}-fail1-deployment-lifecycle-no-kind-worker2.log | \
    grep -a "deployment-2394/test-deployment-" > ${LOG_DIR}/${PROW_JOB_BASE_NAME}-fail1-deployment-lifecycle-pod-details.log
#+END_SRC

*** Locate pod events

#+BEGIN_SRC shell :results silent :async t
LOG_DIR="/tmp/logs"
PROW_JOB_BASE_NAME="ci-kubernetes-kind-e2e-parallel-1366153928210649088"

cat ${LOG_DIR}/${PROW_JOB_BASE_NAME}-fail1-deployment-lifecycle-no-kind-worker2.log | \
    grep -a "test-deployment-7778d6bf57-" > ${LOG_DIR}/${PROW_JOB_BASE_NAME}-fail1-deployment-lifecycle-pod-events.log
#+END_SRC

** Locate test failures
*** Error: cannot find volume

#+BEGIN_SRC shell :results verbatim :exports both
LOG_DIR="/tmp/logs"
PROW_JOB_BASE_NAME="ci-kubernetes-kind-e2e-parallel-1366153928210649088"

grep -a "cannot find volume" ${LOG_DIR}/${PROW_JOB_BASE_NAME}-fail1-deployment-lifecycle-pod-events.log  | tail -1
#+END_SRC

#+RESULTS:
#+begin_example
128903	22:52:20.825451000 [/artifacts/logs/kind-worker/kubelet.log]                      Feb 28 22:52:20 kind-worker kubelet[243]: E0228 22:52:20.825451     243 event.go:264] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"test-deployment-7778d6bf57-fqqvk.16680b29cf00e812", GenerateName:"", Namespace:"deployment-2394", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"deployment-2394", Name:"test-deployment-7778d6bf57-fqqvk", UID:"dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8", APIVersion:"v1", ResourceVersion:"12891", FieldPath:"spec.containers{test-deployment}"}, Reason:"Failed", Message:"Error: cannot find volume \"kube-api-access-2t44s\" to mount into container \"test-deployment\"", Source:v1.EventSource{Component:"kubelet", Host:"kind-worker"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xc0072589242b8012, ext:380893991846, loc:(*time.Location)(0x3e95d80)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xc0072589242b8012, ext:380893991846, loc:(*time.Location)(0x3e95d80)}}, Count:1, Type:"Warning", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'namespaces "deployment-2394" not found' (will not retry!)
#+end_example

*** Locate volume

#+BEGIN_SRC shell :results silent :async t
LOG_DIR="/tmp/logs"
PROW_JOB_BASE_NAME="ci-kubernetes-kind-e2e-parallel-1366153928210649088"

grep -a "kube-api-access-2t44s" ${LOG_DIR}/${PROW_JOB_BASE_NAME}-fail1-deployment-lifecycle.log > ${LOG_DIR}/${PROW_JOB_BASE_NAME}-fail1-deployment-lifecycle-issue-volume.log
#+END_SRC

*** Locate when the volume was around

#+BEGIN_SRC shell :results verbatim :exports both
LOG_DIR="/tmp/logs"
PROW_JOB_BASE_NAME="ci-kubernetes-kind-e2e-parallel-1366153928210649088"

head -c 15 ${LOG_DIR}/${PROW_JOB_BASE_NAME}-fail1-deployment-lifecycle-issue-volume.log && echo
tail -1 ${LOG_DIR}/${PROW_JOB_BASE_NAME}-fail1-deployment-lifecycle-issue-volume.log | head -c 15 && echo
#+END_SRC

#+RESULTS:
#+begin_example
112951	22:51:11
128903	22:52:20
#+end_example

*** Full filter for volume

#+BEGIN_SRC shell :results silent :async t
LOG_DIR="/tmp/logs"
PROW_JOB_BASE_NAME="ci-kubernetes-kind-e2e-parallel-1366153928210649088"

nl ${LOG_DIR}/${PROW_JOB_BASE_NAME}-fail1.log | \
    head -128903 | \
    tail -n +112951 > ${LOG_DIR}/${PROW_JOB_BASE_NAME}-fail1-deployment-lifecycle-track-volume-issue.log
#+END_SRC

*** Volume deleted before pod requests uses it

#+BEGIN_SRC shell :results verbatim :exports both
LOG_DIR="/tmp/logs"
PROW_JOB_BASE_NAME="ci-kubernetes-kind-e2e-parallel-1366153928210649088"

cat ${LOG_DIR}/${PROW_JOB_BASE_NAME}-fail1-deployment-lifecycle-track-volume-issue.log | \
    grep "2t44s"
#+END_SRC

#+RESULTS:
#+begin_example
112951	22:51:11.755606000 [/artifacts/logs/kind-worker/journal.log]                      Feb 28 22:51:11 kind-worker kubelet[243]: I0228 22:51:11.755606     243 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-api-access-2t44s" (UniqueName: "kubernetes.io/projected/dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8-kube-api-access-2t44s") pod "test-deployment-7778d6bf57-fqqvk" (UID: "dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8")
112952	22:51:11.755606000 [/artifacts/logs/kind-worker/kubelet.log]                      Feb 28 22:51:11 kind-worker kubelet[243]: I0228 22:51:11.755606     243 reconciler.go:224] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-api-access-2t44s" (UniqueName: "kubernetes.io/projected/dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8-kube-api-access-2t44s") pod "test-deployment-7778d6bf57-fqqvk" (UID: "dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8")
126665	22:52:10.761000000 [/build-log.txt]                                               &Pod{ObjectMeta:{test-deployment-7778d6bf57-fqqvk test-deployment-7778d6bf57- deployment-2394  dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8 13303 0 2021-02-28 22:51:10 +0000 UTC <nil> <nil> map[pod-template-hash:7778d6bf57 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7778d6bf57 60e4eda1-6635-4b70-9862-62db0dfbf84e 0xc001d5e9b7 0xc001d5e9b8}] []  [{kube-controller-manager Update v1 2021-02-28 22:51:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"60e4eda1-6635-4b70-9862-62db0dfbf84e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-02-28 22:51:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2t44s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/agnhost:2.28,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2t44s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:kind-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-28 22:51:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-28 22:51:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-28 22:51:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-02-28 22:51:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.18.0.3,PodIP:,StartTime:2021-02-28 22:51:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.28,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
128480	22:52:19.542102000 [/artifacts/logs/kind-worker/journal.log]                      Feb 28 22:52:19 kind-worker kubelet[243]: I0228 22:52:19.542102     243 reconciler.go:196] operationExecutor.UnmountVolume started for volume "kube-api-access-2t44s" (UniqueName: "kubernetes.io/projected/dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8-kube-api-access-2t44s") pod "dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8" (UID: "dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8")
128481	22:52:19.542102000 [/artifacts/logs/kind-worker/kubelet.log]                      Feb 28 22:52:19 kind-worker kubelet[243]: I0228 22:52:19.542102     243 reconciler.go:196] operationExecutor.UnmountVolume started for volume "kube-api-access-2t44s" (UniqueName: "kubernetes.io/projected/dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8-kube-api-access-2t44s") pod "dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8" (UID: "dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8")
128495	22:52:19.560271652 [/artifacts/logs/kind-worker/journal.log]                      Feb 28 22:52:19 kind-worker systemd[1]: var-lib-kubelet-pods-dc0a9362\x2dfa20\x2d419e\x2d8ab4\x2d7f2a4f27c9b8-volumes-kubernetes.io\x7eprojected-kube\x2dapi\x2daccess\x2d2t44s.mount: Succeeded.
128498	22:52:19.560997000 [/artifacts/logs/kind-worker/journal.log]                      Feb 28 22:52:19 kind-worker kubelet[243]: I0228 22:52:19.560997     243 operation_generator.go:829] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8-kube-api-access-2t44s" (OuterVolumeSpecName: "kube-api-access-2t44s") pod "dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8" (UID: "dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8"). InnerVolumeSpecName "kube-api-access-2t44s". PluginName "kubernetes.io/projected", VolumeGidValue ""
128499	22:52:19.560997000 [/artifacts/logs/kind-worker/kubelet.log]                      Feb 28 22:52:19 kind-worker kubelet[243]: I0228 22:52:19.560997     243 operation_generator.go:829] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8-kube-api-access-2t44s" (OuterVolumeSpecName: "kube-api-access-2t44s") pod "dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8" (UID: "dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8"). InnerVolumeSpecName "kube-api-access-2t44s". PluginName "kubernetes.io/projected", VolumeGidValue ""
128566	22:52:19.642678000 [/artifacts/logs/kind-worker/journal.log]                      Feb 28 22:52:19 kind-worker kubelet[243]: I0228 22:52:19.642678     243 reconciler.go:319] Volume detached for volume "kube-api-access-2t44s" (UniqueName: "kubernetes.io/projected/dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8-kube-api-access-2t44s") on node "kind-worker" DevicePath ""
128567	22:52:19.642678000 [/artifacts/logs/kind-worker/kubelet.log]                      Feb 28 22:52:19 kind-worker kubelet[243]: I0228 22:52:19.642678     243 reconciler.go:319] Volume detached for volume "kube-api-access-2t44s" (UniqueName: "kubernetes.io/projected/dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8-kube-api-access-2t44s") on node "kind-worker" DevicePath ""
128813	22:52:20.606772000 [/artifacts/logs/kind-worker/journal.log]                      Feb 28 22:52:20 kind-worker kubelet[243]: E0228 22:52:20.606772     243 kubelet_pods.go:159] Mount cannot be satisfied for container "test-deployment", because the volume is missing (ok=false) or the volume mounter (vol.Mounter) is nil (vol={Mounter:<nil> BlockVolumeMapper:<nil> SELinuxLabeled:false ReadOnly:false InnerVolumeSpecName:}): {Name:kube-api-access-2t44s ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil> SubPathExpr:}
128814	22:52:20.606772000 [/artifacts/logs/kind-worker/kubelet.log]                      Feb 28 22:52:20 kind-worker kubelet[243]: E0228 22:52:20.606772     243 kubelet_pods.go:159] Mount cannot be satisfied for container "test-deployment", because the volume is missing (ok=false) or the volume mounter (vol.Mounter) is nil (vol={Mounter:<nil> BlockVolumeMapper:<nil> SELinuxLabeled:false ReadOnly:false InnerVolumeSpecName:}): {Name:kube-api-access-2t44s ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil> SubPathExpr:}
128815	22:52:20.606897000 [/artifacts/logs/kind-worker/journal.log]                      Feb 28 22:52:20 kind-worker kubelet[243]: E0228 22:52:20.606897     243 kuberuntime_manager.go:841] container &Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/agnhost:2.28,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2t44s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod test-deployment-7778d6bf57-fqqvk_deployment-2394(dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8): CreateContainerConfigError: cannot find volume "kube-api-access-2t44s" to mount into container "test-deployment"
128816	22:52:20.606897000 [/artifacts/logs/kind-worker/kubelet.log]                      Feb 28 22:52:20 kind-worker kubelet[243]: E0228 22:52:20.606897     243 kuberuntime_manager.go:841] container &Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/agnhost:2.28,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2t44s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod test-deployment-7778d6bf57-fqqvk_deployment-2394(dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8): CreateContainerConfigError: cannot find volume "kube-api-access-2t44s" to mount into container "test-deployment"
128817	22:52:20.606931000 [/artifacts/logs/kind-worker/journal.log]                      Feb 28 22:52:20 kind-worker kubelet[243]: E0228 22:52:20.606931     243 pod_workers.go:191] Error syncing pod dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8 ("test-deployment-7778d6bf57-fqqvk_deployment-2394(dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8)"), skipping: failed to "StartContainer" for "test-deployment" with CreateContainerConfigError: "cannot find volume \"kube-api-access-2t44s\" to mount into container \"test-deployment\""
128818	22:52:20.606931000 [/artifacts/logs/kind-worker/kubelet.log]                      Feb 28 22:52:20 kind-worker kubelet[243]: E0228 22:52:20.606931     243 pod_workers.go:191] Error syncing pod dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8 ("test-deployment-7778d6bf57-fqqvk_deployment-2394(dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8)"), skipping: failed to "StartContainer" for "test-deployment" with CreateContainerConfigError: "cannot find volume \"kube-api-access-2t44s\" to mount into container \"test-deployment\""
128902	22:52:20.825451000 [/artifacts/logs/kind-worker/journal.log]                      Feb 28 22:52:20 kind-worker kubelet[243]: E0228 22:52:20.825451     243 event.go:264] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"test-deployment-7778d6bf57-fqqvk.16680b29cf00e812", GenerateName:"", Namespace:"deployment-2394", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"deployment-2394", Name:"test-deployment-7778d6bf57-fqqvk", UID:"dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8", APIVersion:"v1", ResourceVersion:"12891", FieldPath:"spec.containers{test-deployment}"}, Reason:"Failed", Message:"Error: cannot find volume \"kube-api-access-2t44s\" to mount into container \"test-deployment\"", Source:v1.EventSource{Component:"kubelet", Host:"kind-worker"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xc0072589242b8012, ext:380893991846, loc:(*time.Location)(0x3e95d80)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xc0072589242b8012, ext:380893991846, loc:(*time.Location)(0x3e95d80)}}, Count:1, Type:"Warning", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'namespaces "deployment-2394" not found' (will not retry!)
128903	22:52:20.825451000 [/artifacts/logs/kind-worker/kubelet.log]                      Feb 28 22:52:20 kind-worker kubelet[243]: E0228 22:52:20.825451     243 event.go:264] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"test-deployment-7778d6bf57-fqqvk.16680b29cf00e812", GenerateName:"", Namespace:"deployment-2394", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"deployment-2394", Name:"test-deployment-7778d6bf57-fqqvk", UID:"dc0a9362-fa20-419e-8ab4-7f2a4f27c9b8", APIVersion:"v1", ResourceVersion:"12891", FieldPath:"spec.containers{test-deployment}"}, Reason:"Failed", Message:"Error: cannot find volume \"kube-api-access-2t44s\" to mount into container \"test-deployment\"", Source:v1.EventSource{Component:"kubelet", Host:"kind-worker"}, FirstTimestamp:v1.Time{Time:time.Time{wall:0xc0072589242b8012, ext:380893991846, loc:(*time.Location)(0x3e95d80)}}, LastTimestamp:v1.Time{Time:time.Time{wall:0xc0072589242b8012, ext:380893991846, loc:(*time.Location)(0x3e95d80)}}, Count:1, Type:"Warning", EventTime:v1.MicroTime{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'namespaces "deployment-2394" not found' (will not retry!)
#+end_example
